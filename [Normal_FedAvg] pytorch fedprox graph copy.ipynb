{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "import psutil\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "import psutil\n",
    "import time\n",
    "import re\n",
    "import tracemalloc\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing\n",
    "# Define a regular expression pattern to match the number\n",
    "pattern = re.compile(r'\\d+')\n",
    "\n",
    "# Use findall to extract all the numbers in the string\n",
    "alpha =5\n",
    "beta = 1\n",
    "gamma =1\n",
    "\n",
    "client_name_base ='client'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2f8c85",
   "metadata": {},
   "source": [
    "## fedprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6c9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedProx 2018|\n",
      "|=======================|\n",
      "client_1\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6090090274810791 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8767406888253558| flobal_FPR: 1.0 \n",
      "client_2\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6056119799613953 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8849961846940467| flobal_FPR: 1.0 \n",
      "client_3\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6432264447212219 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8253781711317854| flobal_FPR: 1.0 \n",
      "client_4\n",
      "comm_round: 0 | global_acc: 63.630% | global_loss: 0.5713022351264954 | global_f1: 0.026690391459074734 | global_precision: 1.0 | global_recall: 0.013525698827772768 | global_auc: 0.9239840056296538| flobal_FPR: 0.9864743011722272 \n",
      "client_5\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6331082582473755 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.849743897291109| flobal_FPR: 1.0 \n",
      "client_6\n",
      "comm_round: 0 | global_acc: 83.444% | global_loss: 0.4411005973815918 | global_f1: 0.711136890951276 | global_precision: 0.9967479674796748 | global_recall: 0.5527502254283138 | global_auc: 0.9708213377929915| flobal_FPR: 0.4472497745716862 \n",
      "client_7\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.5848038792610168 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8955109494769921| flobal_FPR: 1.0 \n",
      "client_8\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6222100853919983 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9158393839289911| flobal_FPR: 1.0 \n",
      "client_9\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6065301895141602 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9131705691049962| flobal_FPR: 1.0 \n",
      "client_10\n",
      "comm_round: 0 | global_acc: 63.198% | global_loss: 0.5049663782119751 | global_f1: 0.0036003600360036 | global_precision: 1.0 | global_recall: 0.0018034265103697023 | global_auc: 0.9424375507777574| flobal_FPR: 0.9981965734896303 \n",
      "[]\n",
      "[[[4, 21.272747685273572], [10, 22.00496330517416]], [[1, 17.789216727896193], [2, 17.505768545981464], [5, 17.176500215321965], [8, 15.518238651517636]], [[3, 19.237777202256076], [6, 19.986063365864275], [7, 20.290810216131337], [9, 19.979450756036094]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6549065113067627 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8365726634159404| flobal_FPR: 1.0 \n",
      "client_2\n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6537374258041382 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.839386540588255| flobal_FPR: 1.0 \n",
      "client_3\n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6326941847801208 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9185651790534718| flobal_FPR: 1.0 \n",
      "client_4\n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.44042667746543884 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9710860587723309| flobal_FPR: 1.0 \n",
      "client_5\n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6573266386985779 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8348119721309349| flobal_FPR: 1.0 \n",
      "client_6\n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.624153733253479 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9210302893032306| flobal_FPR: 1.0 \n",
      "client_7\n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6478021740913391 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9036320668037043| flobal_FPR: 1.0 \n",
      "client_8\n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6503815054893494 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8451949699690076| flobal_FPR: 1.0 \n",
      "client_9\n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6112509965896606 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9356288797055639| flobal_FPR: 1.0 \n",
      "client_10\n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.5647534132003784 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9180497447519956| flobal_FPR: 1.0 \n",
      "[]\n",
      "[[[7, 13.979630022955233]], [[1, 18.928871221381073], [2, 17.723105447590207], [3, 17.674872537157327], [4, 17.434346916006433], [5, 17.861385349700193], [6, 19.127119973767854], [10, 18.13825691486511]], [[8, 21.063833260096636], [9, 23.754058237866097]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6497949361801147 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.87045623651763| flobal_FPR: 1.0 \n",
      "client_2\n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6514391303062439 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8618498369651154| flobal_FPR: 1.0 \n",
      "client_3\n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6519005298614502 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8627911990127214| flobal_FPR: 1.0 \n",
      "client_4\n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6521496772766113 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8654944394349264| flobal_FPR: 1.0 \n",
      "client_5\n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6527929306030273 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8669075508869696| flobal_FPR: 1.0 \n",
      "client_6\n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6540468335151672 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8496194903017152| flobal_FPR: 1.0 \n",
      "client_7\n",
      "comm_round: 2 | global_acc: 94.282% | global_loss: 0.2354050725698471 | global_f1: 0.9193245778611634 | global_precision: 0.9579667644183774 | global_recall: 0.8836789900811542 | global_auc: 0.985978097722165| flobal_FPR: 0.11632100991884581 \n",
      "client_8\n",
      "comm_round: 2 | global_acc: 80.120% | global_loss: 0.38542991876602173 | global_f1: 0.6313193588162761 | global_precision: 0.9980506822612085 | global_recall: 0.4616771866546438 | global_auc: 0.9819092294316548| flobal_FPR: 0.5383228133453561 \n",
      "client_9\n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.4387146234512329 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9753655167567193| flobal_FPR: 1.0 \n",
      "client_10\n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6509724855422974 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8608187309442443| flobal_FPR: 1.0 \n",
      "[]\n",
      "[[[6, 17.418404795414407], [7, 18.012133830852783], [10, 18.10437362997685]], [[8, 22.14323793436881]], [[1, 20.422811838992683], [2, 19.141289067119224], [3, 18.97169260126129], [4, 19.035344516222708], [5, 20.344879053844018], [9, 19.41760692857378]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6355807185173035 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8831630809438408| flobal_FPR: 1.0 \n",
      "client_2\n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.5971850156784058 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9151202450532789| flobal_FPR: 1.0 \n",
      "client_3\n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6398745775222778 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8781153385745714| flobal_FPR: 1.0 \n",
      "client_4\n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6202464699745178 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9040617932365334| flobal_FPR: 1.0 \n",
      "client_5\n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6180809140205383 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9155637417253919| flobal_FPR: 1.0 \n",
      "client_6\n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.654927134513855 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8295006958719195| flobal_FPR: 1.0 \n",
      "client_7\n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6511224508285522 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8586105068825081| flobal_FPR: 1.0 \n",
      "client_8\n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.48048725724220276 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9711432764907353| flobal_FPR: 1.0 \n",
      "client_9\n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6436974406242371 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8706535308080614| flobal_FPR: 1.0 \n",
      "client_10\n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6472068428993225 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8557277785137734| flobal_FPR: 1.0 \n",
      "[]\n",
      "[[[4, 18.968212471244343], [8, 19.64237362280265], [10, 19.198806349058394]], [[2, 20.626163388947262], [5, 21.80706346040446]], [[1, 18.481203633114138], [3, 18.408751592162925], [6, 17.346580314479084], [7, 18.502502791141346], [9, 18.365010889190494]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6409021615982056 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8886932565238883| flobal_FPR: 1.0 \n",
      "client_2\n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.5293723344802856 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9523307554495722| flobal_FPR: 1.0 \n",
      "client_3\n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6439182758331299 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8828188249617401| flobal_FPR: 1.0 \n",
      "client_4\n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.5259197950363159 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9539076852655115| flobal_FPR: 1.0 \n",
      "client_5\n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6224563717842102 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9154044343019511| flobal_FPR: 1.0 \n",
      "client_6\n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6465760469436646 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8732986038401873| flobal_FPR: 1.0 \n",
      "client_7\n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6286951303482056 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9025686719458915| flobal_FPR: 1.0 \n",
      "client_8\n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6209434866905212 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.905703775562194| flobal_FPR: 1.0 \n",
      "client_9\n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6113758683204651 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9180381112739797| flobal_FPR: 1.0 \n",
      "client_10\n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6192142963409424 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.904993658567392| flobal_FPR: 1.0 \n",
      "[]\n",
      "[[[1, 18.489802359293137], [2, 19.529494682252366], [3, 20.154981058675276], [6, 18.948066418481947], [7, 19.93640205095155], [9, 19.700708455977765], [10, 19.349438761953945]], [[5, 17.070685076129834], [8, 16.53256402940991]], [[4, 21.844946906097476]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.5342450141906738 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9433710780340466| flobal_FPR: 1.0 \n",
      "client_2\n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.5905652046203613 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9189455225592132| flobal_FPR: 1.0 \n",
      "client_3\n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.5769338607788086 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.927756576357639| flobal_FPR: 1.0 \n",
      "client_4\n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.541785478591919 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9447039422295727| flobal_FPR: 1.0 \n",
      "client_5\n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.5980185270309448 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9307765322833763| flobal_FPR: 1.0 \n",
      "client_6\n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.48986953496932983 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9600710544347055| flobal_FPR: 1.0 \n",
      "client_7\n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.5408042669296265 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9412723036328267| flobal_FPR: 1.0 \n",
      "client_8\n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.5941696763038635 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9221962487019175| flobal_FPR: 1.0 \n",
      "client_9\n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.47740069031715393 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9656907365700993| flobal_FPR: 1.0 \n",
      "client_10\n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.6034600138664246 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9049240951172157| flobal_FPR: 1.0 \n",
      "[]\n",
      "[[[4, 20.957300195534394], [5, 20.956793517624945], [6, 21.169982442482258], [7, 20.882701381256517], [8, 20.829218353429614], [9, 20.574565719638787]], [[1, 18.308228040297124], [3, 18.853498106313957]], [[2, 17.697073042782307], [10, 17.720419209576615]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.44723498821258545 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9716613223893169| flobal_FPR: 1.0 \n",
      "client_2\n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.5893997550010681 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9183348836723424| flobal_FPR: 1.0 \n",
      "client_3\n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.47150760889053345 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9664053645053564| flobal_FPR: 1.0 \n",
      "client_4\n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.5608963370323181 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9401958507894859| flobal_FPR: 1.0 \n",
      "client_5\n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.5914707779884338 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9313601055275167| flobal_FPR: 1.0 \n",
      "client_6\n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.5475004315376282 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9388022076067752| flobal_FPR: 1.0 \n",
      "client_7\n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.6044834852218628 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9148918490154991| flobal_FPR: 1.0 \n",
      "client_8\n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.5867868661880493 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9222978635711169| flobal_FPR: 1.0 \n",
      "client_9\n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.4920274317264557 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9625995552687547| flobal_FPR: 1.0 \n",
      "client_10\n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.5683621168136597 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9198491351577476| flobal_FPR: 1.0 \n",
      "[]\n",
      "[[[1, 19.67517724950821], [2, 19.227786477400386], [4, 19.716717574005827], [5, 19.009399017871207], [7, 19.512697714995824]], [[3, 22.784787897011412], [9, 22.082048354166957]], [[6, 17.56318586316311], [8, 14.798138581688425], [10, 17.355633183179293]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 7 | global_acc: 81.217% | global_loss: 0.3978038430213928 | global_f1: 0.6586102719033232 | global_precision: 0.9981684981684982 | global_recall: 0.4914337240757439 | global_auc: 0.9810678203278171| flobal_FPR: 0.5085662759242561 \n",
      "client_2\n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.41158074140548706 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9808280282299402| flobal_FPR: 1.0 \n",
      "client_3\n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.6423712968826294 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8822027254627394| flobal_FPR: 1.0 \n",
      "client_4\n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.40679022669792175 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9810507262376714| flobal_FPR: 1.0 \n",
      "client_5\n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.496646910905838 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9671802965919607| flobal_FPR: 1.0 \n",
      "client_6\n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.49345991015434265 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9596669691370949| flobal_FPR: 1.0 \n",
      "client_7\n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.41833481192588806 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9791333391263306| flobal_FPR: 1.0 \n",
      "client_8\n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.5422222018241882 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9393558661931605| flobal_FPR: 1.0 \n",
      "client_9\n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.6098864078521729 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9182631834608981| flobal_FPR: 1.0 \n",
      "client_10\n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.4319104552268982 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9712287469414638| flobal_FPR: 1.0 \n",
      "[]\n",
      "[[[7, 21.585735701444058], [9, 23.708131435800468]], [[3, 16.850979802416347], [6, 16.01654453683099], [8, 17.55974927159308]], [[1, 19.334332285236947], [2, 19.707989287813046], [4, 18.50760373148079], [5, 18.920360170787287], [10, 20.510525938983445]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.5165423154830933 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9457224651007531| flobal_FPR: 1.0 \n",
      "client_2\n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.5115854740142822 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9503701582770299| flobal_FPR: 1.0 \n",
      "client_3\n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.4102577567100525 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9814488760873147| flobal_FPR: 1.0 \n",
      "client_4\n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.6032884120941162 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9164231945910499| flobal_FPR: 1.0 \n",
      "client_5\n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.5267991423606873 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9537372191998922| flobal_FPR: 1.0 \n",
      "client_6\n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.446201890707016 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9770625800395158| flobal_FPR: 1.0 \n",
      "client_7\n",
      "comm_round: 8 | global_acc: 90.226% | global_loss: 0.37205034494400024 | global_f1: 0.848297213622291 | global_precision: 0.991556091676719 | global_recall: 0.7412082957619477 | global_auc: 0.9825146451243144| flobal_FPR: 0.2587917042380523 \n",
      "client_8\n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.3985660672187805 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9819567130153928| flobal_FPR: 1.0 \n",
      "client_9\n",
      "comm_round: 8 | global_acc: 86.569% | global_loss: 0.38709205389022827 | global_f1: 0.7775330396475769 | global_precision: 0.9985855728429985 | global_recall: 0.6366095581605049 | global_auc: 0.9807226146740418| flobal_FPR: 0.36339044183949504 \n",
      "client_10\n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.6024724841117859 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9036520099088743| flobal_FPR: 1.0 \n",
      "[]\n",
      "[[[1, 19.43745864372483], [2, 20.481642134330635], [3, 19.550762812520468], [4, 19.041545996679154], [5, 19.3447670468284], [7, 19.1583760922547], [9, 18.780230615432366]], [[6, 15.602328029246994], [8, 15.26690285148977]], [[10, 17.00810009368109]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.4870518147945404 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9621095246845783| flobal_FPR: 1.0 \n",
      "client_2\n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.46502718329429626 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9704856288559637| flobal_FPR: 1.0 \n",
      "client_3\n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.5102196931838989 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9563056062442812| flobal_FPR: 1.0 \n",
      "client_4\n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.4053558111190796 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9809244199049285| flobal_FPR: 1.0 \n",
      "client_5\n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.5365632772445679 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9538568778309118| flobal_FPR: 1.0 \n",
      "client_6\n",
      "comm_round: 9 | global_acc: 87.932% | global_loss: 0.38658609986305237 | global_f1: 0.8047337278106508 | global_precision: 0.9973333333333333 | global_recall: 0.6744815148782687 | global_auc: 0.9829367741837453| flobal_FPR: 0.3255184851217313 \n",
      "client_7\n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.4712374806404114 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.966840788968234| flobal_FPR: 1.0 \n",
      "client_8\n",
      "comm_round: 9 | global_acc: 89.894% | global_loss: 0.3609550893306732 | global_f1: 0.8415015641293013 | global_precision: 0.9975278121137207 | global_recall: 0.7276825969341749 | global_auc: 0.9844239600264197| flobal_FPR: 0.2723174030658251 \n",
      "client_9\n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.43491995334625244 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.977337509989359| flobal_FPR: 1.0 \n",
      "client_10\n",
      "comm_round: 9 | global_acc: 69.415% | global_loss: 0.4656776189804077 | global_f1: 0.2912172573189522 | global_precision: 1.0 | global_recall: 0.1704238052299369 | global_auc: 0.9591033389981248| flobal_FPR: 0.8295761947700632 \n",
      "[]\n",
      "[[[1, 17.53420288679367], [3, 16.608760180385353], [5, 15.602434292591923], [6, 17.917192185539857], [7, 17.83393764842597], [10, 17.41543890416402]], [[4, 22.541651207942017], [9, 21.61816246513331]], [[2, 19.788947286149117], [8, 19.410302119485713]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.4364897608757019 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.975515090045494| flobal_FPR: 1.0 \n",
      "client_2\n",
      "comm_round: 10 | global_acc: 90.093% | global_loss: 0.3703363835811615 | global_f1: 0.8451143451143451 | global_precision: 0.9975460122699387 | global_recall: 0.7330928764652841 | global_auc: 0.9853033559972479| flobal_FPR: 0.26690712353471596 \n",
      "client_3\n",
      "comm_round: 10 | global_acc: 89.594% | global_loss: 0.37330058217048645 | global_f1: 0.8365535248041776 | global_precision: 0.9937965260545906 | global_recall: 0.7222723174030659 | global_auc: 0.9838712511117094| flobal_FPR: 0.2777276825969342 \n",
      "client_4\n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.4233100116252899 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9783650547414495| flobal_FPR: 1.0 \n",
      "client_5\n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.4170323312282562 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9805250829656916| flobal_FPR: 1.0 \n",
      "client_6\n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.4039801359176636 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.980272945136043| flobal_FPR: 1.0 \n",
      "client_7\n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.42125120759010315 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9785127286868748| flobal_FPR: 1.0 \n",
      "client_8\n",
      "comm_round: 10 | global_acc: 80.120% | global_loss: 0.37943601608276367 | global_f1: 0.6313193588162761 | global_precision: 0.9980506822612085 | global_recall: 0.4616771866546438 | global_auc: 0.9839823626976564| flobal_FPR: 0.5383228133453561 \n",
      "client_9\n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.42062151432037354 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9787572691431253| flobal_FPR: 1.0 \n",
      "client_10\n",
      "comm_round: 10 | global_acc: 92.686% | global_loss: 0.36007174849510193 | global_f1: 0.8913043478260869 | global_precision: 0.985792349726776 | global_recall: 0.8133453561767358 | global_auc: 0.9840630847900109| flobal_FPR: 0.18665464382326422 \n",
      "[]\n",
      "[[[5, 16.141256927083454], [7, 16.095859783928184]], [[1, 20.791655462769903], [2, 21.045558479655092], [3, 21.820442893944122], [8, 20.931118526685413], [9, 22.160379178865785], [10, 20.813315055436266]], [[4, 19.070406078398527], [6, 18.302531920709065]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 11 | global_acc: 63.132% | global_loss: 0.44222790002822876 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9757658033676307| flobal_FPR: 1.0 \n",
      "client_2\n",
      "comm_round: 11 | global_acc: 92.287% | global_loss: 0.34153300523757935 | global_f1: 0.8844621513944223 | global_precision: 0.9877641824249166 | global_recall: 0.8007213706041478 | global_auc: 0.986693675329097| flobal_FPR: 0.1992786293958521 \n",
      "client_3\n",
      "comm_round: 11 | global_acc: 63.132% | global_loss: 0.46301740407943726 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9724343551325718| flobal_FPR: 1.0 \n",
      "client_4\n",
      "comm_round: 11 | global_acc: 88.730% | global_loss: 0.3810426890850067 | global_f1: 0.819968135953266 | global_precision: 0.9974160206718347 | global_recall: 0.6961226330027052 | global_auc: 0.9830160717685879| flobal_FPR: 0.30387736699729484 \n",
      "client_5\n",
      "comm_round: 11 | global_acc: 90.758% | global_loss: 0.35987481474876404 | global_f1: 0.8575819672131147 | global_precision: 0.9928825622775801 | global_recall: 0.7547339945897205 | global_auc: 0.9855702137378555| flobal_FPR: 0.24526600541027954 \n",
      "client_6\n",
      "comm_round: 11 | global_acc: 89.528% | global_loss: 0.3772478401660919 | global_f1: 0.8349921424829754 | global_precision: 0.99625 | global_recall: 0.7186654643823264 | global_auc: 0.982867448151488| flobal_FPR: 0.2813345356176736 \n",
      "client_7\n",
      "comm_round: 11 | global_acc: 63.132% | global_loss: 0.43233874440193176 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9787235557986715| flobal_FPR: 1.0 \n",
      "client_8\n",
      "comm_round: 11 | global_acc: 63.464% | global_loss: 0.38142919540405273 | global_f1: 0.017873100983020553 | global_precision: 1.0 | global_recall: 0.009017132551848512 | global_auc: 0.9832188266711492| flobal_FPR: 0.9909828674481514 \n",
      "client_9\n",
      "comm_round: 11 | global_acc: 63.132% | global_loss: 0.4448055624961853 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9762411140408482| flobal_FPR: 1.0 \n",
      "client_10\n",
      "comm_round: 11 | global_acc: 93.551% | global_loss: 0.34853461384773254 | global_f1: 0.9055501460564751 | global_precision: 0.9841269841269841 | global_recall: 0.8385933273219116 | global_auc: 0.9852183603823568| flobal_FPR: 0.16140667267808836 \n",
      "[]\n",
      "[[[1, 18.194531767289607], [7, 18.529339118832155], [9, 17.446467971303846]], [[2, 20.68743823541753], [4, 19.921151701577774], [5, 21.086874133601775], [6, 20.08438834607969], [8, 20.777858763024057], [10, 20.373864248735142]], [[3, 15.997798674111522]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 12 | global_acc: 93.019% | global_loss: 0.3307437598705292 | global_f1: 0.8970588235294117 | global_precision: 0.9828141783029001 | global_recall: 0.8250676284941388 | global_auc: 0.9853437170434253| flobal_FPR: 0.17493237150586113 \n",
      "client_2\n",
      "comm_round: 12 | global_acc: 63.132% | global_loss: 0.42114943265914917 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.979951956109974| flobal_FPR: 1.0 \n",
      "client_3\n",
      "comm_round: 12 | global_acc: 63.132% | global_loss: 0.6013608574867249 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9249631171263316| flobal_FPR: 1.0 \n",
      "client_4\n",
      "comm_round: 12 | global_acc: 63.132% | global_loss: 0.48584693670272827 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9685668172371107| flobal_FPR: 1.0 \n",
      "client_5\n",
      "comm_round: 12 | global_acc: 63.132% | global_loss: 0.45913928747177124 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9766475735176456| flobal_FPR: 1.0 \n",
      "client_6\n",
      "comm_round: 12 | global_acc: 63.132% | global_loss: 0.42451512813568115 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9788256455037082| flobal_FPR: 1.0 \n",
      "client_7\n",
      "comm_round: 12 | global_acc: 93.384% | global_loss: 0.33164218068122864 | global_f1: 0.9027845627747924 | global_precision: 0.9850746268656716 | global_recall: 0.8331830477908025 | global_auc: 0.9874287212053613| flobal_FPR: 0.16681695220919748 \n",
      "client_8\n",
      "comm_round: 12 | global_acc: 87.400% | global_loss: 0.3705369234085083 | global_f1: 0.7941336230309614 | global_precision: 0.9986338797814208 | global_recall: 0.6591523895401262 | global_auc: 0.9837895793476801| flobal_FPR: 0.3408476104598738 \n",
      "client_9\n",
      "comm_round: 12 | global_acc: 81.283% | global_loss: 0.3960418999195099 | global_f1: 0.660229330114665 | global_precision: 0.9981751824817519 | global_recall: 0.4932371505861136 | global_auc: 0.9813731397712525| flobal_FPR: 0.5067628494138864 \n",
      "client_10\n",
      "comm_round: 12 | global_acc: 63.132% | global_loss: 0.457976758480072 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9694970206425384| flobal_FPR: 1.0 \n",
      "[]\n",
      "[[[6, 21.606550354008746], [7, 20.659757268830386], [8, 21.779859399407623]], [[4, 16.4376408356475], [5, 16.159830418688458], [9, 17.20504234573526], [10, 16.50321310295154]], [[1, 19.278234695084382], [2, 18.26250077259695], [3, 19.506159112316507]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 13 | global_acc: 63.132% | global_loss: 0.5821006894111633 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9235979640938636| flobal_FPR: 1.0 \n",
      "client_2\n",
      "comm_round: 13 | global_acc: 63.132% | global_loss: 0.5776115655899048 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9250538107712711| flobal_FPR: 1.0 \n",
      "client_3\n",
      "comm_round: 13 | global_acc: 63.132% | global_loss: 0.6119660139083862 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9108958680260268| flobal_FPR: 1.0 \n",
      "client_4\n",
      "comm_round: 13 | global_acc: 63.132% | global_loss: 0.46926119923591614 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9691209506593332| flobal_FPR: 1.0 \n",
      "client_5\n",
      "comm_round: 13 | global_acc: 63.132% | global_loss: 0.4634854793548584 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9746366437463407| flobal_FPR: 1.0 \n",
      "client_6\n",
      "comm_round: 13 | global_acc: 94.315% | global_loss: 0.2917589545249939 | global_f1: 0.9175108538350217 | global_precision: 0.9865145228215768 | global_recall: 0.8575293056807936 | global_auc: 0.9882074519786647| flobal_FPR: 0.1424706943192065 \n",
      "client_7\n",
      "comm_round: 13 | global_acc: 89.362% | global_loss: 0.3815207779407501 | global_f1: 0.8322851153039832 | global_precision: 0.9937421777221527 | global_recall: 0.7159603246167718 | global_auc: 0.9840459906998653| flobal_FPR: 0.2840396753832281 \n",
      "client_8\n",
      "comm_round: 13 | global_acc: 93.451% | global_loss: 0.31210997700691223 | global_f1: 0.9035731767009301 | global_precision: 0.9882226980728052 | global_recall: 0.8322813345356177 | global_auc: 0.9870445790129208| flobal_FPR: 0.16771866546438233 \n",
      "client_9\n",
      "comm_round: 13 | global_acc: 63.132% | global_loss: 0.41546913981437683 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9799571793041851| flobal_FPR: 1.0 \n",
      "client_10\n",
      "comm_round: 13 | global_acc: 92.620% | global_loss: 0.37288787961006165 | global_f1: 0.8898809523809523 | global_precision: 0.9889746416758545 | global_recall: 0.8088367899008115 | global_auc: 0.9829367741837455| flobal_FPR: 0.19116321009918846 \n",
      "[]\n",
      "[[[5, 16.28801879288244], [7, 16.060264613426757]], [[4, 28.163083653020163], [6, 28.144629240854286]], [[1, 18.703767132232734], [2, 18.949668808763896], [3, 18.178511408495417], [8, 20.986033640787433], [9, 18.31325468156562], [10, 20.425728813105014]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 14 | global_acc: 63.132% | global_loss: 0.4420849084854126 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9756869806186256| flobal_FPR: 1.0 \n",
      "client_2\n",
      "comm_round: 14 | global_acc: 86.104% | global_loss: 0.3866586983203888 | global_f1: 0.7682926829268293 | global_precision: 0.9971223021582734 | global_recall: 0.6248872858431019 | global_auc: 0.9837088572553254| flobal_FPR: 0.3751127141568981 \n",
      "client_3\n",
      "comm_round: 14 | global_acc: 63.132% | global_loss: 0.4428900182247162 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9766701282199212| flobal_FPR: 1.0 \n",
      "client_4\n",
      "comm_round: 14 | global_acc: 88.597% | global_loss: 0.38115018606185913 | global_f1: 0.8174560936668441 | global_precision: 0.9974025974025974 | global_recall: 0.6925157799819658 | global_auc: 0.9829149317352258| flobal_FPR: 0.30748422001803427 \n",
      "client_5\n",
      "comm_round: 14 | global_acc: 90.791% | global_loss: 0.3599492311477661 | global_f1: 0.8581669226830516 | global_precision: 0.9928909952606635 | global_recall: 0.7556357078449053 | global_auc: 0.9856015529031226| flobal_FPR: 0.24436429215509467 \n",
      "client_6\n",
      "comm_round: 14 | global_acc: 63.132% | global_loss: 0.493321031332016 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9603151200551189| flobal_FPR: 1.0 \n",
      "client_7\n",
      "comm_round: 14 | global_acc: 63.132% | global_loss: 0.4140240252017975 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9813465489643594| flobal_FPR: 1.0 \n",
      "client_8\n",
      "comm_round: 14 | global_acc: 63.132% | global_loss: 0.46539485454559326 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9702629308482325| flobal_FPR: 1.0 \n",
      "client_9\n",
      "comm_round: 14 | global_acc: 87.600% | global_loss: 0.37534385919570923 | global_f1: 0.7980508933405522 | global_precision: 0.9986449864498645 | global_recall: 0.6645626690712354 | global_auc: 0.9843384895756915| flobal_FPR: 0.33543733092876465 \n",
      "client_10\n",
      "comm_round: 14 | global_acc: 63.132% | global_loss: 0.4251546263694763 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9766546960552063| flobal_FPR: 1.0 \n",
      "[]\n",
      "[[[7, 15.133545922186974], [8, 13.34369645066644]], [[4, 21.476219026410764], [9, 20.101640782151236], [10, 19.003668540577777]], [[1, 16.733647178140615], [2, 17.111839461347117], [3, 16.399869245430928], [5, 18.162092553128996], [6, 17.530520968900497]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 15 | global_acc: 63.132% | global_loss: 0.47143760323524475 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9635952860197409| flobal_FPR: 1.0 \n",
      "client_2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 209\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m# fit local model with client's data\u001b[39;00m\n\u001b[0;32m    205\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(TensorDataset(torch\u001b[39m.\u001b[39mtensor(clients_batched[client]\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mtensors[\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32),\n\u001b[0;32m    206\u001b[0m                                         torch\u001b[39m.\u001b[39mtensor(clients_batched[client]\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mtensors[\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)),\n\u001b[0;32m    207\u001b[0m                           batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 209\u001b[0m train_model_prox(local_model, train_loader, loss, optimizer,cluster_weights)\n\u001b[0;32m    211\u001b[0m \u001b[39m# # scale the model weights and add to the list\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39m# scaling_factor = weight_scalling_factor(clients_batched, client)\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39m# scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[39m# scaled_local_weight_list.append(scaled_weights)\u001b[39;00m\n\u001b[0;32m    216\u001b[0m cpu_freq_after \u001b[39m=\u001b[39m psutil\u001b[39m.\u001b[39mcpu_freq()\n",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m, in \u001b[0;36mtrain_model_prox\u001b[1;34m(model, train_loader, loss_fn, optimizer, global_parameters, mu)\u001b[0m\n\u001b[0;32m     17\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(random\u001b[39m.\u001b[39mrandint(\u001b[39m5\u001b[39m, \u001b[39m12\u001b[39m)):\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     20\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     21\u001b[0m         outputs \u001b[39m=\u001b[39m model(inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;49;00m samples \u001b[39min\u001b[39;49;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def train_model_prox(model, train_loader, loss_fn, optimizer,global_parameters, mu=0.01):\n",
    "    model.train()\n",
    "    for _ in range(random.randint(5, 12)):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # if global_parameters!= None:\n",
    "            #     # Add the proximal term\n",
    "            #     for param, param_global in zip(model.parameters(), global_parameters):\n",
    "            #         loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example: Sparse binary backdoor pattern for a dataset with binary features\n",
    "num_features = 265\n",
    "num_modified_features = 20\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for _ in range(random.randint(8, 20)):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "K_cluster =3\n",
    "all_avg = []\n",
    "all_std = []\n",
    "target_client = 'client_3'  # Change this to the desired target client\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "            \n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features_column_num = use_data.shape[1] - 1\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, features_column_num)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "    \n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "            all_results = [[],[],[],[],[],[],[],[],[],[]]\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            global_model_list =list()\n",
    "            # smlp_global1 = SimpleLSTM(input_size=X.shape[1], hidden_size=128, num_layers=2, classes=1)\n",
    "            # smlp_global2 = SimpleLSTM(input_size=X.shape[1], hidden_size=128, num_layers=2, classes=1)\n",
    "            # smlp_global3 = SimpleLSTM(input_size=X.shape[1], hidden_size=128, num_layers=2, classes=1)\n",
    "\n",
    "            smlp_global1 = SimpleMLP(X.shape[1], 1)\n",
    "            smlp_global2 = SimpleMLP(X.shape[1], 1)\n",
    "            smlp_global3 = SimpleMLP(X.shape[1], 1)\n",
    "            global_model_list = [smlp_global1,smlp_global2,smlp_global3]\n",
    "\n",
    "\n",
    "            num_modified_features = 20\n",
    "            # Create a sparse binary backdoor pattern\n",
    "            backdoor_pattern = np.zeros(features_column_num)\n",
    "            modified_indices = np.random.choice(features_column_num, num_modified_features, replace=False)\n",
    "            backdoor_pattern[modified_indices] = 1  # Set modified features to 1\n",
    "            error_list_history = [[] for _ in range(10)]\n",
    "\n",
    "            cluster_group =[[],[],[]]    \n",
    "            # -----------------------------------------------\n",
    "\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedProx 2018|')\n",
    "            print('|=======================|')\n",
    "            \n",
    "            list_average_weights=list()\n",
    "            list_clients_weights_old =list()\n",
    "            list_clients_weights_new =list()\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # global_weights_list =list()\n",
    "                # for i in range(K_cluster):\n",
    "                #     global_weights_list.append(param.data.clone() for param in global_model_list[i].parameters())\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "                list_clients_weights_delta =list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                sending_time = random.uniform(1, 3)\n",
    "                UTILL_usage_list = list()\n",
    "                list_similarity_matrix = list()\n",
    "                for client in client_names:\n",
    "\n",
    "                    print(client)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    tracemalloc.start()\n",
    "                    cpu_before = psutil.cpu_percent(interval=None)\n",
    "                    cpu_freq_before = psutil.cpu_freq()\n",
    "                     \n",
    "\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "   \n",
    "                    # set local model weight to the weight of the global model\n",
    "                    # clients_batched = add_backdoor_attack(clients_batched, backdoor_pattern, target_client)\n",
    "\n",
    "                   # Find the location of the element 3\n",
    "                    location = None\n",
    "                    cluster_weights = None\n",
    "                    # print(list_average_weights)\n",
    "\n",
    "                    for i, sublist in enumerate(cluster_group):\n",
    "\n",
    "                        if int(pattern.findall(client)[0]) in sublist:\n",
    "                            location = i\n",
    "                            cluster_weights =list_average_weights[location]\n",
    "                            local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), cluster_weights)})\n",
    "                            break\n",
    "\n",
    "                    # if cluster_weights == None:\n",
    "                    #     cluster_weights = global_weights_list[0]\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model_prox(local_model, train_loader, loss, optimizer,cluster_weights)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "                    # # scale the model weights and add to the list\n",
    "                    # scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    # scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    # scaled_local_weight_list.append(scaled_weights)\n",
    "                    \n",
    "                    cpu_freq_after = psutil.cpu_freq()\n",
    "                    mean_cpu_freq_within_loop = (cpu_freq_before.current + cpu_freq_after.current) / 2\n",
    "                    cpu_after = psutil.cpu_percent(interval=None)\n",
    "                    memorey =tracemalloc.get_traced_memory()\n",
    "                    memorey= memorey[1]-memorey[0]\n",
    "                    tracemalloc.stop()\n",
    "                    num_used_cpus = multiprocessing.cpu_count()\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    recieved_time =random.uniform(1, 5)\n",
    "                    cpu_usage = np.abs(cpu_after-cpu_before)\n",
    "                    if cpu_usage==0:\n",
    "                        cpu_usage=0.01\n",
    "                    UTILL =sending_time + recieved_time+ alpha* elapsed_time +beta *np.log10(memorey) + gamma*np.log10(cpu_usage*mean_cpu_freq_within_loop*num_used_cpus)\n",
    "                    UTILL_usage_list.append([int(pattern.findall(client)[0]),UTILL])\n",
    "                    list_clients_weights_new.append([int(pattern.findall(client)[0]),local_model.state_dict().values()])\n",
    "                    list_clients_weights_delta.append(local_model.state_dict().items())\n",
    "\n",
    "                    # list_clients_weights_delta = \n",
    "                    torch.cuda.empty_cache()\n",
    "                    # print(client)\n",
    "                    # test global model and print out metrics after each communications round\n",
    "                    for X_test_batch, Y_test_batch in test_batched:\n",
    "                        global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, local_model, comm_round)\n",
    "                        all_results[int(pattern.findall(client)[0])-1].append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "                        error_list_history[int(pattern.findall(client)[0])-1].append(global_loss)\n",
    "                \n",
    "                # ...\n",
    "                # Extract the energy values for clustering\n",
    "                \n",
    "                # Sort data based on client number\n",
    "                UTILL_usage_list = sorted(UTILL_usage_list, key=lambda x: x[0])\n",
    "                energy_values = np.array([entry[1] for entry in UTILL_usage_list]).reshape(-1, 1)\n",
    "                list_clients_weights_new = sorted(list_clients_weights_new, key=lambda x: x[0])\n",
    "                list_similarity_matrix = sorted(list_similarity_matrix, key=lambda x: x[0])\n",
    "                print(list_similarity_matrix)\n",
    "                # Apply k-means clustering\n",
    "                try:\n",
    "                    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "                    clusters = kmeans.fit_predict(energy_values)\n",
    "                except:\n",
    "                    print(energy_values)\n",
    "                list_average_weights = list()\n",
    "\n",
    "                # Create separate lists for each cluster\n",
    "                cluster_lists = [[] for _ in range(3)]\n",
    "\n",
    "                # Distribute clients into cluster lists\n",
    "                for i, client_entry in enumerate(UTILL_usage_list):\n",
    "                    cluster_lists[clusters[i]].append(client_entry)\n",
    "                # Print the result\n",
    "                cluster_group= [[],[],[]]\n",
    "                print(cluster_lists)\n",
    "                # print(cluster_lists)\n",
    "                for cluster_idx, cluster_list in enumerate(cluster_lists):\n",
    "                    learning_threshold = cluster_idx / 2\n",
    "                    print(learning_threshold)\n",
    "                    clients_batched_cluster = dict()\n",
    "                    scaled_local_weight_list = list()\n",
    "                    for client_entry in cluster_list:\n",
    "                        name =client_name_base+ '_' +str(client_entry[0]) \n",
    "                        clients_batched_cluster[name] =clients_batched[name]\n",
    "                    for client_entry in cluster_list:\n",
    "                        name = client_name_base+ '_' +str(client_entry[0]) \n",
    "                        scaling_factor = weight_scalling_factor(clients_batched_cluster, name)\n",
    "                        scaled_weights = scale_model_weights(list_clients_weights_new[client_entry[0]-1][1], scaling_factor)\n",
    "                        scaled_local_weight_list.append(scaled_weights)\n",
    "                        cluster_group[cluster_idx].append(client_entry[0])\n",
    "                    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "                    list_average_weights.append(average_weights)\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "            for i in range(len(all_results)):\n",
    "                all_R = pd.DataFrame(all_results[i], columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "                flname = f'results/round-{r}/{cl}-clients/FedAvg-{dataset[d]}-clisel-results_{i}.csv'\n",
    "                all_R.to_csv(flname, index=None)\n",
    "\n",
    "#             all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "#             all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "# ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "# ALL_AVG.to_csv('FedAvg-results.csv')\n",
    "\n",
    "# ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "# ALL_STD.to_csv('FedAvg-all-std-results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb904cc7",
   "metadata": {},
   "source": [
    "## fedprox non idd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b47632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "Client client_1: {(0,): 6, (1,): 1197}\n",
      "Client client_2: {(0,): 978, (1,): 225}\n",
      "Client client_3: {(0,): 810, (1,): 393}\n",
      "Client client_4: {(0,): 367, (1,): 836}\n",
      "Client client_5: {(0,): 601, (1,): 602}\n",
      "Client client_6: {(0,): 757, (1,): 446}\n",
      "Client client_7: {(0,): 465, (1,): 738}\n",
      "Client client_8: {(0,): 813, (1,): 390}\n",
      "Client client_9: {(0,): 597, (1,): 606}\n",
      "Client client_10: {(0,): 842, (1,): 361}\n",
      "|=======================|\n",
      "|Traditional FedProx 2018|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 40.293% | global_loss: 0.6978632211685181 | global_f1: 0.5430025445292621 | global_precision: 0.3782346685572492 | global_recall: 0.9621280432822362 | global_auc: 0.6203537906857153| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 1 | global_acc: 52.128% | global_loss: 0.6901611685752869 | global_f1: 0.5871559633027522 | global_precision: 0.43043295502311896 | global_recall: 0.9233543733092876 | global_auc: 0.7452790633958075| flobal_FPR: 0.07664562669071236 \n",
      "comm_round: 2 | global_acc: 69.382% | global_loss: 0.6820592880249023 | global_f1: 0.6792058516196448 | global_precision: 0.5533484676503972 | global_recall: 0.8791704238052299 | global_auc: 0.8100993783924053| flobal_FPR: 0.12082957619477007 \n",
      "comm_round: 3 | global_acc: 76.064% | global_loss: 0.6730197072029114 | global_f1: 0.7204968944099379 | global_precision: 0.632583503749148 | global_recall: 0.8367899008115419 | global_auc: 0.8505430460054197| flobal_FPR: 0.16321009918845808 \n",
      "comm_round: 4 | global_acc: 79.056% | global_loss: 0.6621758341789246 | global_f1: 0.7381546134663343 | global_precision: 0.6846569005397071 | global_recall: 0.8007213706041478 | global_auc: 0.8768900246962119| flobal_FPR: 0.1992786293958521 \n",
      "comm_round: 5 | global_acc: 79.820% | global_loss: 0.648622453212738 | global_f1: 0.7404873877725524 | global_precision: 0.7040650406504065 | global_recall: 0.7808836789900812 | global_auc: 0.89543521316093| flobal_FPR: 0.21911632100991885 \n",
      "comm_round: 6 | global_acc: 81.017% | global_loss: 0.631080687046051 | global_f1: 0.7556696619597776 | global_precision: 0.7190553745928339 | global_recall: 0.7962128043282236 | global_auc: 0.9117073624721094| flobal_FPR: 0.2037871956717764 \n",
      "comm_round: 7 | global_acc: 82.181% | global_loss: 0.6079218983650208 | global_f1: 0.7707442258340462 | global_precision: 0.7331163547599675 | global_recall: 0.8124436429215509 | global_auc: 0.9246027167257601| flobal_FPR: 0.18755635707844906 \n",
      "comm_round: 8 | global_acc: 84.076% | global_loss: 0.5781079530715942 | global_f1: 0.7989928661351237 | global_precision: 0.7472527472527473 | global_recall: 0.8584310189359784 | global_auc: 0.9372219539399742| flobal_FPR: 0.14156898106402163 \n",
      "comm_round: 9 | global_acc: 84.840% | global_loss: 0.5411466956138611 | global_f1: 0.8117258464079273 | global_precision: 0.7486671744097486 | global_recall: 0.8863841298467088 | global_auc: 0.9495221014714689| flobal_FPR: 0.11361587015329125 \n",
      "comm_round: 10 | global_acc: 85.638% | global_loss: 0.49791473150253296 | global_f1: 0.8229508196721311 | global_precision: 0.754320060105184 | global_recall: 0.9053201082055906 | global_auc: 0.961035920856262| flobal_FPR: 0.09467989179440937 \n",
      "comm_round: 11 | global_acc: 86.835% | global_loss: 0.4509587287902832 | global_f1: 0.8391551584077986 | global_precision: 0.7634885439763488 | global_recall: 0.9314697926059513 | global_auc: 0.9706860095793381| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 12 | global_acc: 87.999% | global_loss: 0.4020342230796814 | global_f1: 0.8529531568228105 | global_precision: 0.7778603268945022 | global_recall: 0.9440937781785392 | global_auc: 0.977060205860329| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 13 | global_acc: 92.287% | global_loss: 0.35332930088043213 | global_f1: 0.9003436426116839 | global_precision: 0.859721082854799 | global_recall: 0.9449954914337241 | global_auc: 0.9811034330156206| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 14 | global_acc: 93.318% | global_loss: 0.30763038992881775 | global_f1: 0.9123419101613607 | global_precision: 0.8834459459459459 | global_recall: 0.9431920649233544 | global_auc: 0.9841585267933244| flobal_FPR: 0.056807935076645624 \n",
      "comm_round: 15 | global_acc: 93.850% | global_loss: 0.2660227119922638 | global_f1: 0.9185380889476001 | global_precision: 0.8975903614457831 | global_recall: 0.9404869251577999 | global_auc: 0.9862929138823481| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 16 | global_acc: 94.781% | global_loss: 0.23177163302898407 | global_f1: 0.9301911960871498 | global_precision: 0.9175438596491228 | global_recall: 0.9431920649233544 | global_auc: 0.9873408765754459| flobal_FPR: 0.056807935076645624 \n",
      "comm_round: 17 | global_acc: 94.980% | global_loss: 0.20357894897460938 | global_f1: 0.9327394209354121 | global_precision: 0.9216549295774648 | global_recall: 0.9440937781785392 | global_auc: 0.9881979552619171| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 18 | global_acc: 94.880% | global_loss: 0.1819087266921997 | global_f1: 0.9313113291703836 | global_precision: 0.9214474845542807 | global_recall: 0.9413886384129847 | global_auc: 0.9888836182110939| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 19 | global_acc: 95.047% | global_loss: 0.16524545848369598 | global_f1: 0.933392936969155 | global_precision: 0.925531914893617 | global_recall: 0.9413886384129847 | global_auc: 0.9897050842097617| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 20 | global_acc: 95.146% | global_loss: 0.15266165137290955 | global_f1: 0.9348795718108831 | global_precision: 0.9249779346866726 | global_recall: 0.9449954914337241 | global_auc: 0.9902986290064867| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 21 | global_acc: 95.445% | global_loss: 0.14256754517555237 | global_f1: 0.9386475593372144 | global_precision: 0.9323843416370107 | global_recall: 0.9449954914337241 | global_auc: 0.9907639681271193| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 22 | global_acc: 95.545% | global_loss: 0.13424000144004822 | global_f1: 0.9399103139013454 | global_precision: 0.9348795718108831 | global_recall: 0.9449954914337241 | global_auc: 0.9910783094514648| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 23 | global_acc: 95.944% | global_loss: 0.12750087678432465 | global_f1: 0.945193171608266 | global_precision: 0.9418084153983886 | global_recall: 0.9486023444544635 | global_auc: 0.9913503903862837| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 24 | global_acc: 96.210% | global_loss: 0.12201910465955734 | global_f1: 0.9487410071942445 | global_precision: 0.9461883408071748 | global_recall: 0.951307484220018 | global_auc: 0.9916362415603865| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 25 | global_acc: 96.210% | global_loss: 0.11761845648288727 | global_f1: 0.9487410071942445 | global_precision: 0.9461883408071748 | global_recall: 0.951307484220018 | global_auc: 0.9918024341034695| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 26 | global_acc: 96.243% | global_loss: 0.11391613632440567 | global_f1: 0.9491677912730543 | global_precision: 0.9470377019748654 | global_recall: 0.951307484220018 | global_auc: 0.9920094625285674| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 27 | global_acc: 96.343% | global_loss: 0.11069890111684799 | global_f1: 0.9504950495049506 | global_precision: 0.9487870619946092 | global_recall: 0.9522091974752029 | global_auc: 0.9921732808924635| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 28 | global_acc: 96.410% | global_loss: 0.10811721533536911 | global_f1: 0.9513513513513514 | global_precision: 0.9504950495049505 | global_recall: 0.9522091974752029 | global_auc: 0.9923532436748305| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 29 | global_acc: 96.376% | global_loss: 0.10619794577360153 | global_f1: 0.9509230076542098 | global_precision: 0.9496402877697842 | global_recall: 0.9522091974752029 | global_auc: 0.9924956944260446| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 30 | global_acc: 96.410% | global_loss: 0.10415706038475037 | global_f1: 0.9513951395139515 | global_precision: 0.949685534591195 | global_recall: 0.9531109107303878 | global_auc: 0.9926044318328046| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 31 | global_acc: 96.509% | global_loss: 0.10203823447227478 | global_f1: 0.952638700947226 | global_precision: 0.9530685920577617 | global_recall: 0.9522091974752029 | global_auc: 0.9927364361955964| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 32 | global_acc: 96.509% | global_loss: 0.10046226531267166 | global_f1: 0.9525530953456846 | global_precision: 0.9547101449275363 | global_recall: 0.9504057709648331 | global_auc: 0.9928418497514947| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 33 | global_acc: 96.576% | global_loss: 0.09929908812046051 | global_f1: 0.9534988713318284 | global_precision: 0.9547920433996383 | global_recall: 0.9522091974752029 | global_auc: 0.9929681560842378| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 34 | global_acc: 96.609% | global_loss: 0.0981154516339302 | global_f1: 0.9539711191335741 | global_precision: 0.95483288166215 | global_recall: 0.9531109107303878 | global_auc: 0.9930560007141531| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 35 | global_acc: 96.609% | global_loss: 0.09717991948127747 | global_f1: 0.953929539295393 | global_precision: 0.9556561085972851 | global_recall: 0.9522091974752029 | global_auc: 0.9931552414041656| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 36 | global_acc: 96.576% | global_loss: 0.09636089950799942 | global_f1: 0.9535408209291836 | global_precision: 0.953971119133574 | global_recall: 0.9531109107303878 | global_auc: 0.9932350138248454| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 37 | global_acc: 96.609% | global_loss: 0.09551827609539032 | global_f1: 0.9539711191335741 | global_precision: 0.95483288166215 | global_recall: 0.9531109107303878 | global_auc: 0.9932829722444207| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 38 | global_acc: 96.609% | global_loss: 0.09495963156223297 | global_f1: 0.9539711191335741 | global_precision: 0.95483288166215 | global_recall: 0.9531109107303878 | global_auc: 0.9933575214708895| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 39 | global_acc: 96.609% | global_loss: 0.09446966648101807 | global_f1: 0.9539711191335741 | global_precision: 0.95483288166215 | global_recall: 0.9531109107303878 | global_auc: 0.9934838278036325| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 40 | global_acc: 96.576% | global_loss: 0.09375156462192535 | global_f1: 0.9535408209291836 | global_precision: 0.953971119133574 | global_recall: 0.9531109107303878 | global_auc: 0.9935640750601499| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 41 | global_acc: 96.642% | global_loss: 0.09294922649860382 | global_f1: 0.9544018058690744 | global_precision: 0.9556962025316456 | global_recall: 0.9531109107303878 | global_auc: 0.9936163070022616| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 42 | global_acc: 96.642% | global_loss: 0.09240669012069702 | global_f1: 0.9543605964753729 | global_precision: 0.9565217391304348 | global_recall: 0.9522091974752029 | global_auc: 0.9936846833628444| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 43 | global_acc: 96.642% | global_loss: 0.09207788109779358 | global_f1: 0.9543605964753729 | global_precision: 0.9565217391304348 | global_recall: 0.9522091974752029 | global_auc: 0.9937364404691189| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 44 | global_acc: 96.676% | global_loss: 0.09188410639762878 | global_f1: 0.95483288166215 | global_precision: 0.9565610859728507 | global_recall: 0.9531109107303878 | global_auc: 0.9937962697846286| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 45 | global_acc: 96.676% | global_loss: 0.09144190698862076 | global_f1: 0.95483288166215 | global_precision: 0.9565610859728507 | global_recall: 0.9531109107303878 | global_auc: 0.9938518255776021| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 46 | global_acc: 96.676% | global_loss: 0.09085413068532944 | global_f1: 0.9547920433996383 | global_precision: 0.957388939256573 | global_recall: 0.9522091974752029 | global_auc: 0.9938869634295684| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 47 | global_acc: 96.709% | global_loss: 0.0909595862030983 | global_f1: 0.9552643470402169 | global_precision: 0.957427536231884 | global_recall: 0.9531109107303878 | global_auc: 0.9939406198791922| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 48 | global_acc: 96.709% | global_loss: 0.09091772139072418 | global_f1: 0.9552643470402169 | global_precision: 0.957427536231884 | global_recall: 0.9531109107303878 | global_auc: 0.9939828802687192| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 49 | global_acc: 96.676% | global_loss: 0.09003544598817825 | global_f1: 0.9547920433996383 | global_precision: 0.957388939256573 | global_recall: 0.9522091974752029 | global_auc: 0.9939957008363284| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 50 | global_acc: 96.709% | global_loss: 0.08955671638250351 | global_f1: 0.955223880597015 | global_precision: 0.9582577132486388 | global_recall: 0.9522091974752029 | global_auc: 0.9940232413148963| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 51 | global_acc: 96.676% | global_loss: 0.09016560763120651 | global_f1: 0.95483288166215 | global_precision: 0.9565610859728507 | global_recall: 0.9531109107303878 | global_auc: 0.9940783222720325| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 52 | global_acc: 96.742% | global_loss: 0.0899265855550766 | global_f1: 0.9557362240289069 | global_precision: 0.9574660633484163 | global_recall: 0.9540126239855726 | global_auc: 0.994140050930892| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 53 | global_acc: 96.742% | global_loss: 0.0897253081202507 | global_f1: 0.9557362240289069 | global_precision: 0.9574660633484163 | global_recall: 0.9540126239855726 | global_auc: 0.9941713900961591| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 54 | global_acc: 96.775% | global_loss: 0.0890984833240509 | global_f1: 0.9561680976050609 | global_precision: 0.9583333333333334 | global_recall: 0.9540126239855726 | global_auc: 0.9941580946927123| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 55 | global_acc: 96.775% | global_loss: 0.08916162699460983 | global_f1: 0.9561680976050609 | global_precision: 0.9583333333333334 | global_recall: 0.9540126239855726 | global_auc: 0.9941837358279308| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 56 | global_acc: 96.775% | global_loss: 0.08958202600479126 | global_f1: 0.9562471808750563 | global_precision: 0.9566787003610109 | global_recall: 0.9558160504959423 | global_auc: 0.9942141253215233| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 57 | global_acc: 96.842% | global_loss: 0.08909104019403458 | global_f1: 0.9571106094808128 | global_precision: 0.9584086799276673 | global_recall: 0.9558160504959423 | global_auc: 0.9942146001573606| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 58 | global_acc: 96.842% | global_loss: 0.0886472761631012 | global_f1: 0.9571106094808128 | global_precision: 0.9584086799276673 | global_recall: 0.9558160504959423 | global_auc: 0.9942217226949213| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 59 | global_acc: 96.842% | global_loss: 0.08898452669382095 | global_f1: 0.9571106094808128 | global_precision: 0.9584086799276673 | global_recall: 0.9558160504959423 | global_auc: 0.9942502128451641| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 60 | global_acc: 96.842% | global_loss: 0.08854187279939651 | global_f1: 0.9571106094808128 | global_precision: 0.9584086799276673 | global_recall: 0.9558160504959423 | global_auc: 0.9942649327561228| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 61 | global_acc: 96.908% | global_loss: 0.08881168067455292 | global_f1: 0.9580514208389717 | global_precision: 0.9584837545126353 | global_recall: 0.957619477006312 | global_auc: 0.9942777533237321| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 62 | global_acc: 96.908% | global_loss: 0.08857796341180801 | global_f1: 0.9580514208389717 | global_precision: 0.9584837545126353 | global_recall: 0.957619477006312 | global_auc: 0.9943119415040235| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 63 | global_acc: 96.908% | global_loss: 0.08878421783447266 | global_f1: 0.9580892293826048 | global_precision: 0.9576576576576576 | global_recall: 0.9585211902614968 | global_auc: 0.9943185892057468| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 64 | global_acc: 96.908% | global_loss: 0.08859418332576752 | global_f1: 0.9580892293826048 | global_precision: 0.9576576576576576 | global_recall: 0.9585211902614968 | global_auc: 0.9943380574750794| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 65 | global_acc: 96.875% | global_loss: 0.08877632766962051 | global_f1: 0.9576576576576576 | global_precision: 0.9567956795679567 | global_recall: 0.9585211902614968 | global_auc: 0.9943257117433076| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 66 | global_acc: 96.941% | global_loss: 0.08892667293548584 | global_f1: 0.9585585585585585 | global_precision: 0.9576957695769577 | global_recall: 0.9594229035166817 | global_auc: 0.9943485038635017| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 67 | global_acc: 97.008% | global_loss: 0.08875413984060287 | global_f1: 0.9594594594594594 | global_precision: 0.9585958595859586 | global_recall: 0.9603246167718665 | global_auc: 0.9943494535351766| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 68 | global_acc: 96.941% | global_loss: 0.08851204812526703 | global_f1: 0.9585585585585585 | global_precision: 0.9576957695769577 | global_recall: 0.9594229035166817 | global_auc: 0.994355151565225| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 69 | global_acc: 97.074% | global_loss: 0.08896404504776001 | global_f1: 0.960431654676259 | global_precision: 0.957847533632287 | global_recall: 0.9630297565374211 | global_auc: 0.9943893397455166| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 70 | global_acc: 97.041% | global_loss: 0.08914877474308014 | global_f1: 0.96 | global_precision: 0.956989247311828 | global_recall: 0.9630297565374211 | global_auc: 0.9944016854772885| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 71 | global_acc: 97.041% | global_loss: 0.08919905871152878 | global_f1: 0.96 | global_precision: 0.956989247311828 | global_recall: 0.9630297565374211 | global_auc: 0.9944107073581986| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 72 | global_acc: 97.041% | global_loss: 0.08935078978538513 | global_f1: 0.96 | global_precision: 0.956989247311828 | global_recall: 0.9630297565374211 | global_auc: 0.9944202040749461| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 73 | global_acc: 97.074% | global_loss: 0.08897389471530914 | global_f1: 0.960431654676259 | global_precision: 0.957847533632287 | global_recall: 0.9630297565374211 | global_auc: 0.9944187795674342| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 74 | global_acc: 97.041% | global_loss: 0.08949171006679535 | global_f1: 0.9600359227660531 | global_precision: 0.9561717352415027 | global_recall: 0.9639314697926059 | global_auc: 0.9944463200460021| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 75 | global_acc: 96.975% | global_loss: 0.08945586532354355 | global_f1: 0.9591011235955056 | global_precision: 0.9560931899641577 | global_recall: 0.9621280432822362 | global_auc: 0.9944515432402133| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 76 | global_acc: 96.975% | global_loss: 0.08943072706460953 | global_f1: 0.9591011235955056 | global_precision: 0.9560931899641577 | global_recall: 0.9621280432822362 | global_auc: 0.9944406220159535| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 77 | global_acc: 97.008% | global_loss: 0.08948964625597 | global_f1: 0.9595323741007195 | global_precision: 0.95695067264574 | global_recall: 0.9621280432822362 | global_auc: 0.9944520180760507| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 78 | global_acc: 97.008% | global_loss: 0.08970987051725388 | global_f1: 0.9595323741007195 | global_precision: 0.95695067264574 | global_recall: 0.9621280432822362 | global_auc: 0.9944596154494487| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 79 | global_acc: 97.008% | global_loss: 0.08965525031089783 | global_f1: 0.9595323741007195 | global_precision: 0.95695067264574 | global_recall: 0.9621280432822362 | global_auc: 0.9944600902852861| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 80 | global_acc: 97.008% | global_loss: 0.08978056162595749 | global_f1: 0.9595323741007195 | global_precision: 0.95695067264574 | global_recall: 0.9621280432822362 | global_auc: 0.9944491690610263| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 81 | global_acc: 96.809% | global_loss: 0.09050742536783218 | global_f1: 0.9569120287253142 | global_precision: 0.9526362823949955 | global_recall: 0.9612263300270514 | global_auc: 0.9944382478367666| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 82 | global_acc: 96.975% | global_loss: 0.09001866728067398 | global_f1: 0.9590643274853802 | global_precision: 0.9569120287253142 | global_recall: 0.9612263300270514 | global_auc: 0.9944562915985871| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 83 | global_acc: 96.975% | global_loss: 0.09027722477912903 | global_f1: 0.9590643274853802 | global_precision: 0.9569120287253142 | global_recall: 0.9612263300270514 | global_auc: 0.9944382478367666| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 84 | global_acc: 96.875% | global_loss: 0.0906435176730156 | global_f1: 0.9578475336322869 | global_precision: 0.9527207850133809 | global_recall: 0.9630297565374211 | global_auc: 0.9944591406136113| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 85 | global_acc: 96.908% | global_loss: 0.09056797623634338 | global_f1: 0.9582772543741588 | global_precision: 0.9535714285714286 | global_recall: 0.9630297565374211 | global_auc: 0.9944520180760507| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 86 | global_acc: 96.875% | global_loss: 0.09119976311922073 | global_f1: 0.9578475336322869 | global_precision: 0.9527207850133809 | global_recall: 0.9630297565374211 | global_auc: 0.9944743353604076| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 87 | global_acc: 96.875% | global_loss: 0.09136107563972473 | global_f1: 0.9578475336322869 | global_precision: 0.9527207850133809 | global_recall: 0.9630297565374211 | global_auc: 0.994480983062131| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 88 | global_acc: 96.875% | global_loss: 0.0915374681353569 | global_f1: 0.9578475336322869 | global_precision: 0.9527207850133809 | global_recall: 0.9630297565374211 | global_auc: 0.9944790837187814| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 89 | global_acc: 96.875% | global_loss: 0.09187585860490799 | global_f1: 0.9578475336322869 | global_precision: 0.9527207850133809 | global_recall: 0.9630297565374211 | global_auc: 0.9944933287939027| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 90 | global_acc: 96.875% | global_loss: 0.0917397066950798 | global_f1: 0.9578475336322869 | global_precision: 0.9527207850133809 | global_recall: 0.9630297565374211 | global_auc: 0.9944833572413176| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 91 | global_acc: 96.908% | global_loss: 0.09139128774404526 | global_f1: 0.9582772543741588 | global_precision: 0.9535714285714286 | global_recall: 0.9630297565374211 | global_auc: 0.9944691121661964| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 92 | global_acc: 96.842% | global_loss: 0.09247473627328873 | global_f1: 0.9574563367666815 | global_precision: 0.951067615658363 | global_recall: 0.9639314697926059 | global_auc: 0.9945016384210569| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 93 | global_acc: 96.908% | global_loss: 0.09183191508054733 | global_f1: 0.9582772543741588 | global_precision: 0.9535714285714286 | global_recall: 0.9630297565374211 | global_auc: 0.9944790837187814| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 94 | global_acc: 96.908% | global_loss: 0.09235925227403641 | global_f1: 0.9583146571044374 | global_precision: 0.9527629233511586 | global_recall: 0.9639314697926059 | global_auc: 0.994486206256342| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 95 | global_acc: 96.908% | global_loss: 0.0924108549952507 | global_f1: 0.9583146571044374 | global_precision: 0.9527629233511586 | global_recall: 0.9639314697926059 | global_auc: 0.9944881055996915| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 96 | global_acc: 96.875% | global_loss: 0.09269677847623825 | global_f1: 0.9578853046594982 | global_precision: 0.9519145146927872 | global_recall: 0.9639314697926059 | global_auc: 0.9944895301072036| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 97 | global_acc: 96.875% | global_loss: 0.09285452216863632 | global_f1: 0.9578853046594982 | global_precision: 0.9519145146927872 | global_recall: 0.9639314697926059 | global_auc: 0.9944876307638543| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 98 | global_acc: 96.875% | global_loss: 0.09286542981863022 | global_f1: 0.9578853046594982 | global_precision: 0.9519145146927872 | global_recall: 0.9639314697926059 | global_auc: 0.9944942784655774| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 99 | global_acc: 96.908% | global_loss: 0.09295301884412766 | global_f1: 0.9583146571044374 | global_precision: 0.9527629233511586 | global_recall: 0.9639314697926059 | global_auc: 0.9944909546147158| flobal_FPR: 0.03606853020739405 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, classes):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        out, _ = self.lstm(x.unsqueeze(1))  # Add an extra dimension for sequence_length (1)\n",
    "        \n",
    "        # Extract the output from the last time step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Example: Sparse binary backdoor pattern for a dataset with binary features\n",
    "num_features = 265\n",
    "num_modified_features = 20\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for _ in range(random.randint(1, 20)):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "K_cluster =3\n",
    "all_avg = []\n",
    "all_std = []\n",
    "target_client = 'client_3'  # Change this to the desired target client\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "            \n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features_column_num = use_data.shape[1] - 1\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, features_column_num)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "    \n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "            all_results = [[],[],[],[],[],[],[],[],[],[]]\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            global_model_list =list()\n",
    "            # smlp_global1 = SimpleLSTM(input_size=X.shape[1], hidden_size=128, num_layers=2, classes=1)\n",
    "            # smlp_global2 = SimpleLSTM(input_size=X.shape[1], hidden_size=128, num_layers=2, classes=1)\n",
    "            # smlp_global3 = SimpleLSTM(input_size=X.shape[1], hidden_size=128, num_layers=2, classes=1)\n",
    "\n",
    "            smlp_global1 = SimpleMLP(X.shape[1], 1)\n",
    "            smlp_global2 = SimpleMLP(X.shape[1], 1)\n",
    "            smlp_global3 = SimpleMLP(X.shape[1], 1)\n",
    "            global_model_list = [smlp_global1,smlp_global2,smlp_global3]\n",
    "\n",
    "\n",
    "            num_modified_features = 20\n",
    "            # Create a sparse binary backdoor pattern\n",
    "            backdoor_pattern = np.zeros(features_column_num)\n",
    "            modified_indices = np.random.choice(features_column_num, num_modified_features, replace=False)\n",
    "            backdoor_pattern[modified_indices] = 1  # Set modified features to 1\n",
    "            error_list_history = [[] for _ in range(10)]\n",
    "\n",
    "            cluster_group =[[],[],[]]    \n",
    "            # -----------------------------------------------\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "            \n",
    "            list_average_weights=list()\n",
    "            list_clients_weights_old =list()\n",
    "            list_clients_weights_new =list()\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # global_weights_list =list()\n",
    "                # for i in range(K_cluster):\n",
    "                #     global_weights_list.append(param.data.clone() for param in global_model_list[i].parameters())\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "                list_clients_weights_delta =list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                sending_time = random.uniform(1, 3)\n",
    "                UTILL_usage_list = list()\n",
    "                list_similarity_matrix = list()\n",
    "                for client in client_names:\n",
    "\n",
    "                    print(client)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    tracemalloc.start()\n",
    "                    cpu_before = psutil.cpu_percent(interval=None)\n",
    "                    cpu_freq_before = psutil.cpu_freq()\n",
    "                     \n",
    "\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "   \n",
    "                    # set local model weight to the weight of the global model\n",
    "                    # clients_batched = add_backdoor_attack(clients_batched, backdoor_pattern, target_client)\n",
    "\n",
    "                   # Find the location of the element 3\n",
    "                    location = None\n",
    "                    cluster_weights = None\n",
    "                    # print(list_average_weights)\n",
    "\n",
    "                    for i, sublist in enumerate(cluster_group):\n",
    "\n",
    "                        if int(pattern.findall(client)[0]) in sublist:\n",
    "                            location = i\n",
    "                            cluster_weights =list_average_weights[location]\n",
    "                            local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), cluster_weights)})\n",
    "                            break\n",
    "\n",
    "                    # if cluster_weights == None:\n",
    "                    #     cluster_weights = global_weights_list[0]\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "                    # # scale the model weights and add to the list\n",
    "                    # scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    # scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    # scaled_local_weight_list.append(scaled_weights)\n",
    "                    \n",
    "                    cpu_freq_after = psutil.cpu_freq()\n",
    "                    mean_cpu_freq_within_loop = (cpu_freq_before.current + cpu_freq_after.current) / 2\n",
    "                    cpu_after = psutil.cpu_percent(interval=None)\n",
    "                    memorey =tracemalloc.get_traced_memory()\n",
    "                    memorey= memorey[1]-memorey[0]\n",
    "                    tracemalloc.stop()\n",
    "                    num_used_cpus = multiprocessing.cpu_count()\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    recieved_time =random.uniform(1, 5)\n",
    "                    cpu_usage = np.abs(cpu_after-cpu_before)\n",
    "                    if cpu_usage==0:\n",
    "                        cpu_usage=0.01\n",
    "                    UTILL =sending_time + recieved_time+ alpha* elapsed_time +beta *np.log10(memorey) + gamma*np.log10(cpu_usage*mean_cpu_freq_within_loop*num_used_cpus)\n",
    "                    UTILL_usage_list.append([int(pattern.findall(client)[0]),UTILL])\n",
    "                    list_clients_weights_new.append([int(pattern.findall(client)[0]),local_model.state_dict().values()])\n",
    "                    list_clients_weights_delta.append(local_model.state_dict().items())\n",
    "\n",
    "                    # list_clients_weights_delta = \n",
    "                    torch.cuda.empty_cache()\n",
    "                    # print(client)\n",
    "                    # test global model and print out metrics after each communications round\n",
    "                    for X_test_batch, Y_test_batch in test_batched:\n",
    "                        global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, local_model, comm_round)\n",
    "                        all_results[int(pattern.findall(client)[0])-1].append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "                        error_list_history[int(pattern.findall(client)[0])-1].append(global_loss)\n",
    "                \n",
    "                # ...\n",
    "                # Extract the energy values for clustering\n",
    "                \n",
    "                # Sort data based on client number\n",
    "                UTILL_usage_list = sorted(UTILL_usage_list, key=lambda x: x[0])\n",
    "                energy_values = np.array([entry[1] for entry in UTILL_usage_list]).reshape(-1, 1)\n",
    "                list_clients_weights_new = sorted(list_clients_weights_new, key=lambda x: x[0])\n",
    "                list_similarity_matrix = sorted(list_similarity_matrix, key=lambda x: x[0])\n",
    "                print(list_similarity_matrix)\n",
    "                # Apply k-means clustering\n",
    "                try:\n",
    "                    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "                    clusters = kmeans.fit_predict(energy_values)\n",
    "                except:\n",
    "                    print(energy_values)\n",
    "                list_average_weights = list()\n",
    "\n",
    "                # Create separate lists for each cluster\n",
    "                cluster_lists = [[] for _ in range(3)]\n",
    "\n",
    "                # Distribute clients into cluster lists\n",
    "                for i, client_entry in enumerate(UTILL_usage_list):\n",
    "                    cluster_lists[clusters[i]].append(client_entry)\n",
    "                # Print the result\n",
    "                cluster_group= [[],[],[]]\n",
    "                print(cluster_lists)\n",
    "                # print(cluster_lists)\n",
    "                for cluster_idx, cluster_list in enumerate(cluster_lists):\n",
    "                    learning_threshold = cluster_idx / 2\n",
    "                    print(learning_threshold)\n",
    "                    clients_batched_cluster = dict()\n",
    "                    scaled_local_weight_list = list()\n",
    "                    for client_entry in cluster_list:\n",
    "                        name =client_name_base+ '_' +str(client_entry[0]) \n",
    "                        clients_batched_cluster[name] =clients_batched[name]\n",
    "                    for client_entry in cluster_list:\n",
    "                        name = client_name_base+ '_' +str(client_entry[0]) \n",
    "                        scaling_factor = weight_scalling_factor(clients_batched_cluster, name)\n",
    "                        scaled_weights = scale_model_weights(list_clients_weights_new[client_entry[0]-1][1], scaling_factor)\n",
    "                        scaled_local_weight_list.append(scaled_weights)\n",
    "                        cluster_group[cluster_idx].append(client_entry[0])\n",
    "                    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "                    list_average_weights.append(average_weights)\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "            for i in range(len(all_results)):\n",
    "                all_R = pd.DataFrame(all_results[i], columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "                flname = f'results/round-{r}/{cl}-clients/FedAvg-{dataset[d]}-clisel-results_{i}.csv'\n",
    "                all_R.to_csv(flname, index=None)\n",
    "\n",
    "#             all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "#             all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "# ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "# ALL_AVG.to_csv('FedAvg-results.csv')\n",
    "\n",
    "# ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "# ALL_STD.to_csv('FedAvg-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
