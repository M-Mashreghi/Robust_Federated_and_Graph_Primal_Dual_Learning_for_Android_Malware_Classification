{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7796121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "import re\n",
    "import tracemalloc\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing\n",
    "# Define a regular expression pattern to match the number\n",
    "pattern = re.compile(r'\\d+')\n",
    "\n",
    "# Use findall to extract all the numbers in the string\n",
    "alpha =5\n",
    "beta = 1\n",
    "gamma =1\n",
    "\n",
    "client_name_base ='client'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "import psutil\n",
    "from sklearn.cluster import KMeans\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce5a9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_backdoor_attack(client_data, backdoor_pattern, target_client):\n",
    "    \"\"\"\n",
    "    Add a backdoor attack to the specified client's data.\n",
    "    \"\"\"\n",
    "    if target_client not in client_data:\n",
    "        # Target client not found, no backdoor attack applied\n",
    "        return client_data\n",
    "\n",
    "    # Access the DataLoader object\n",
    "    data_loader = client_data[target_client]\n",
    "\n",
    "    # Extract the dataset from the DataLoader\n",
    "    target_client_data = data_loader.dataset\n",
    "\n",
    "    data_size = len(target_client_data)\n",
    "    num_poisoned = int(data_size * 0.1)  # Adjust the fraction of data to be poisoned\n",
    "\n",
    "    # Randomly select indices for poisoning\n",
    "    poisoned_indices = random.sample(range(data_size), num_poisoned)\n",
    "\n",
    "    # Create a new dataset with the modified values\n",
    "    modified_data = []\n",
    "    for idx in range(data_size):\n",
    "        x, y = target_client_data[idx]\n",
    "        if idx in poisoned_indices:\n",
    "            # Convert the NumPy array to a PyTorch tensor\n",
    "            backdoor_tensor = torch.tensor(backdoor_pattern, dtype=torch.float32)\n",
    "            # Modify the last few features to be the backdoor pattern\n",
    "            x[-len(backdoor_pattern):] = backdoor_tensor\n",
    "            # Change the label to the targeted label (e.g., malicious)\n",
    "            y = torch.tensor([1], dtype=torch.float32)  # Assuming 1 represents a malicious label\n",
    "        modified_data.append((x, y))\n",
    "\n",
    "    # Convert lists to tensors before creating TensorDataset\n",
    "    modified_data_tensors = [torch.stack(t) if not isinstance(t, torch.Tensor) else t.unsqueeze(0) for t in zip(*modified_data)]\n",
    "    modified_dataset = torch.utils.data.TensorDataset(*modified_data_tensors)\n",
    "    # Create a new DataLoader with the modified dataset\n",
    "    modified_data_loader = torch.utils.data.DataLoader(\n",
    "        modified_dataset, batch_size=data_loader.batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Replace the original dataset in the client_data dictionary\n",
    "    client_data[target_client] = modified_data_loader\n",
    "\n",
    "    return client_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b0c136",
   "metadata": {},
   "source": [
    "## fedavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02ee67b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 30\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "client_1\n",
      "comm_round: 0 | global_acc: 95.612% | global_loss: 0.11836297810077667 | global_f1: 0.94 | global_precision: 0.9477543538038496 | global_recall: 0.9323715058611362 | global_auc: 0.9904206618166934| flobal_FPR: 0.06762849413886383 \n",
      "client_2\n",
      "comm_round: 0 | global_acc: 95.445% | global_loss: 0.1490548998117447 | global_f1: 0.9374714742126883 | global_precision: 0.9491682070240296 | global_recall: 0.9260595130748422 | global_auc: 0.9884334738372577| flobal_FPR: 0.0739404869251578 \n",
      "client_3\n",
      "comm_round: 0 | global_acc: 95.080% | global_loss: 0.19161899387836456 | global_f1: 0.9324200913242009 | global_precision: 0.9444958371877891 | global_recall: 0.9206492335437331 | global_auc: 0.9869168481726655| flobal_FPR: 0.0793507664562669 \n",
      "client_4\n",
      "comm_round: 0 | global_acc: 94.515% | global_loss: 0.24575845897197723 | global_f1: 0.9216152019002375 | global_precision: 0.9738955823293173 | global_recall: 0.8746618575293057 | global_auc: 0.987392158845883| flobal_FPR: 0.12533814247069433 \n",
      "client_5\n",
      "comm_round: 0 | global_acc: 96.177% | global_loss: 0.1145414263010025 | global_f1: 0.9477035015916325 | global_precision: 0.9559633027522936 | global_recall: 0.939585211902615 | global_auc: 0.9909643488504937| flobal_FPR: 0.060414788097385035 \n",
      "client_6\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6454980373382568 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7937707236165776| flobal_FPR: 1.0 \n",
      "client_7\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.5666836500167847 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9273463181941424| flobal_FPR: 1.0 \n",
      "client_8\n",
      "comm_round: 0 | global_acc: 96.144% | global_loss: 0.09789791703224182 | global_f1: 0.9477477477477476 | global_precision: 0.946894689468947 | global_recall: 0.9486023444544635 | global_auc: 0.9929667315767257| flobal_FPR: 0.05139765554553652 \n",
      "client_9\n",
      "comm_round: 0 | global_acc: 81.416% | global_loss: 0.4281098246574402 | global_f1: 0.6646670665866826 | global_precision: 0.992831541218638 | global_recall: 0.4995491433724076 | global_auc: 0.9679129682890383| flobal_FPR: 0.5004508566275925 \n",
      "client_10\n",
      "comm_round: 0 | global_acc: 96.243% | global_loss: 0.1118398904800415 | global_f1: 0.9487528344671202 | global_precision: 0.9543795620437956 | global_recall: 0.9431920649233544 | global_auc: 0.9917183881602533| flobal_FPR: 0.056807935076645624 \n",
      "[]\n",
      "[[[1, 37.279518184095764], [8, 33.76739350578083]], [[2, 27.705312509472606], [3, 23.011821332131426], [4, 23.55531682166473], [5, 28.364782450743878], [9, 21.97567470710538], [10, 27.810503470783893]], [[6, 14.875757286449884], [7, 17.81222411770673]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 1 | global_acc: 97.141% | global_loss: 0.08950646221637726 | global_f1: 0.9608735213830756 | global_precision: 0.9696969696969697 | global_recall: 0.9522091974752029 | global_auc: 0.9934282720106592| flobal_FPR: 0.047790802524797116 \n",
      "client_2\n",
      "comm_round: 1 | global_acc: 97.307% | global_loss: 0.08609995245933533 | global_f1: 0.9631650750341064 | global_precision: 0.9715596330275229 | global_recall: 0.9549143372407575 | global_auc: 0.9944111821940359| flobal_FPR: 0.04508566275924256 \n",
      "client_3\n",
      "comm_round: 1 | global_acc: 97.307% | global_loss: 0.08636973798274994 | global_f1: 0.9633318243549117 | global_precision: 0.9672727272727273 | global_recall: 0.9594229035166817 | global_auc: 0.9942995957722517| flobal_FPR: 0.0405770964833183 \n",
      "client_4\n",
      "comm_round: 1 | global_acc: 97.241% | global_loss: 0.08748458325862885 | global_f1: 0.9619440623567171 | global_precision: 0.9785447761194029 | global_recall: 0.9458972046889089 | global_auc: 0.9942003550822393| flobal_FPR: 0.05410279531109107 \n",
      "client_5\n",
      "comm_round: 1 | global_acc: 97.307% | global_loss: 0.08635537326335907 | global_f1: 0.9634970707525913 | global_precision: 0.9630630630630631 | global_recall: 0.9639314697926059 | global_auc: 0.9941884841863046| flobal_FPR: 0.03606853020739405 \n",
      "client_6\n",
      "comm_round: 1 | global_acc: 96.775% | global_loss: 0.09406031668186188 | global_f1: 0.9560887279311906 | global_precision: 0.96 | global_recall: 0.9522091974752029 | global_auc: 0.9933010160062412| flobal_FPR: 0.047790802524797116 \n",
      "client_7\n",
      "comm_round: 1 | global_acc: 97.241% | global_loss: 0.08772923052310944 | global_f1: 0.9622212107419208 | global_precision: 0.9715073529411765 | global_recall: 0.9531109107303878 | global_auc: 0.9940968408696904| flobal_FPR: 0.046889089269612265 \n",
      "client_8\n",
      "comm_round: 1 | global_acc: 96.543% | global_loss: 0.09245255589485168 | global_f1: 0.9527272727272728 | global_precision: 0.9605866177818515 | global_recall: 0.9449954914337241 | global_auc: 0.9933033901854282| flobal_FPR: 0.05500450856627592 \n",
      "client_9\n",
      "comm_round: 1 | global_acc: 97.041% | global_loss: 0.0855163037776947 | global_f1: 0.959563834620627 | global_precision: 0.967032967032967 | global_recall: 0.9522091974752029 | global_auc: 0.9940550553160009| flobal_FPR: 0.047790802524797116 \n",
      "client_10\n",
      "comm_round: 1 | global_acc: 96.908% | global_loss: 0.09099438041448593 | global_f1: 0.9578995020371208 | global_precision: 0.9618181818181818 | global_recall: 0.9540126239855726 | global_auc: 0.9935821188219703| flobal_FPR: 0.045987376014427414 \n",
      "[]\n",
      "[[[1, 35.195798853461916], [3, 38.56740597373782], [4, 40.48764594694518], [5, 42.84738381393078], [7, 40.276841937427875], [9, 42.825519207181756], [10, 39.24461453997147]], [[6, 19.51037476578315], [8, 25.636616355544042]], [[2, 58.96334258605671]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 2 | global_acc: 97.440% | global_loss: 0.08800888061523438 | global_f1: 0.9651110104213865 | global_precision: 0.9699453551912568 | global_recall: 0.9603246167718665 | global_auc: 0.9944244775974825| flobal_FPR: 0.03967538322813345 \n",
      "client_2\n",
      "comm_round: 2 | global_acc: 97.640% | global_loss: 0.08101323992013931 | global_f1: 0.967741935483871 | global_precision: 0.9752747252747253 | global_recall: 0.9603246167718665 | global_auc: 0.9955090026500587| flobal_FPR: 0.03967538322813345 \n",
      "client_3\n",
      "comm_round: 2 | global_acc: 97.507% | global_loss: 0.08776132762432098 | global_f1: 0.9659863945578231 | global_precision: 0.9717153284671532 | global_recall: 0.9603246167718665 | global_auc: 0.9948043462673867| flobal_FPR: 0.03967538322813345 \n",
      "client_4\n",
      "comm_round: 2 | global_acc: 97.407% | global_loss: 0.0855339765548706 | global_f1: 0.96448087431694 | global_precision: 0.9742410303587856 | global_recall: 0.9549143372407575 | global_auc: 0.9948575278811732| flobal_FPR: 0.04508566275924256 \n",
      "client_5\n",
      "comm_round: 2 | global_acc: 97.374% | global_loss: 0.08503922075033188 | global_f1: 0.964301852688658 | global_precision: 0.9664855072463768 | global_recall: 0.9621280432822362 | global_auc: 0.9948375847760031| flobal_FPR: 0.03787195671776375 \n",
      "client_6\n",
      "comm_round: 2 | global_acc: 97.773% | global_loss: 0.07919201999902725 | global_f1: 0.9698605488079172 | global_precision: 0.9676840215439856 | global_recall: 0.9720468890892696 | global_auc: 0.9953713002572185| flobal_FPR: 0.027953110910730387 \n",
      "client_7\n",
      "comm_round: 2 | global_acc: 97.241% | global_loss: 0.09274137765169144 | global_f1: 0.9622898682417084 | global_precision: 0.9697802197802198 | global_recall: 0.9549143372407575 | global_auc: 0.994332359445031| flobal_FPR: 0.04508566275924256 \n",
      "client_8\n",
      "comm_round: 2 | global_acc: 97.573% | global_loss: 0.08755061030387878 | global_f1: 0.9670428893905192 | global_precision: 0.9683544303797469 | global_recall: 0.9657348963029756 | global_auc: 0.9943698714761837| flobal_FPR: 0.034265103697024346 \n",
      "client_9\n",
      "comm_round: 2 | global_acc: 97.374% | global_loss: 0.08530318737030029 | global_f1: 0.9643662607126747 | global_precision: 0.9648014440433214 | global_recall: 0.9639314697926059 | global_auc: 0.9946927598456023| flobal_FPR: 0.03606853020739405 \n",
      "client_10\n",
      "comm_round: 2 | global_acc: 97.207% | global_loss: 0.09561780840158463 | global_f1: 0.9620938628158845 | global_precision: 0.9629629629629629 | global_recall: 0.9612263300270514 | global_auc: 0.9944045344923126| flobal_FPR: 0.0387736699729486 \n",
      "[]\n",
      "[[[7, 69.132371865379], [8, 65.26552569985863]], [[2, 23.57983007897828], [3, 24.157423272836542], [4, 29.299987411418325]], [[1, 49.11265993767531], [5, 43.658811008868845], [6, 50.0298761083795], [9, 39.172743231071905], [10, 43.337493123712306]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 3 | global_acc: 97.473% | global_loss: 0.11337248980998993 | global_f1: 0.9655797101449276 | global_precision: 0.9699727024567789 | global_recall: 0.9612263300270514 | global_auc: 0.9938337818157819| flobal_FPR: 0.0387736699729486 \n",
      "client_2\n",
      "comm_round: 3 | global_acc: 97.573% | global_loss: 0.10073160380125046 | global_f1: 0.9669234254644313 | global_precision: 0.9717668488160291 | global_recall: 0.9621280432822362 | global_auc: 0.9954895343807262| flobal_FPR: 0.03787195671776375 \n",
      "client_3\n",
      "comm_round: 3 | global_acc: 97.540% | global_loss: 0.10709571093320847 | global_f1: 0.9665461121157324 | global_precision: 0.9691749773345422 | global_recall: 0.9639314697926059 | global_auc: 0.9945303659892184| flobal_FPR: 0.03606853020739405 \n",
      "client_4\n",
      "comm_round: 3 | global_acc: 97.207% | global_loss: 0.11695993691682816 | global_f1: 0.9618874773139746 | global_precision: 0.9680365296803652 | global_recall: 0.9558160504959423 | global_auc: 0.9935681111647675| flobal_FPR: 0.04418394950405771 \n",
      "client_5\n",
      "comm_round: 3 | global_acc: 97.407% | global_loss: 0.10772508382797241 | global_f1: 0.9650224215246638 | global_precision: 0.9598572702943801 | global_recall: 0.9702434625788999 | global_auc: 0.9946801766959119| flobal_FPR: 0.029756537421100092 \n",
      "client_6\n",
      "comm_round: 3 | global_acc: 97.706% | global_loss: 0.09605797380208969 | global_f1: 0.9689608636977057 | global_precision: 0.966786355475763 | global_recall: 0.9711451758340848 | global_auc: 0.9952677860446696| flobal_FPR: 0.028854824165915238 \n",
      "client_7\n",
      "comm_round: 3 | global_acc: 97.174% | global_loss: 0.11207195371389389 | global_f1: 0.9617633828160144 | global_precision: 0.9596050269299821 | global_recall: 0.9639314697926059 | global_auc: 0.9941122730344052| flobal_FPR: 0.03606853020739405 \n",
      "client_8\n",
      "comm_round: 3 | global_acc: 97.739% | global_loss: 0.10062522441148758 | global_f1: 0.9693417493237151 | global_precision: 0.9693417493237151 | global_recall: 0.9693417493237151 | global_auc: 0.9945412872134781| flobal_FPR: 0.030658250676284943 \n",
      "client_9\n",
      "comm_round: 3 | global_acc: 97.673% | global_loss: 0.10965874046087265 | global_f1: 0.9685816876122083 | global_precision: 0.9642537980339589 | global_recall: 0.9729486023444545 | global_auc: 0.9945906701405657| flobal_FPR: 0.027051397655545536 \n",
      "client_10\n",
      "comm_round: 3 | global_acc: 97.174% | global_loss: 0.1292683333158493 | global_f1: 0.9616944569625958 | global_precision: 0.9612612612612612 | global_recall: 0.9621280432822362 | global_auc: 0.9942478386659773| flobal_FPR: 0.03787195671776375 \n",
      "[]\n",
      "[[[1, 34.292557086442386], [2, 47.23907707059009], [3, 54.736620966665164], [5, 46.054059847451995], [6, 44.36538332204141], [8, 34.66967799699959], [10, 50.35599443759919]], [[4, 100.95020205404373]], [[7, 72.10428800846896], [9, 65.87032775087903]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 4 | global_acc: 97.074% | global_loss: 0.15453144907951355 | global_f1: 0.9603603603603604 | global_precision: 0.9594959495949595 | global_recall: 0.9612263300270514 | global_auc: 0.9932746626172667| flobal_FPR: 0.0387736699729486 \n",
      "client_2\n",
      "comm_round: 4 | global_acc: 97.340% | global_loss: 0.15060867369174957 | global_f1: 0.9638336347197106 | global_precision: 0.9664551223934723 | global_recall: 0.9612263300270514 | global_auc: 0.9947993604910943| flobal_FPR: 0.0387736699729486 \n",
      "client_3\n",
      "comm_round: 4 | global_acc: 97.407% | global_loss: 0.15267106890678406 | global_f1: 0.9648964896489649 | global_precision: 0.963162623539982 | global_recall: 0.9666366095581606 | global_auc: 0.9938850640862188| flobal_FPR: 0.033363390441839495 \n",
      "client_4\n",
      "comm_round: 4 | global_acc: 97.174% | global_loss: 0.1599452644586563 | global_f1: 0.9614512471655328 | global_precision: 0.9671532846715328 | global_recall: 0.9558160504959423 | global_auc: 0.9928029132128295| flobal_FPR: 0.04418394950405771 \n",
      "client_5\n",
      "comm_round: 4 | global_acc: 97.108% | global_loss: 0.1860840767621994 | global_f1: 0.9610738255033558 | global_precision: 0.9538188277087034 | global_recall: 0.9684400360685302 | global_auc: 0.9942582850543995| flobal_FPR: 0.031559963931469794 \n",
      "client_6\n",
      "comm_round: 4 | global_acc: 97.640% | global_loss: 0.13349458575248718 | global_f1: 0.9681757059614523 | global_precision: 0.9625668449197861 | global_recall: 0.9738503155996393 | global_auc: 0.9948950399123264| flobal_FPR: 0.026149684400360685 \n",
      "client_7\n",
      "comm_round: 4 | global_acc: 97.108% | global_loss: 0.1929044872522354 | global_f1: 0.9609339919173776 | global_precision: 0.9570661896243292 | global_recall: 0.9648331830477908 | global_auc: 0.9932506834074788| flobal_FPR: 0.0351668169522092 \n",
      "client_8\n",
      "comm_round: 4 | global_acc: 97.773% | global_loss: 0.16743700206279755 | global_f1: 0.9697243560777227 | global_precision: 0.9719202898550725 | global_recall: 0.9675383228133454 | global_auc: 0.9940355870466682| flobal_FPR: 0.032461677186654644 \n",
      "client_9\n",
      "comm_round: 4 | global_acc: 97.340% | global_loss: 0.15915468335151672 | global_f1: 0.9642857142857142 | global_precision: 0.9549071618037135 | global_recall: 0.9738503155996393 | global_auc: 0.9940170684490106| flobal_FPR: 0.026149684400360685 \n",
      "client_10\n",
      "comm_round: 4 | global_acc: 97.074% | global_loss: 0.22507116198539734 | global_f1: 0.9603960396039604 | global_precision: 0.958670260557053 | global_recall: 0.9621280432822362 | global_auc: 0.9934016812037658| flobal_FPR: 0.03787195671776375 \n",
      "[]\n",
      "[[[2, 24.54026529952926], [5, 26.173991771218397], [7, 44.89282957364555], [8, 34.759655562437544], [9, 36.036207161940226], [10, 34.56461719405891]], [[6, 98.08234548132177]], [[1, 72.67532838861146], [3, 59.36962289587859], [4, 74.68531564384978]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 5 | global_acc: 96.576% | global_loss: 0.31334543228149414 | global_f1: 0.954079358002675 | global_precision: 0.9435626102292769 | global_recall: 0.9648331830477908 | global_auc: 0.9925939854443824| flobal_FPR: 0.0351668169522092 \n",
      "client_2\n",
      "comm_round: 5 | global_acc: 97.307% | global_loss: 0.25266802310943604 | global_f1: 0.9633981021238137 | global_precision: 0.9655797101449275 | global_recall: 0.9612263300270514 | global_auc: 0.9940828332124877| flobal_FPR: 0.0387736699729486 \n",
      "client_3\n",
      "comm_round: 5 | global_acc: 97.374% | global_loss: 0.22570134699344635 | global_f1: 0.9645580978017049 | global_precision: 0.9598214285714286 | global_recall: 0.9693417493237151 | global_auc: 0.9938758047873899| flobal_FPR: 0.030658250676284943 \n",
      "client_4\n",
      "comm_round: 5 | global_acc: 96.941% | global_loss: 0.262941837310791 | global_f1: 0.9584837545126353 | global_precision: 0.959349593495935 | global_recall: 0.957619477006312 | global_auc: 0.9916371912320613| flobal_FPR: 0.04238052299368801 \n",
      "client_5\n",
      "comm_round: 5 | global_acc: 97.074% | global_loss: 0.272098571062088 | global_f1: 0.9607843137254902 | global_precision: 0.9497797356828194 | global_recall: 0.9720468890892696 | global_auc: 0.9936343507640821| flobal_FPR: 0.027953110910730387 \n",
      "client_6\n",
      "comm_round: 5 | global_acc: 97.174% | global_loss: 0.23032699525356293 | global_f1: 0.9621717846016911 | global_precision: 0.9499121265377856 | global_recall: 0.9747520288548241 | global_auc: 0.9940944666905035| flobal_FPR: 0.025247971145175834 \n",
      "client_7\n",
      "comm_round: 5 | global_acc: 96.709% | global_loss: 0.3929849863052368 | global_f1: 0.9561752988047809 | global_precision: 0.9391304347826087 | global_recall: 0.9738503155996393 | global_auc: 0.9922530533131433| flobal_FPR: 0.026149684400360685 \n",
      "client_8\n",
      "comm_round: 5 | global_acc: 97.540% | global_loss: 0.2779606878757477 | global_f1: 0.9667266187050361 | global_precision: 0.9641255605381166 | global_recall: 0.9693417493237151 | global_auc: 0.9936277030623588| flobal_FPR: 0.030658250676284943 \n",
      "client_9\n",
      "comm_round: 5 | global_acc: 97.074% | global_loss: 0.2916887700557709 | global_f1: 0.9607142857142857 | global_precision: 0.9513704686118479 | global_recall: 0.9702434625788999 | global_auc: 0.9931148803579883| flobal_FPR: 0.029756537421100092 \n",
      "client_10\n",
      "comm_round: 5 | global_acc: 96.875% | global_loss: 0.43782612681388855 | global_f1: 0.9577338129496403 | global_precision: 0.9551569506726457 | global_recall: 0.9603246167718665 | global_auc: 0.9919752743482759| flobal_FPR: 0.03967538322813345 \n",
      "[]\n",
      "[[[2, 36.81441506642098], [7, 28.044670530615534], [8, 48.43479780449714]], [[3, 102.00267073604675], [4, 85.96922102438654], [6, 96.2627365278426], [9, 95.24062866971875], [10, 87.19806819829935]], [[1, 61.85270329950191], [5, 75.05970402200616]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 6 | global_acc: 96.742% | global_loss: 0.5548146367073059 | global_f1: 0.9564831261101243 | global_precision: 0.9422572178477691 | global_recall: 0.9711451758340848 | global_auc: 0.9914617393901494| flobal_FPR: 0.028854824165915238 \n",
      "client_2\n",
      "comm_round: 6 | global_acc: 97.141% | global_loss: 0.38378044962882996 | global_f1: 0.9611913357400722 | global_precision: 0.962059620596206 | global_recall: 0.9603246167718665 | global_auc: 0.9931371976423451| flobal_FPR: 0.03967538322813345 \n",
      "client_3\n",
      "comm_round: 6 | global_acc: 97.074% | global_loss: 0.4648965001106262 | global_f1: 0.960573476702509 | global_precision: 0.9545859305431879 | global_recall: 0.9666366095581606 | global_auc: 0.9920505358285008| flobal_FPR: 0.033363390441839495 \n",
      "client_4\n",
      "comm_round: 6 | global_acc: 96.609% | global_loss: 0.42849233746528625 | global_f1: 0.9540126239855726 | global_precision: 0.9540126239855726 | global_recall: 0.9540126239855726 | global_auc: 0.991856565388931| flobal_FPR: 0.045987376014427414 \n",
      "client_5\n",
      "comm_round: 6 | global_acc: 96.476% | global_loss: 0.6000210642814636 | global_f1: 0.9532215357458075 | global_precision: 0.9334485738980121 | global_recall: 0.9738503155996393 | global_auc: 0.9917535260122193| flobal_FPR: 0.026149684400360685 \n",
      "client_6\n",
      "comm_round: 6 | global_acc: 97.108% | global_loss: 0.592645525932312 | global_f1: 0.9614190687361419 | global_precision: 0.9458987783595113 | global_recall: 0.9774571686203787 | global_auc: 0.9918639253444104| flobal_FPR: 0.02254283137962128 \n",
      "client_7\n",
      "comm_round: 6 | global_acc: 96.775% | global_loss: 0.5958788394927979 | global_f1: 0.9564436461607543 | global_precision: 0.9525939177101967 | global_recall: 0.9603246167718665 | global_auc: 0.9900842406259098| flobal_FPR: 0.03967538322813345 \n",
      "client_8\n",
      "comm_round: 6 | global_acc: 97.174% | global_loss: 0.41187772154808044 | global_f1: 0.961866307761328 | global_precision: 0.9571428571428572 | global_recall: 0.9666366095581606 | global_auc: 0.9926075182657476| flobal_FPR: 0.033363390441839495 \n",
      "client_9\n",
      "comm_round: 6 | global_acc: 96.941% | global_loss: 0.6228384971618652 | global_f1: 0.9590747330960854 | global_precision: 0.9464442493415277 | global_recall: 0.9720468890892696 | global_auc: 0.9911858597686315| flobal_FPR: 0.027953110910730387 \n",
      "client_10\n",
      "comm_round: 6 | global_acc: 96.809% | global_loss: 0.6893905401229858 | global_f1: 0.9567956795679567 | global_precision: 0.9550763701707098 | global_recall: 0.9585211902614968 | global_auc: 0.9900965863576816| flobal_FPR: 0.04147880973850315 \n",
      "[]\n",
      "[[[3, 54.98526004237386], [5, 75.41407874749497], [7, 63.23831123099114], [8, 80.63062384527595], [10, 84.62505168889932]], [[4, 26.629986471690273]], [[1, 109.5200978461559], [2, 106.87338561171373], [6, 99.80262182407232], [9, 95.98878694536396]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 7 | global_acc: 96.576% | global_loss: 1.1757824420928955 | global_f1: 0.9543642002658396 | global_precision: 0.9381533101045296 | global_recall: 0.9711451758340848 | global_auc: 0.9869320429194618| flobal_FPR: 0.028854824165915238 \n",
      "client_2\n",
      "comm_round: 7 | global_acc: 97.074% | global_loss: 0.630684494972229 | global_f1: 0.9603246167718665 | global_precision: 0.9603246167718665 | global_recall: 0.9603246167718665 | global_auc: 0.9917860522670799| flobal_FPR: 0.03967538322813345 \n",
      "client_3\n",
      "comm_round: 7 | global_acc: 96.975% | global_loss: 0.8944182991981506 | global_f1: 0.9594293357111012 | global_precision: 0.9488536155202821 | global_recall: 0.9702434625788999 | global_auc: 0.9894446367529586| flobal_FPR: 0.029756537421100092 \n",
      "client_4\n",
      "comm_round: 7 | global_acc: 96.941% | global_loss: 0.6496646404266357 | global_f1: 0.9587443946188341 | global_precision: 0.9536128456735058 | global_recall: 0.9639314697926059 | global_auc: 0.9905640622395823| flobal_FPR: 0.03606853020739405 \n",
      "client_5\n",
      "comm_round: 7 | global_acc: 96.676% | global_loss: 0.9728352427482605 | global_f1: 0.9556344276841171 | global_precision: 0.9406113537117904 | global_recall: 0.9711451758340848 | global_auc: 0.9893033730913379| flobal_FPR: 0.028854824165915238 \n",
      "client_6\n",
      "comm_round: 7 | global_acc: 96.642% | global_loss: 0.8688563704490662 | global_f1: 0.9556043956043957 | global_precision: 0.9322469982847341 | global_recall: 0.9801623083859333 | global_auc: 0.9900730819837311| flobal_FPR: 0.019837691614066726 \n",
      "client_7\n",
      "comm_round: 7 | global_acc: 96.609% | global_loss: 0.8360508680343628 | global_f1: 0.9541778975741241 | global_precision: 0.9507609668755596 | global_recall: 0.957619477006312 | global_auc: 0.9888840930469314| flobal_FPR: 0.04238052299368801 \n",
      "client_8\n",
      "comm_round: 7 | global_acc: 96.742% | global_loss: 0.6059859991073608 | global_f1: 0.956366874443455 | global_precision: 0.9445910290237467 | global_recall: 0.9684400360685302 | global_auc: 0.9909543772979088| flobal_FPR: 0.031559963931469794 \n",
      "client_9\n",
      "comm_round: 7 | global_acc: 97.074% | global_loss: 0.9354423880577087 | global_f1: 0.960749330954505 | global_precision: 0.9505736981465137 | global_recall: 0.9711451758340848 | global_auc: 0.9891215109656214| flobal_FPR: 0.028854824165915238 \n",
      "client_10\n",
      "comm_round: 7 | global_acc: 96.576% | global_loss: 1.2003450393676758 | global_f1: 0.9536244934714093 | global_precision: 0.9523381294964028 | global_recall: 0.9549143372407575 | global_auc: 0.9861642333704179| flobal_FPR: 0.04508566275924256 \n",
      "[]\n",
      "[[[6, 118.4018078074059], [8, 135.5370779986172]], [[1, 42.87478006682635], [3, 40.722364338058036], [4, 42.70618345818741], [10, 49.87110285867405]], [[2, 74.31402275610958], [5, 75.50991824395743], [7, 63.599220491803074], [9, 95.44268006148064]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 8 | global_acc: 96.642% | global_loss: 1.411922574043274 | global_f1: 0.9551708832667554 | global_precision: 0.9405594405594405 | global_recall: 0.9702434625788999 | global_auc: 0.9856279062920972| flobal_FPR: 0.029756537421100092 \n",
      "client_2\n",
      "comm_round: 8 | global_acc: 97.041% | global_loss: 1.2146800756454468 | global_f1: 0.9600717810677435 | global_precision: 0.9553571428571429 | global_recall: 0.9648331830477908 | global_auc: 0.9874377430862715| flobal_FPR: 0.0351668169522092 \n",
      "client_3\n",
      "comm_round: 8 | global_acc: 97.041% | global_loss: 1.1318244934082031 | global_f1: 0.9602855867916109 | global_precision: 0.950530035335689 | global_recall: 0.9702434625788999 | global_auc: 0.9883446795356676| flobal_FPR: 0.029756537421100092 \n",
      "client_4\n",
      "comm_round: 8 | global_acc: 92.786% | global_loss: 1.2865055799484253 | global_f1: 0.908245243128964 | global_precision: 0.8550955414012739 | global_recall: 0.9684400360685302 | global_auc: 0.9863116698979245| flobal_FPR: 0.031559963931469794 \n",
      "client_5\n",
      "comm_round: 8 | global_acc: 96.576% | global_loss: 1.3692424297332764 | global_f1: 0.9544851966416261 | global_precision: 0.9358752166377816 | global_recall: 0.9738503155996393 | global_auc: 0.9867200287180715| flobal_FPR: 0.026149684400360685 \n",
      "client_6\n",
      "comm_round: 8 | global_acc: 96.975% | global_loss: 1.1726593971252441 | global_f1: 0.959752321981424 | global_precision: 0.9418402777777778 | global_recall: 0.9783588818755635 | global_auc: 0.9881791992463406| flobal_FPR: 0.02164111812443643 \n",
      "client_7\n",
      "comm_round: 8 | global_acc: 97.108% | global_loss: 1.1515812873840332 | global_f1: 0.9608988764044945 | global_precision: 0.9578853046594982 | global_recall: 0.9639314697926059 | global_auc: 0.9875137168202522| flobal_FPR: 0.03606853020739405 \n",
      "client_8\n",
      "comm_round: 8 | global_acc: 96.509% | global_loss: 1.3366725444793701 | global_f1: 0.9532710280373832 | global_precision: 0.9411247803163445 | global_recall: 0.9657348963029756 | global_auc: 0.985345378968856| flobal_FPR: 0.034265103697024346 \n",
      "client_9\n",
      "comm_round: 8 | global_acc: 97.074% | global_loss: 1.0059617757797241 | global_f1: 0.960573476702509 | global_precision: 0.9545859305431879 | global_recall: 0.9666366095581606 | global_auc: 0.9893510940929948| flobal_FPR: 0.033363390441839495 \n",
      "client_10\n",
      "comm_round: 8 | global_acc: 96.476% | global_loss: 1.652691125869751 | global_f1: 0.9522522522522522 | global_precision: 0.9513951395139514 | global_recall: 0.9531109107303878 | global_auc: 0.9826594700547154| flobal_FPR: 0.046889089269612265 \n",
      "[]\n",
      "[[[1, 63.476378302728236], [2, 58.36460006386982], [7, 63.9631757296089]], [[3, 51.41032266652232], [4, 47.89792926147535], [5, 46.454604190460806], [8, 44.75933828955291], [10, 48.14174675942232]], [[6, 85.53044268126067], [9, 76.775447236088]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 9 | global_acc: 96.676% | global_loss: 1.8049815893173218 | global_f1: 0.9553969669937555 | global_precision: 0.9452780229479258 | global_recall: 0.9657348963029756 | global_auc: 0.9824920904220389| flobal_FPR: 0.034265103697024346 \n",
      "client_2\n",
      "comm_round: 9 | global_acc: 96.941% | global_loss: 1.5636731386184692 | global_f1: 0.9588182632050133 | global_precision: 0.952 | global_recall: 0.9657348963029756 | global_auc: 0.9848420529812331| flobal_FPR: 0.034265103697024346 \n",
      "client_3\n",
      "comm_round: 9 | global_acc: 97.108% | global_loss: 1.4492356777191162 | global_f1: 0.9611433675748102 | global_precision: 0.952212389380531 | global_recall: 0.9702434625788999 | global_auc: 0.9856965200705987| flobal_FPR: 0.029756537421100092 \n",
      "client_4\n",
      "comm_round: 9 | global_acc: 97.041% | global_loss: 0.941346287727356 | global_f1: 0.9597467209407508 | global_precision: 0.9627949183303085 | global_recall: 0.9567177637511272 | global_auc: 0.9896946378213393| flobal_FPR: 0.04328223624887286 \n",
      "client_5\n",
      "comm_round: 9 | global_acc: 96.941% | global_loss: 1.3798683881759644 | global_f1: 0.9587813620071686 | global_precision: 0.9528049866429208 | global_recall: 0.9648331830477908 | global_auc: 0.9870030308771499| flobal_FPR: 0.0351668169522092 \n",
      "client_6\n",
      "comm_round: 9 | global_acc: 97.108% | global_loss: 1.1537268161773682 | global_f1: 0.9612472160356347 | global_precision: 0.9498239436619719 | global_recall: 0.9729486023444545 | global_auc: 0.9890564584559004| flobal_FPR: 0.027051397655545536 \n",
      "client_7\n",
      "comm_round: 9 | global_acc: 97.141% | global_loss: 1.3939971923828125 | global_f1: 0.961156278229449 | global_precision: 0.96289592760181 | global_recall: 0.9594229035166817 | global_auc: 0.9862549270153577| flobal_FPR: 0.0405770964833183 \n",
      "client_8\n",
      "comm_round: 9 | global_acc: 96.476% | global_loss: 1.5775035619735718 | global_f1: 0.9529724933451642 | global_precision: 0.9379912663755459 | global_recall: 0.9684400360685302 | global_auc: 0.9845702094643329| flobal_FPR: 0.031559963931469794 \n",
      "client_9\n",
      "comm_round: 9 | global_acc: 96.941% | global_loss: 1.372713565826416 | global_f1: 0.9587073608617595 | global_precision: 0.9544235924932976 | global_recall: 0.9630297565374211 | global_auc: 0.9868285287069128| flobal_FPR: 0.0369702434625789 \n",
      "client_10\n",
      "comm_round: 9 | global_acc: 96.376% | global_loss: 1.870294451713562 | global_f1: 0.9509671614934773 | global_precision: 0.9488330341113106 | global_recall: 0.9531109107303878 | global_auc: 0.9814232349520962| flobal_FPR: 0.046889089269612265 \n",
      "[]\n",
      "[[[1, 132.02635556867807], [4, 141.41789529069501], [7, 119.37146794417231], [9, 145.96128522561148]], [[3, 19.492316775929083], [10, 38.650465825087046]], [[2, 103.18479732326972], [5, 66.36675424322574], [6, 85.79965516617638], [8, 81.78067113760609]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 10 | global_acc: 96.676% | global_loss: 2.20963716506958 | global_f1: 0.9553571428571429 | global_precision: 0.9460654288240495 | global_recall: 0.9648331830477908 | global_auc: 0.979773417835119| flobal_FPR: 0.0351668169522092 \n",
      "client_2\n",
      "comm_round: 10 | global_acc: 97.041% | global_loss: 1.576442003250122 | global_f1: 0.9599640125955916 | global_precision: 0.9578096947935368 | global_recall: 0.9621280432822362 | global_auc: 0.9857891130588878| flobal_FPR: 0.03787195671776375 \n",
      "client_3\n",
      "comm_round: 10 | global_acc: 96.809% | global_loss: 1.7671436071395874 | global_f1: 0.9570277529095791 | global_precision: 0.9502222222222222 | global_recall: 0.9639314697926059 | global_auc: 0.9841936646452905| flobal_FPR: 0.03606853020739405 \n",
      "client_4\n",
      "comm_round: 10 | global_acc: 97.207% | global_loss: 1.7475625276565552 | global_f1: 0.962432915921288 | global_precision: 0.9547471162377995 | global_recall: 0.9702434625788999 | global_auc: 0.9841024961645135| flobal_FPR: 0.029756537421100092 \n",
      "client_5\n",
      "comm_round: 10 | global_acc: 96.842% | global_loss: 1.7856199741363525 | global_f1: 0.9574944071588367 | global_precision: 0.9502664298401421 | global_recall: 0.9648331830477908 | global_auc: 0.9843897718461285| flobal_FPR: 0.0351668169522092 \n",
      "client_6\n",
      "comm_round: 10 | global_acc: 96.941% | global_loss: 1.7800906896591187 | global_f1: 0.9590017825311944 | global_precision: 0.9480176211453745 | global_recall: 0.9702434625788999 | global_auc: 0.9843287554410252| flobal_FPR: 0.029756537421100092 \n",
      "client_7\n",
      "comm_round: 10 | global_acc: 96.842% | global_loss: 1.7635712623596191 | global_f1: 0.9574181981174361 | global_precision: 0.9518716577540107 | global_recall: 0.9630297565374211 | global_auc: 0.9840989348957331| flobal_FPR: 0.0369702434625789 \n",
      "client_8\n",
      "comm_round: 10 | global_acc: 96.210% | global_loss: 1.919725775718689 | global_f1: 0.9493783303730017 | global_precision: 0.9352580927384077 | global_recall: 0.9639314697926059 | global_auc: 0.9823952239112133| flobal_FPR: 0.03606853020739405 \n",
      "client_9\n",
      "comm_round: 10 | global_acc: 97.008% | global_loss: 1.5897374153137207 | global_f1: 0.9596774193548386 | global_precision: 0.9536954585930543 | global_recall: 0.9657348963029756 | global_auc: 0.9858605758524135| flobal_FPR: 0.034265103697024346 \n",
      "client_10\n",
      "comm_round: 10 | global_acc: 96.310% | global_loss: 2.151348352432251 | global_f1: 0.950112359550562 | global_precision: 0.9471326164874552 | global_recall: 0.9531109107303878 | global_auc: 0.9797978718807441| flobal_FPR: 0.046889089269612265 \n",
      "[]\n",
      "[[[6, 148.12554592024279], [7, 177.43079199415902]], [[1, 118.87694197305905], [2, 101.70428047150322], [3, 90.9502514418722], [8, 106.62439039350406], [9, 90.57688617020993], [10, 89.46289646538752]], [[4, 36.056046208257996], [5, 47.7978207480639]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 11 | global_acc: 96.676% | global_loss: 1.9262734651565552 | global_f1: 0.9553571428571429 | global_precision: 0.9460654288240495 | global_recall: 0.9648331830477908 | global_auc: 0.9840737685963521| flobal_FPR: 0.0351668169522092 \n",
      "client_2\n",
      "comm_round: 11 | global_acc: 97.340% | global_loss: 1.7531110048294067 | global_f1: 0.96386630532972 | global_precision: 0.9656108597285068 | global_recall: 0.9621280432822362 | global_auc: 0.9846611405271913| flobal_FPR: 0.03787195671776375 \n",
      "client_3\n",
      "comm_round: 11 | global_acc: 96.609% | global_loss: 2.078679084777832 | global_f1: 0.9545859305431877 | global_precision: 0.9428320140721196 | global_recall: 0.9666366095581606 | global_auc: 0.982016304912984| flobal_FPR: 0.033363390441839495 \n",
      "client_4\n",
      "comm_round: 11 | global_acc: 96.842% | global_loss: 1.793436050415039 | global_f1: 0.957149300857014 | global_precision: 0.9575812274368231 | global_recall: 0.9567177637511272 | global_auc: 0.9844894873719784| flobal_FPR: 0.04328223624887286 \n",
      "client_5\n",
      "comm_round: 11 | global_acc: 96.809% | global_loss: 1.840159296989441 | global_f1: 0.956989247311828 | global_precision: 0.9510240427426536 | global_recall: 0.9630297565374211 | global_auc: 0.984885737878272| flobal_FPR: 0.0369702434625789 \n",
      "client_6\n",
      "comm_round: 11 | global_acc: 97.207% | global_loss: 1.8421361446380615 | global_f1: 0.9625334522747547 | global_precision: 0.9523389232127096 | global_recall: 0.9729486023444545 | global_auc: 0.9845243878060258| flobal_FPR: 0.027051397655545536 \n",
      "client_7\n",
      "comm_round: 11 | global_acc: 96.908% | global_loss: 2.0718233585357666 | global_f1: 0.9583519928347515 | global_precision: 0.9519572953736655 | global_recall: 0.9648331830477908 | global_auc: 0.9820250893759754| flobal_FPR: 0.0351668169522092 \n",
      "client_8\n",
      "comm_round: 11 | global_acc: 96.576% | global_loss: 1.956593632698059 | global_f1: 0.9535408209291836 | global_precision: 0.953971119133574 | global_recall: 0.9531109107303878 | global_auc: 0.982336344267378| flobal_FPR: 0.046889089269612265 \n",
      "client_9\n",
      "comm_round: 11 | global_acc: 97.207% | global_loss: 1.60555899143219 | global_f1: 0.9622641509433962 | global_precision: 0.9588182632050134 | global_recall: 0.9657348963029756 | global_auc: 0.9864467606936593| flobal_FPR: 0.034265103697024346 \n",
      "client_10\n",
      "comm_round: 11 | global_acc: 96.243% | global_loss: 2.3642115592956543 | global_f1: 0.9492590929501572 | global_precision: 0.945438282647585 | global_recall: 0.9531109107303878 | global_auc: 0.978800479204327| flobal_FPR: 0.046889089269612265 \n",
      "[]\n",
      "[[[2, 129.21232689805666], [3, 178.06889995551097], [7, 172.6465943569221]], [[6, 110.90859012482456], [8, 81.97808324527585], [9, 73.04212543022722], [10, 70.27256311622125]], [[1, 36.69414503200132], [4, 30.803308867036957], [5, 33.40204335599677]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 12 | global_acc: 96.543% | global_loss: 2.3326480388641357 | global_f1: 0.9531531531531532 | global_precision: 0.9522952295229523 | global_recall: 0.9540126239855726 | global_auc: 0.9801556606842099| flobal_FPR: 0.045987376014427414 \n",
      "client_2\n",
      "comm_round: 12 | global_acc: 96.709% | global_loss: 2.021389961242676 | global_f1: 0.955505617977528 | global_precision: 0.9525089605734767 | global_recall: 0.9585211902614968 | global_auc: 0.9829849700212396| flobal_FPR: 0.04147880973850315 \n",
      "client_3\n",
      "comm_round: 12 | global_acc: 96.543% | global_loss: 2.38942551612854 | global_f1: 0.9537777777777777 | global_precision: 0.9404031551270815 | global_recall: 0.9675383228133454 | global_auc: 0.9798049944183047| flobal_FPR: 0.032461677186654644 \n",
      "client_4\n",
      "comm_round: 12 | global_acc: 97.141% | global_loss: 1.8873933553695679 | global_f1: 0.961050724637681 | global_precision: 0.9654231119199272 | global_recall: 0.9567177637511272 | global_auc: 0.9839890103993797| flobal_FPR: 0.04328223624887286 \n",
      "client_5\n",
      "comm_round: 12 | global_acc: 96.809% | global_loss: 2.1346805095672607 | global_f1: 0.9571045576407506 | global_precision: 0.9486271036315324 | global_recall: 0.9657348963029756 | global_auc: 0.9814521999381765| flobal_FPR: 0.034265103697024346 \n",
      "client_6\n",
      "comm_round: 12 | global_acc: 97.041% | global_loss: 2.1623589992523193 | global_f1: 0.9602145730889584 | global_precision: 0.9521276595744681 | global_recall: 0.9684400360685302 | global_auc: 0.9814063782798692| flobal_FPR: 0.031559963931469794 \n",
      "client_7\n",
      "comm_round: 12 | global_acc: 97.174% | global_loss: 1.999969482421875 | global_f1: 0.9617977528089888 | global_precision: 0.9587813620071685 | global_recall: 0.9648331830477908 | global_auc: 0.9832976494201543| flobal_FPR: 0.0351668169522092 \n",
      "client_8\n",
      "comm_round: 12 | global_acc: 96.343% | global_loss: 2.5405356884002686 | global_f1: 0.9509366636931311 | global_precision: 0.940864960282436 | global_recall: 0.9612263300270514 | global_auc: 0.9772537014640613| flobal_FPR: 0.0387736699729486 \n",
      "client_9\n",
      "comm_round: 12 | global_acc: 97.141% | global_loss: 1.9669406414031982 | global_f1: 0.9617097061442564 | global_precision: 0.9498680738786279 | global_recall: 0.9738503155996393 | global_auc: 0.9831927107000932| flobal_FPR: 0.026149684400360685 \n",
      "client_10\n",
      "comm_round: 12 | global_acc: 96.144% | global_loss: 2.5582523345947266 | global_f1: 0.947982062780269 | global_precision: 0.9429081177520071 | global_recall: 0.9531109107303878 | global_auc: 0.9778648151867696| flobal_FPR: 0.046889089269612265 \n",
      "[]\n",
      "[[[1, 71.49908636775558], [4, 52.728316366864824], [5, 50.19280512681763], [7, 55.550613274987825], [8, 50.382346991296586], [9, 31.339479184418384], [10, 89.02687526704123]], [[2, 128.6507019340802], [3, 134.82982888145767]], [[6, 199.94217342338916]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 13 | global_acc: 96.975% | global_loss: 2.3814356327056885 | global_f1: 0.9591745177209512 | global_precision: 0.9544642857142858 | global_recall: 0.9639314697926059 | global_auc: 0.9808289779016149| flobal_FPR: 0.03606853020739405 \n",
      "client_2\n",
      "comm_round: 13 | global_acc: 97.108% | global_loss: 2.156679153442383 | global_f1: 0.9607223476297968 | global_precision: 0.9620253164556962 | global_recall: 0.9594229035166817 | global_auc: 0.9824092315684161| flobal_FPR: 0.0405770964833183 \n",
      "client_3\n",
      "comm_round: 13 | global_acc: 97.174% | global_loss: 2.073246479034424 | global_f1: 0.9619346171070309 | global_precision: 0.9555160142348754 | global_recall: 0.9684400360685302 | global_auc: 0.9820656878400716| flobal_FPR: 0.031559963931469794 \n",
      "client_4\n",
      "comm_round: 13 | global_acc: 96.975% | global_loss: 1.9192627668380737 | global_f1: 0.9592841163310961 | global_precision: 0.9520426287744227 | global_recall: 0.9666366095581606 | global_auc: 0.9829446089750621| flobal_FPR: 0.033363390441839495 \n",
      "client_5\n",
      "comm_round: 13 | global_acc: 96.609% | global_loss: 2.4970290660858154 | global_f1: 0.954626334519573 | global_precision: 0.9420544337137841 | global_recall: 0.9675383228133454 | global_auc: 0.9789016192376889| flobal_FPR: 0.032461677186654644 \n",
      "client_6\n",
      "comm_round: 13 | global_acc: 96.941% | global_loss: 2.466696262359619 | global_f1: 0.9591836734693878 | global_precision: 0.9441048034934498 | global_recall: 0.9747520288548241 | global_auc: 0.9783277801282153| flobal_FPR: 0.025247971145175834 \n",
      "client_7\n",
      "comm_round: 13 | global_acc: 97.041% | global_loss: 1.9444245100021362 | global_f1: 0.9597830998644374 | global_precision: 0.9619565217391305 | global_recall: 0.957619477006312 | global_auc: 0.9830737643228293| flobal_FPR: 0.04238052299368801 \n",
      "client_8\n",
      "comm_round: 13 | global_acc: 96.709% | global_loss: 2.172156572341919 | global_f1: 0.9555455770094298 | global_precision: 0.9516994633273703 | global_recall: 0.9594229035166817 | global_auc: 0.9818930850131837| flobal_FPR: 0.0405770964833183 \n",
      "client_9\n",
      "comm_round: 13 | global_acc: 96.941% | global_loss: 2.259723424911499 | global_f1: 0.9589652096342552 | global_precision: 0.9488084730803178 | global_recall: 0.9693417493237151 | global_auc: 0.9809602700106506| flobal_FPR: 0.030658250676284943 \n",
      "client_10\n",
      "comm_round: 13 | global_acc: 96.110% | global_loss: 2.866177797317505 | global_f1: 0.9475100942126514 | global_precision: 0.9428571428571428 | global_recall: 0.9522091974752029 | global_auc: 0.9737059655050758| flobal_FPR: 0.047790802524797116 \n",
      "[]\n",
      "[[[2, 188.644560615065], [7, 196.21835977728844], [9, 205.8681636470015]], [[1, 153.1517981081576], [3, 130.44644630888737], [5, 121.94584286016797], [6, 158.8863729963375], [8, 144.6825119468097], [10, 140.5513183812968]], [[4, 81.15048003030533]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 14 | global_acc: 96.809% | global_loss: 2.507767915725708 | global_f1: 0.95695067264574 | global_precision: 0.951828724353256 | global_recall: 0.9621280432822362 | global_auc: 0.9779747396831231| flobal_FPR: 0.03787195671776375 \n",
      "client_2\n",
      "comm_round: 14 | global_acc: 97.374% | global_loss: 2.2218844890594482 | global_f1: 0.9648420115709835 | global_precision: 0.9525483304042179 | global_recall: 0.9774571686203787 | global_auc: 0.9802995359429361| flobal_FPR: 0.02254283137962128 \n",
      "client_3\n",
      "comm_round: 14 | global_acc: 96.908% | global_loss: 2.3799965381622314 | global_f1: 0.958648288128057 | global_precision: 0.9456140350877194 | global_recall: 0.9720468890892696 | global_auc: 0.9783911707125055| flobal_FPR: 0.027953110910730387 \n",
      "client_4\n",
      "comm_round: 14 | global_acc: 96.742% | global_loss: 2.191984176635742 | global_f1: 0.9554545454545454 | global_precision: 0.9633363886342805 | global_recall: 0.9477006311992786 | global_auc: 0.9811646868386427| flobal_FPR: 0.05229936880072137 \n",
      "client_5\n",
      "comm_round: 14 | global_acc: 96.376% | global_loss: 2.787975311279297 | global_f1: 0.9517485613103143 | global_precision: 0.9347826086956522 | global_recall: 0.9693417493237151 | global_auc: 0.9761798602178261| flobal_FPR: 0.030658250676284943 \n",
      "client_6\n",
      "comm_round: 14 | global_acc: 97.008% | global_loss: 2.2843172550201416 | global_f1: 0.9596774193548386 | global_precision: 0.9536954585930543 | global_recall: 0.9657348963029756 | global_auc: 0.9787988172788962| flobal_FPR: 0.034265103697024346 \n",
      "client_7\n",
      "comm_round: 14 | global_acc: 96.709% | global_loss: 2.7568681240081787 | global_f1: 0.9560975609756098 | global_precision: 0.9406631762652705 | global_recall: 0.9720468890892696 | global_auc: 0.975753457635859| flobal_FPR: 0.027953110910730387 \n",
      "client_8\n",
      "comm_round: 14 | global_acc: 96.642% | global_loss: 2.4802732467651367 | global_f1: 0.955011135857461 | global_precision: 0.9436619718309859 | global_recall: 0.9666366095581606 | global_auc: 0.9780507134171038| flobal_FPR: 0.033363390441839495 \n",
      "client_9\n",
      "comm_round: 14 | global_acc: 97.241% | global_loss: 2.209172010421753 | global_f1: 0.9630289532293987 | global_precision: 0.9515845070422535 | global_recall: 0.9747520288548241 | global_auc: 0.9801224221755934| flobal_FPR: 0.025247971145175834 \n",
      "client_10\n",
      "comm_round: 14 | global_acc: 95.977% | global_loss: 3.075693368911743 | global_f1: 0.9456668163448586 | global_precision: 0.9418604651162791 | global_recall: 0.9495040577096483 | global_auc: 0.9728920968798063| flobal_FPR: 0.05049594229035167 \n",
      "[]\n",
      "[[[6, 175.7519363072369], [8, 159.46327183115295], [10, 145.92721738499372]], [[3, 35.09617919587471], [5, 52.542254803520194], [9, 55.880562813246605]], [[1, 128.2678252270476], [2, 98.16676944237827], [4, 135.5183640800645], [7, 125.38206623742231]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 15 | global_acc: 97.008% | global_loss: 2.5262343883514404 | global_f1: 0.9594959495949595 | global_precision: 0.9577717879604672 | global_recall: 0.9612263300270514 | global_auc: 0.9762385024437428| flobal_FPR: 0.0387736699729486 \n",
      "client_2\n",
      "comm_round: 15 | global_acc: 96.709% | global_loss: 2.555737257003784 | global_f1: 0.9554655870445344 | global_precision: 0.9533213644524237 | global_recall: 0.957619477006312 | global_auc: 0.9780129639680321| flobal_FPR: 0.04238052299368801 \n",
      "client_3\n",
      "comm_round: 15 | global_acc: 97.174% | global_loss: 2.424858331680298 | global_f1: 0.962070504239179 | global_precision: 0.9522968197879859 | global_recall: 0.9720468890892696 | global_auc: 0.976901610690644| flobal_FPR: 0.027953110910730387 \n",
      "client_4\n",
      "comm_round: 15 | global_acc: 96.709% | global_loss: 2.3979499340057373 | global_f1: 0.9550204452521581 | global_precision: 0.9624542124542125 | global_recall: 0.9477006311992786 | global_auc: 0.9774099224545594| flobal_FPR: 0.05229936880072137 \n",
      "client_5\n",
      "comm_round: 15 | global_acc: 96.243% | global_loss: 3.167433738708496 | global_f1: 0.9498000888494003 | global_precision: 0.936077057793345 | global_recall: 0.9639314697926059 | global_auc: 0.9714989285329328| flobal_FPR: 0.03606853020739405 \n",
      "client_6\n",
      "comm_round: 15 | global_acc: 97.041% | global_loss: 2.429928779602051 | global_f1: 0.960178970917226 | global_precision: 0.9529307282415631 | global_recall: 0.9675383228133454 | global_auc: 0.9787931192488477| flobal_FPR: 0.032461677186654644 \n",
      "client_7\n",
      "comm_round: 15 | global_acc: 96.642% | global_loss: 2.289513349533081 | global_f1: 0.9544840018026138 | global_precision: 0.9540540540540541 | global_recall: 0.9549143372407575 | global_auc: 0.9773814323043166| flobal_FPR: 0.04508566275924256 \n",
      "client_8\n",
      "comm_round: 15 | global_acc: 96.609% | global_loss: 2.8733482360839844 | global_f1: 0.9543010752688171 | global_precision: 0.9483526268922529 | global_recall: 0.9603246167718665 | global_auc: 0.9725150772249264| flobal_FPR: 0.03967538322813345 \n",
      "client_9\n",
      "comm_round: 15 | global_acc: 97.141% | global_loss: 2.261793851852417 | global_f1: 0.961156278229449 | global_precision: 0.96289592760181 | global_recall: 0.9594229035166817 | global_auc: 0.9800034757983296| flobal_FPR: 0.0405770964833183 \n",
      "client_10\n",
      "comm_round: 15 | global_acc: 95.944% | global_loss: 3.1746582984924316 | global_f1: 0.9452423698384201 | global_precision: 0.9410187667560321 | global_recall: 0.9495040577096483 | global_auc: 0.9713099438696557| flobal_FPR: 0.05049594229035167 \n",
      "[]\n",
      "[[[4, 138.20052425193435], [6, 131.12889690389332], [8, 107.69276258775392], [9, 90.68611114828053], [10, 89.54557083535023]], [[1, 234.2724806746046], [2, 197.01382989538237], [3, 215.10695288654932], [7, 183.71896100388759]], [[5, 42.358131643532126]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 16 | global_acc: 96.609% | global_loss: 2.777372360229492 | global_f1: 0.9546666666666667 | global_precision: 0.9412795793163892 | global_recall: 0.9684400360685302 | global_auc: 0.9745758647591563| flobal_FPR: 0.031559963931469794 \n",
      "client_2\n",
      "comm_round: 16 | global_acc: 97.074% | global_loss: 2.484711170196533 | global_f1: 0.9606791778373548 | global_precision: 0.9521700620017715 | global_recall: 0.9693417493237151 | global_auc: 0.976992779171421| flobal_FPR: 0.030658250676284943 \n",
      "client_3\n",
      "comm_round: 16 | global_acc: 96.775% | global_loss: 2.560295581817627 | global_f1: 0.9565606806986117 | global_precision: 0.9501779359430605 | global_recall: 0.9630297565374211 | global_auc: 0.9766869848921481| flobal_FPR: 0.0369702434625789 \n",
      "client_4\n",
      "comm_round: 16 | global_acc: 96.875% | global_loss: 2.389378070831299 | global_f1: 0.9575812274368231 | global_precision: 0.9584462511291779 | global_recall: 0.9567177637511272 | global_auc: 0.9771228841908631| flobal_FPR: 0.04328223624887286 \n",
      "client_5\n",
      "comm_round: 16 | global_acc: 96.476% | global_loss: 2.816955804824829 | global_f1: 0.9527207850133809 | global_precision: 0.942630185348632 | global_recall: 0.9630297565374211 | global_auc: 0.9742577247481113| flobal_FPR: 0.0369702434625789 \n",
      "client_6\n",
      "comm_round: 16 | global_acc: 96.676% | global_loss: 2.898607015609741 | global_f1: 0.9556344276841171 | global_precision: 0.9406113537117904 | global_recall: 0.9711451758340848 | global_auc: 0.9739723484098459| flobal_FPR: 0.028854824165915238 \n",
      "client_7\n",
      "comm_round: 16 | global_acc: 96.709% | global_loss: 2.79907488822937 | global_f1: 0.9555854643337819 | global_precision: 0.9508928571428571 | global_recall: 0.9603246167718665 | global_auc: 0.9749132356216147| flobal_FPR: 0.03967538322813345 \n",
      "client_8\n",
      "comm_round: 16 | global_acc: 96.410% | global_loss: 2.7098138332366943 | global_f1: 0.9515260323159784 | global_precision: 0.9472743521000894 | global_recall: 0.9558160504959423 | global_auc: 0.9755715955101422| flobal_FPR: 0.04418394950405771 \n",
      "client_9\n",
      "comm_round: 16 | global_acc: 96.709% | global_loss: 2.6743807792663574 | global_f1: 0.9555455770094298 | global_precision: 0.9516994633273703 | global_recall: 0.9594229035166817 | global_auc: 0.9744196437686581| flobal_FPR: 0.0405770964833183 \n",
      "client_10\n",
      "comm_round: 16 | global_acc: 95.844% | global_loss: 3.3121442794799805 | global_f1: 0.9438202247191011 | global_precision: 0.9408602150537635 | global_recall: 0.9467989179440938 | global_auc: 0.9695639724956089| flobal_FPR: 0.05320108205590622 \n",
      "[]\n",
      "[[[4, 237.29871814122097], [5, 216.91571655247088], [7, 192.40354759919413], [8, 227.29974973743327]], [[6, 34.96600895880368], [9, 55.288195142389874], [10, 66.41377831761058]], [[1, 162.81121077769666], [2, 110.43762156581644], [3, 134.77868608647915]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 17 | global_acc: 96.376% | global_loss: 3.0327084064483643 | global_f1: 0.9515340151178302 | global_precision: 0.9385964912280702 | global_recall: 0.9648331830477908 | global_auc: 0.9727002632015046| flobal_FPR: 0.0351668169522092 \n",
      "client_2\n",
      "comm_round: 17 | global_acc: 96.642% | global_loss: 2.867316484451294 | global_f1: 0.9544840018026138 | global_precision: 0.9540540540540541 | global_recall: 0.9549143372407575 | global_auc: 0.9715962698795959| flobal_FPR: 0.04508566275924256 \n",
      "client_3\n",
      "comm_round: 17 | global_acc: 94.847% | global_loss: 4.362715244293213 | global_f1: 0.9275362318840579 | global_precision: 0.9631067961165048 | global_recall: 0.8944995491433724 | global_auc: 0.9575708063329805| flobal_FPR: 0.1055004508566276 \n",
      "client_4\n",
      "comm_round: 17 | global_acc: 96.476% | global_loss: 2.9774179458618164 | global_f1: 0.9517304189435338 | global_precision: 0.9613615455381784 | global_recall: 0.9422903516681695 | global_auc: 0.9699357689562778| flobal_FPR: 0.057709648331830475 \n",
      "client_5\n",
      "comm_round: 17 | global_acc: 96.543% | global_loss: 3.000562906265259 | global_f1: 0.9534467323187108 | global_precision: 0.9466666666666667 | global_recall: 0.9603246167718665 | global_auc: 0.9734331723165007| flobal_FPR: 0.03967538322813345 \n",
      "client_6\n",
      "comm_round: 17 | global_acc: 96.642% | global_loss: 2.7034242153167725 | global_f1: 0.9538180155464108 | global_precision: 0.9675324675324676 | global_recall: 0.9404869251577999 | global_auc: 0.9730276625113782| flobal_FPR: 0.059513074842200184 \n",
      "client_7\n",
      "comm_round: 17 | global_acc: 95.944% | global_loss: 3.322608470916748 | global_f1: 0.9440879926672777 | global_precision: 0.9599254426840633 | global_recall: 0.9287646528403968 | global_auc: 0.9682021433140028| flobal_FPR: 0.07123534715960325 \n",
      "client_8\n",
      "comm_round: 17 | global_acc: 95.811% | global_loss: 3.5051639080047607 | global_f1: 0.944639718804921 | global_precision: 0.921165381319623 | global_recall: 0.9693417493237151 | global_auc: 0.9691798303031683| flobal_FPR: 0.030658250676284943 \n",
      "client_9\n",
      "comm_round: 17 | global_acc: 96.476% | global_loss: 2.8954074382781982 | global_f1: 0.9533039647577092 | global_precision: 0.9319552110249785 | global_recall: 0.975653742110009 | global_auc: 0.9740127094560233| flobal_FPR: 0.024346257889990983 \n",
      "client_10\n",
      "comm_round: 17 | global_acc: 95.878% | global_loss: 3.414957284927368 | global_f1: 0.9442946990116802 | global_precision: 0.9409131602506714 | global_recall: 0.9477006311992786 | global_auc: 0.9670684727522577| flobal_FPR: 0.05229936880072137 \n",
      "[]\n",
      "[[[7, 182.8544052041681], [9, 172.7649542343649], [10, 145.13169607542238]], [[2, 309.5700496859369], [4, 270.38926155466646], [5, 286.13567751569815]], [[1, 92.03720997028532], [3, 111.15218403575703], [6, 91.08128250026925], [8, 115.36971852244196]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 18 | global_acc: 96.709% | global_loss: 2.8416993618011475 | global_f1: 0.9549795361527967 | global_precision: 0.963302752293578 | global_recall: 0.9467989179440938 | global_auc: 0.9715107994288674| flobal_FPR: 0.05320108205590622 \n",
      "client_2\n",
      "comm_round: 18 | global_acc: 96.576% | global_loss: 2.8063910007476807 | global_f1: 0.9536244934714093 | global_precision: 0.9523381294964028 | global_recall: 0.9549143372407575 | global_auc: 0.9750015550873674| flobal_FPR: 0.04508566275924256 \n",
      "client_3\n",
      "comm_round: 18 | global_acc: 96.011% | global_loss: 3.522188186645508 | global_f1: 0.9469964664310955 | global_precision: 0.9281385281385282 | global_recall: 0.9666366095581606 | global_auc: 0.9682819157346826| flobal_FPR: 0.033363390441839495 \n",
      "client_4\n",
      "comm_round: 18 | global_acc: 96.642% | global_loss: 2.860969066619873 | global_f1: 0.9544429409111412 | global_precision: 0.9548736462093863 | global_recall: 0.9540126239855726 | global_auc: 0.9705148312599627| flobal_FPR: 0.045987376014427414 \n",
      "client_5\n",
      "comm_round: 18 | global_acc: 95.911% | global_loss: 3.4745821952819824 | global_f1: 0.9455992923485184 | global_precision: 0.9279513888888888 | global_recall: 0.9639314697926059 | global_auc: 0.9662750220679956| flobal_FPR: 0.03606853020739405 \n",
      "client_6\n",
      "comm_round: 18 | global_acc: 96.110% | global_loss: 3.5275871753692627 | global_f1: 0.9480692410119841 | global_precision: 0.9335664335664335 | global_recall: 0.9630297565374211 | global_auc: 0.9680877078771941| flobal_FPR: 0.0369702434625789 \n",
      "client_7\n",
      "comm_round: 18 | global_acc: 96.310% | global_loss: 3.1197171211242676 | global_f1: 0.9491525423728814 | global_precision: 0.9646182495344506 | global_recall: 0.9341749323715058 | global_auc: 0.9683897034697678| flobal_FPR: 0.06582506762849413 \n",
      "client_8\n",
      "comm_round: 18 | global_acc: 95.944% | global_loss: 3.325690984725952 | global_f1: 0.945729537366548 | global_precision: 0.9332748024582967 | global_recall: 0.9585211902614968 | global_auc: 0.971473524815633| flobal_FPR: 0.04147880973850315 \n",
      "client_9\n",
      "comm_round: 18 | global_acc: 96.609% | global_loss: 2.6133415699005127 | global_f1: 0.9545859305431877 | global_precision: 0.9428320140721196 | global_recall: 0.9666366095581606 | global_auc: 0.9774797233226542| flobal_FPR: 0.033363390441839495 \n",
      "client_10\n",
      "comm_round: 18 | global_acc: 95.878% | global_loss: 3.468761682510376 | global_f1: 0.9442946990116802 | global_precision: 0.9409131602506714 | global_recall: 0.9477006311992786 | global_auc: 0.9659549827136014| flobal_FPR: 0.05229936880072137 \n",
      "[]\n",
      "[[[2, 191.75027527872246], [3, 204.68952180634693], [6, 216.93400229248618], [8, 238.76434758468793]], [[1, 43.203689833667994], [5, 51.82426451447207]], [[4, 123.82164287288971], [7, 119.9583990137223], [9, 140.26119470778238], [10, 168.6734087925068]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 19 | global_acc: 95.977% | global_loss: 3.417880058288574 | global_f1: 0.9462938304482912 | global_precision: 0.9318181818181818 | global_recall: 0.9612263300270514 | global_auc: 0.9687102176599994| flobal_FPR: 0.0387736699729486 \n",
      "client_2\n",
      "comm_round: 19 | global_acc: 96.243% | global_loss: 3.335742473602295 | global_f1: 0.9492590929501572 | global_precision: 0.945438282647585 | global_recall: 0.9531109107303878 | global_auc: 0.9688483948886772| flobal_FPR: 0.046889089269612265 \n",
      "client_3\n",
      "comm_round: 19 | global_acc: 96.609% | global_loss: 3.07517671585083 | global_f1: 0.9545859305431877 | global_precision: 0.9428320140721196 | global_recall: 0.9666366095581606 | global_auc: 0.9715188716381029| flobal_FPR: 0.033363390441839495 \n",
      "client_4\n",
      "comm_round: 19 | global_acc: 96.543% | global_loss: 2.8657262325286865 | global_f1: 0.9522935779816515 | global_precision: 0.969187675070028 | global_recall: 0.9359783588818755 | global_auc: 0.9697937930409011| flobal_FPR: 0.06402164111812443 \n",
      "client_5\n",
      "comm_round: 19 | global_acc: 96.509% | global_loss: 3.030972957611084 | global_f1: 0.9525959367945824 | global_precision: 0.953887884267631 | global_recall: 0.951307484220018 | global_auc: 0.9698495862517932| flobal_FPR: 0.04869251577998197 \n",
      "client_6\n",
      "comm_round: 19 | global_acc: 96.343% | global_loss: 3.1707425117492676 | global_f1: 0.9506726457399103 | global_precision: 0.9455842997323818 | global_recall: 0.9558160504959423 | global_auc: 0.9693403248162029| flobal_FPR: 0.04418394950405771 \n",
      "client_7\n",
      "comm_round: 19 | global_acc: 96.310% | global_loss: 3.3806943893432617 | global_f1: 0.9508196721311475 | global_precision: 0.9346689895470384 | global_recall: 0.9675383228133454 | global_auc: 0.9699884757342268| flobal_FPR: 0.032461677186654644 \n",
      "client_8\n",
      "comm_round: 19 | global_acc: 95.844% | global_loss: 3.7157044410705566 | global_f1: 0.9441215914170765 | global_precision: 0.9361702127659575 | global_recall: 0.9522091974752029 | global_auc: 0.9623089557362781| flobal_FPR: 0.047790802524797116 \n",
      "client_9\n",
      "comm_round: 19 | global_acc: 96.443% | global_loss: 2.6774282455444336 | global_f1: 0.9508949059201468 | global_precision: 0.9682242990654205 | global_recall: 0.9341749323715058 | global_auc: 0.9724388660730269| flobal_FPR: 0.06582506762849413 \n",
      "client_10\n",
      "comm_round: 19 | global_acc: 95.878% | global_loss: 3.564121723175049 | global_f1: 0.9442946990116802 | global_precision: 0.9409131602506714 | global_recall: 0.9477006311992786 | global_auc: 0.9647519861195989| flobal_FPR: 0.05229936880072137 \n",
      "[]\n",
      "[[[4, 54.32841460733229], [5, 64.11541125432272], [7, 81.6027336911612], [9, 68.54907409302625]], [[1, 331.50544707030485], [2, 306.77720530889206]], [[3, 149.1110368168369], [6, 145.06751544708197], [8, 182.75950500451972], [10, 142.29807670480932]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 20 | global_acc: 96.875% | global_loss: 2.7985095977783203 | global_f1: 0.9579606440071556 | global_precision: 0.9503105590062112 | global_recall: 0.9657348963029756 | global_auc: 0.9731098091112451| flobal_FPR: 0.034265103697024346 \n",
      "client_2\n",
      "comm_round: 20 | global_acc: 96.310% | global_loss: 3.2330970764160156 | global_f1: 0.9505127061970574 | global_precision: 0.9400352733686067 | global_recall: 0.9612263300270514 | global_auc: 0.9703500632243918| flobal_FPR: 0.0387736699729486 \n",
      "client_3\n",
      "comm_round: 20 | global_acc: 96.509% | global_loss: 3.222291946411133 | global_f1: 0.9535603715170279 | global_precision: 0.9357638888888888 | global_recall: 0.9720468890892696 | global_auc: 0.9704561890340462| flobal_FPR: 0.027953110910730387 \n",
      "client_4\n",
      "comm_round: 20 | global_acc: 95.080% | global_loss: 4.241628646850586 | global_f1: 0.9324817518248174 | global_precision: 0.9436749769159741 | global_recall: 0.9215509467989179 | global_auc: 0.954321029861951| flobal_FPR: 0.07844905320108206 \n",
      "client_5\n",
      "comm_round: 20 | global_acc: 96.476% | global_loss: 3.142439603805542 | global_f1: 0.9533039647577092 | global_precision: 0.9319552110249785 | global_recall: 0.975653742110009 | global_auc: 0.9714566681434063| flobal_FPR: 0.024346257889990983 \n",
      "client_6\n",
      "comm_round: 20 | global_acc: 95.246% | global_loss: 4.152084827423096 | global_f1: 0.9349704411095954 | global_precision: 0.9431192660550459 | global_recall: 0.9269612263300271 | global_auc: 0.9602249012460166| flobal_FPR: 0.07303877366997295 \n",
      "client_7\n",
      "comm_round: 20 | global_acc: 96.011% | global_loss: 3.4576165676116943 | global_f1: 0.9451553930530164 | global_precision: 0.9582947173308619 | global_recall: 0.9323715058611362 | global_auc: 0.9675224158127932| flobal_FPR: 0.06762849413886383 \n",
      "client_8\n",
      "comm_round: 20 | global_acc: 95.379% | global_loss: 3.9016177654266357 | global_f1: 0.935796766743649 | global_precision: 0.959280303030303 | global_recall: 0.9134355275022543 | global_auc: 0.9585029090817577| flobal_FPR: 0.08656447249774572 \n",
      "client_9\n",
      "comm_round: 20 | global_acc: 96.509% | global_loss: 2.990967035293579 | global_f1: 0.9534780682321665 | global_precision: 0.9372822299651568 | global_recall: 0.9702434625788999 | global_auc: 0.9718241910815383| flobal_FPR: 0.029756537421100092 \n",
      "client_10\n",
      "comm_round: 20 | global_acc: 95.878% | global_loss: 3.559462308883667 | global_f1: 0.9442946990116802 | global_precision: 0.9409131602506714 | global_recall: 0.9477006311992786 | global_auc: 0.965098853698805| flobal_FPR: 0.05229936880072137 \n",
      "[]\n",
      "[[[7, 30.955284483156415]], [[1, 175.54923843946537], [2, 166.49653212683023], [4, 158.10891433948092], [8, 140.31212216206046], [9, 138.76298858798816], [10, 158.83730665405082]], [[3, 95.87454687814902], [5, 99.26537318920539], [6, 107.61029073003657]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 21 | global_acc: 96.277% | global_loss: 3.337470531463623 | global_f1: 0.9501779359430605 | global_precision: 0.9376646180860404 | global_recall: 0.9630297565374211 | global_auc: 0.9691318718835931| flobal_FPR: 0.0369702434625789 \n",
      "client_2\n",
      "comm_round: 21 | global_acc: 95.346% | global_loss: 4.139777183532715 | global_f1: 0.9347623485554519 | global_precision: 0.9672131147540983 | global_recall: 0.9044183949504058 | global_auc: 0.9522241548040804| flobal_FPR: 0.09558160504959423 \n",
      "client_3\n",
      "comm_round: 21 | global_acc: 96.941% | global_loss: 2.936513662338257 | global_f1: 0.958295557570263 | global_precision: 0.9635369188696444 | global_recall: 0.9531109107303878 | global_auc: 0.9680893698026249| flobal_FPR: 0.046889089269612265 \n",
      "client_4\n",
      "comm_round: 21 | global_acc: 95.711% | global_loss: 3.871513843536377 | global_f1: 0.9428444838280903 | global_precision: 0.926829268292683 | global_recall: 0.9594229035166817 | global_auc: 0.9622111395537777| flobal_FPR: 0.0405770964833183 \n",
      "client_5\n",
      "comm_round: 21 | global_acc: 97.041% | global_loss: 2.613880157470703 | global_f1: 0.9600359227660531 | global_precision: 0.9561717352415027 | global_recall: 0.9639314697926059 | global_auc: 0.9745103374135977| flobal_FPR: 0.03606853020739405 \n",
      "client_6\n",
      "comm_round: 21 | global_acc: 96.775% | global_loss: 2.795032024383545 | global_f1: 0.9566383549396513 | global_precision: 0.9485815602836879 | global_recall: 0.9648331830477908 | global_auc: 0.9737577226113503| flobal_FPR: 0.0351668169522092 \n",
      "client_7\n",
      "comm_round: 21 | global_acc: 91.988% | global_loss: 7.714669704437256 | global_f1: 0.899625156184923 | global_precision: 0.8359133126934984 | global_recall: 0.9738503155996393 | global_auc: 0.9347943557213683| flobal_FPR: 0.026149684400360685 \n",
      "client_8\n",
      "comm_round: 21 | global_acc: 96.476% | global_loss: 3.0403549671173096 | global_f1: 0.9526362823949955 | global_precision: 0.9441984056687334 | global_recall: 0.9612263300270514 | global_auc: 0.9725991231681427| flobal_FPR: 0.0387736699729486 \n",
      "client_9\n",
      "comm_round: 21 | global_acc: 96.044% | global_loss: 3.6797690391540527 | global_f1: 0.947784115840281 | global_precision: 0.9230769230769231 | global_recall: 0.9738503155996393 | global_auc: 0.9665520887791068| flobal_FPR: 0.026149684400360685 \n",
      "client_10\n",
      "comm_round: 21 | global_acc: 95.878% | global_loss: 3.640230178833008 | global_f1: 0.9443447037701975 | global_precision: 0.9401251117068812 | global_recall: 0.9486023444544635 | global_auc: 0.9640523155132192| flobal_FPR: 0.05139765554553652 \n",
      "[]\n",
      "[[[2, 106.48591125118514], [3, 120.87354017339085], [7, 94.29776772600127], [8, 90.42716833020393], [9, 133.75306993053286]], [[5, 216.2246772194576], [6, 182.18824306925418]], [[1, 60.673076213448155], [4, 36.36028256519657], [10, 34.26400819954543]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 22 | global_acc: 94.681% | global_loss: 4.847298622131348 | global_f1: 0.9292661361626878 | global_precision: 0.9115351257588898 | global_recall: 0.9477006311992786 | global_auc: 0.9551204159941805| flobal_FPR: 0.05229936880072137 \n",
      "client_2\n",
      "comm_round: 22 | global_acc: 96.376% | global_loss: 3.4656805992126465 | global_f1: 0.9515770768547313 | global_precision: 0.9378283712784589 | global_recall: 0.9657348963029756 | global_auc: 0.9677306313274843| flobal_FPR: 0.034265103697024346 \n",
      "client_3\n",
      "comm_round: 22 | global_acc: 96.243% | global_loss: 3.2681918144226074 | global_f1: 0.9490761604326274 | global_precision: 0.9486486486486486 | global_recall: 0.9495040577096483 | global_auc: 0.9674459672429749| flobal_FPR: 0.05049594229035167 \n",
      "client_4\n",
      "comm_round: 22 | global_acc: 94.847% | global_loss: 4.713699817657471 | global_f1: 0.9331031506258092 | global_precision: 0.8948675496688742 | global_recall: 0.9747520288548241 | global_auc: 0.9584983981413024| flobal_FPR: 0.025247971145175834 \n",
      "client_5\n",
      "comm_round: 22 | global_acc: 94.814% | global_loss: 4.852392673492432 | global_f1: 0.9297929792979297 | global_precision: 0.9281221922731356 | global_recall: 0.9314697926059513 | global_auc: 0.9491412831298901| flobal_FPR: 0.06853020739404869 \n",
      "client_6\n",
      "comm_round: 22 | global_acc: 96.941% | global_loss: 2.813873291015625 | global_f1: 0.9587443946188341 | global_precision: 0.9536128456735058 | global_recall: 0.9639314697926059 | global_auc: 0.9721409065850708| flobal_FPR: 0.03606853020739405 \n",
      "client_7\n",
      "comm_round: 22 | global_acc: 94.714% | global_loss: 5.001280307769775 | global_f1: 0.9282167042889391 | global_precision: 0.9294755877034359 | global_recall: 0.9269612263300271 | global_auc: 0.9480021519560151| flobal_FPR: 0.07303877366997295 \n",
      "client_8\n",
      "comm_round: 22 | global_acc: 96.842% | global_loss: 3.0437188148498535 | global_f1: 0.9576082106202588 | global_precision: 0.9478798586572438 | global_recall: 0.9675383228133454 | global_auc: 0.9700243258399489| flobal_FPR: 0.032461677186654644 \n",
      "client_9\n",
      "comm_round: 22 | global_acc: 96.376% | global_loss: 3.2179114818573 | global_f1: 0.9510112359550562 | global_precision: 0.9480286738351255 | global_recall: 0.9540126239855726 | global_auc: 0.9680867582055195| flobal_FPR: 0.045987376014427414 \n",
      "client_10\n",
      "comm_round: 22 | global_acc: 95.844% | global_loss: 3.662121057510376 | global_f1: 0.9439210408254822 | global_precision: 0.9392857142857143 | global_recall: 0.9486023444544635 | global_auc: 0.963285693053769| flobal_FPR: 0.05139765554553652 \n",
      "[]\n",
      "[[[2, 163.80540511857157], [4, 118.40523819389296], [7, 138.49011244935946]], [[1, 58.82997463277291], [3, 42.16867137316572], [5, 88.21366617954365], [10, 34.103276040030266]], [[6, 227.14187760513343], [8, 188.82013997340098], [9, 200.25545850710927]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 23 | global_acc: 94.614% | global_loss: 5.000899791717529 | global_f1: 0.9244402985074627 | global_precision: 0.957487922705314 | global_recall: 0.8935978358881875 | global_auc: 0.944308403977035| flobal_FPR: 0.10640216411181244 \n",
      "client_2\n",
      "comm_round: 23 | global_acc: 96.343% | global_loss: 3.3526811599731445 | global_f1: 0.9504057709648331 | global_precision: 0.9504057709648331 | global_recall: 0.9504057709648331 | global_auc: 0.9663108721737177| flobal_FPR: 0.04959422903516682 \n",
      "client_3\n",
      "comm_round: 23 | global_acc: 94.947% | global_loss: 4.724268436431885 | global_f1: 0.9305936073059361 | global_precision: 0.9426456984273821 | global_recall: 0.9188458070333634 | global_auc: 0.9487882426847978| flobal_FPR: 0.0811541929666366 \n",
      "client_4\n",
      "comm_round: 23 | global_acc: 95.745% | global_loss: 3.621703863143921 | global_f1: 0.9402427637721756 | global_precision: 0.9748305905130688 | global_recall: 0.9080252479711451 | global_auc: 0.9605803158702956| flobal_FPR: 0.09197475202885483 \n",
      "client_5\n",
      "comm_round: 23 | global_acc: 96.210% | global_loss: 3.7036709785461426 | global_f1: 0.9496910856134156 | global_precision: 0.9299913569576491 | global_recall: 0.9702434625788999 | global_auc: 0.9651408766704132| flobal_FPR: 0.029756537421100092 \n",
      "client_6\n",
      "comm_round: 23 | global_acc: 94.814% | global_loss: 4.8111443519592285 | global_f1: 0.9313984168865436 | global_precision: 0.9090128755364807 | global_recall: 0.9549143372407575 | global_auc: 0.9560026609800327| flobal_FPR: 0.04508566275924256 \n",
      "client_7\n",
      "comm_round: 23 | global_acc: 95.080% | global_loss: 4.64674186706543 | global_f1: 0.9362068965517241 | global_precision: 0.8967795210569777 | global_recall: 0.9792605951307484 | global_auc: 0.9593001584527191| flobal_FPR: 0.020739404869251576 \n",
      "client_8\n",
      "comm_round: 23 | global_acc: 95.844% | global_loss: 3.596282958984375 | global_f1: 0.9444691248334074 | global_precision: 0.9308231173380035 | global_recall: 0.9585211902614968 | global_auc: 0.9677586466418897| flobal_FPR: 0.04147880973850315 \n",
      "client_9\n",
      "comm_round: 23 | global_acc: 96.144% | global_loss: 3.2778751850128174 | global_f1: 0.947176684881603 | global_precision: 0.9567617295308187 | global_recall: 0.9377817853922452 | global_auc: 0.9656199860303296| flobal_FPR: 0.062218214607754736 \n",
      "client_10\n",
      "comm_round: 23 | global_acc: 95.844% | global_loss: 3.7308456897735596 | global_f1: 0.9439210408254822 | global_precision: 0.9392857142857143 | global_recall: 0.9486023444544635 | global_auc: 0.9628144184851691| flobal_FPR: 0.05139765554553652 \n",
      "[]\n",
      "[[[2, 216.72800363753362], [3, 195.4132235747879], [6, 198.7742438515799], [10, 212.38917958718542]], [[4, 85.89865297135643], [8, 68.69535110412791], [9, 82.02103734836649]], [[1, 174.5036767852194], [5, 152.84746270342356], [7, 130.5267407583449]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 24 | global_acc: 94.581% | global_loss: 5.187888145446777 | global_f1: 0.9247113163972286 | global_precision: 0.9479166666666666 | global_recall: 0.9026149684400361 | global_auc: 0.9414430071163646| flobal_FPR: 0.09738503155996393 \n",
      "client_2\n",
      "comm_round: 24 | global_acc: 94.980% | global_loss: 4.80954122543335 | global_f1: 0.9344902386117138 | global_precision: 0.9005016722408027 | global_recall: 0.9711451758340848 | global_auc: 0.9569941182084823| flobal_FPR: 0.028854824165915238 \n",
      "client_3\n",
      "comm_round: 24 | global_acc: 95.711% | global_loss: 4.038267612457275 | global_f1: 0.9429960229783472 | global_precision: 0.9246100519930676 | global_recall: 0.9621280432822362 | global_auc: 0.9618260476896624| flobal_FPR: 0.03787195671776375 \n",
      "client_4\n",
      "comm_round: 24 | global_acc: 95.678% | global_loss: 3.871999979019165 | global_f1: 0.941546762589928 | global_precision: 0.9390134529147982 | global_recall: 0.9440937781785392 | global_auc: 0.9608110860872626| flobal_FPR: 0.05590622182146077 \n",
      "client_5\n",
      "comm_round: 24 | global_acc: 94.980% | global_loss: 4.555986404418945 | global_f1: 0.9305747126436782 | global_precision: 0.949343339587242 | global_recall: 0.9125338142470695 | global_auc: 0.9532590595116504| flobal_FPR: 0.08746618575293057 \n",
      "client_6\n",
      "comm_round: 24 | global_acc: 94.548% | global_loss: 5.029916286468506 | global_f1: 0.9288194444444444 | global_precision: 0.895397489539749 | global_recall: 0.9648331830477908 | global_auc: 0.9543222169515444| flobal_FPR: 0.0351668169522092 \n",
      "client_7\n",
      "comm_round: 24 | global_acc: 95.811% | global_loss: 3.972898006439209 | global_f1: 0.943800178412132 | global_precision: 0.9338040600176523 | global_recall: 0.9540126239855726 | global_auc: 0.9602324986194147| flobal_FPR: 0.045987376014427414 \n",
      "client_8\n",
      "comm_round: 24 | global_acc: 95.844% | global_loss: 3.9777426719665527 | global_f1: 0.9435665914221218 | global_precision: 0.9448462929475587 | global_recall: 0.9422903516681695 | global_auc: 0.957599533901142| flobal_FPR: 0.057709648331830475 \n",
      "client_9\n",
      "comm_round: 24 | global_acc: 96.144% | global_loss: 3.294407844543457 | global_f1: 0.9482142857142858 | global_precision: 0.9389920424403183 | global_recall: 0.957619477006312 | global_auc: 0.9685960196411095| flobal_FPR: 0.04238052299368801 \n",
      "client_10\n",
      "comm_round: 24 | global_acc: 95.778% | global_loss: 3.7437071800231934 | global_f1: 0.9430748543254146 | global_precision: 0.9376114081996435 | global_recall: 0.9486023444544635 | global_auc: 0.9628217784406486| flobal_FPR: 0.05139765554553652 \n",
      "[]\n",
      "[[[2, 205.284187477235], [5, 158.07750839584082], [6, 229.4292242357627], [9, 194.21617414061993], [10, 177.81672331423025]], [[1, 55.071752736409096], [7, 116.07496637991183], [8, 53.857149197372635]], [[3, 284.2869797521529], [4, 275.1006514229662]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 25 | global_acc: 95.246% | global_loss: 4.683444499969482 | global_f1: 0.9342528735632184 | global_precision: 0.9530956848030019 | global_recall: 0.9161406672678089 | global_auc: 0.9459142987790545| flobal_FPR: 0.08385933273219116 \n",
      "client_2\n",
      "comm_round: 25 | global_acc: 95.346% | global_loss: 4.267716884613037 | global_f1: 0.9386503067484663 | global_precision: 0.9130434782608695 | global_recall: 0.9657348963029756 | global_auc: 0.9618569120190922| flobal_FPR: 0.034265103697024346 \n",
      "client_3\n",
      "comm_round: 25 | global_acc: 90.193% | global_loss: 9.653355598449707 | global_f1: 0.880130028443722 | global_precision: 0.8010355029585798 | global_recall: 0.9765554553651938 | global_auc: 0.9189887326204148| flobal_FPR: 0.023444544634806132 \n",
      "client_4\n",
      "comm_round: 25 | global_acc: 90.226% | global_loss: 9.333035469055176 | global_f1: 0.8765743073047859 | global_precision: 0.8201099764336214 | global_recall: 0.9413886384129847 | global_auc: 0.9174108531328006| flobal_FPR: 0.058611361587015326 \n",
      "client_5\n",
      "comm_round: 25 | global_acc: 95.678% | global_loss: 4.102604866027832 | global_f1: 0.941123188405797 | global_precision: 0.9454049135577798 | global_recall: 0.9368800721370604 | global_auc: 0.9572859048305525| flobal_FPR: 0.06311992786293959 \n",
      "client_6\n",
      "comm_round: 25 | global_acc: 95.379% | global_loss: 4.088039875030518 | global_f1: 0.9361506660542032 | global_precision: 0.954119850187266 | global_recall: 0.9188458070333634 | global_auc: 0.9609478388084278| flobal_FPR: 0.0811541929666366 \n",
      "client_7\n",
      "comm_round: 25 | global_acc: 95.578% | global_loss: 4.163597106933594 | global_f1: 0.9409675987572127 | global_precision: 0.9265734265734266 | global_recall: 0.9558160504959423 | global_auc: 0.9595511091927743| flobal_FPR: 0.04418394950405771 \n",
      "client_8\n",
      "comm_round: 25 | global_acc: 95.711% | global_loss: 4.0163254737854 | global_f1: 0.9411227749885895 | global_precision: 0.9528650646950092 | global_recall: 0.9296663660955816 | global_auc: 0.9574737024042362| flobal_FPR: 0.0703336339044184 \n",
      "client_9\n",
      "comm_round: 25 | global_acc: 95.878% | global_loss: 3.842052936553955 | global_f1: 0.9442946990116802 | global_precision: 0.9409131602506714 | global_recall: 0.9477006311992786 | global_auc: 0.9615639383074286| flobal_FPR: 0.05229936880072137 \n",
      "client_10\n",
      "comm_round: 25 | global_acc: 95.711% | global_loss: 3.790360689163208 | global_f1: 0.9421265141318976 | global_precision: 0.9375 | global_recall: 0.9467989179440938 | global_auc: 0.9621793255526733| flobal_FPR: 0.05320108205590622 \n",
      "[]\n",
      "[[[2, 29.78023018110538], [5, 103.08630898237115], [6, 29.814832705267044], [10, 61.88263969948408]], [[3, 213.30243647677236], [4, 252.94966766654005], [8, 248.6308681012834], [9, 238.60703232669124]], [[1, 179.07094260885867], [7, 164.08554013632332]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 26 | global_acc: 94.648% | global_loss: 5.197789192199707 | global_f1: 0.923660502607871 | global_precision: 0.974 | global_recall: 0.8782687105500451 | global_auc: 0.936485958392035| flobal_FPR: 0.12173128944995491 \n",
      "client_2\n",
      "comm_round: 26 | global_acc: 93.850% | global_loss: 5.913657188415527 | global_f1: 0.9215103945693677 | global_precision: 0.8701923076923077 | global_recall: 0.9792605951307484 | global_auc: 0.9493369154948905| flobal_FPR: 0.020739404869251576 \n",
      "client_3\n",
      "comm_round: 26 | global_acc: 94.282% | global_loss: 5.495010852813721 | global_f1: 0.9239610963748895 | global_precision: 0.9063313096270599 | global_recall: 0.9422903516681695 | global_auc: 0.9447960603820243| flobal_FPR: 0.057709648331830475 \n",
      "client_4\n",
      "comm_round: 26 | global_acc: 95.445% | global_loss: 4.2584662437438965 | global_f1: 0.9389210878288007 | global_precision: 0.9285714285714286 | global_recall: 0.9495040577096483 | global_auc: 0.9589335851862615| flobal_FPR: 0.05049594229035167 \n",
      "client_5\n",
      "comm_round: 26 | global_acc: 88.830% | global_loss: 10.640471458435059 | global_f1: 0.8621821164889253 | global_precision: 0.7908201655379985 | global_recall: 0.9477006311992786 | global_auc: 0.9061439483834451| flobal_FPR: 0.05229936880072137 \n",
      "client_6\n",
      "comm_round: 26 | global_acc: 94.980% | global_loss: 4.798952102661133 | global_f1: 0.9315813321250567 | global_precision: 0.936247723132969 | global_recall: 0.9269612263300271 | global_auc: 0.9500467950717738| flobal_FPR: 0.07303877366997295 \n",
      "client_7\n",
      "comm_round: 26 | global_acc: 94.814% | global_loss: 4.683984756469727 | global_f1: 0.9322916666666666 | global_precision: 0.8987447698744769 | global_recall: 0.9684400360685302 | global_auc: 0.9574725153146428| flobal_FPR: 0.031559963931469794 \n",
      "client_8\n",
      "comm_round: 26 | global_acc: 94.215% | global_loss: 5.34206485748291 | global_f1: 0.9186915887850468 | global_precision: 0.953443258971872 | global_recall: 0.8863841298467088 | global_auc: 0.9438504248118819| flobal_FPR: 0.11361587015329125 \n",
      "client_9\n",
      "comm_round: 26 | global_acc: 95.479% | global_loss: 4.168445110321045 | global_f1: 0.9377289377289377 | global_precision: 0.9525581395348838 | global_recall: 0.9233543733092876 | global_auc: 0.9582863839399123| flobal_FPR: 0.07664562669071236 \n",
      "client_10\n",
      "comm_round: 26 | global_acc: 95.711% | global_loss: 3.8082025051116943 | global_f1: 0.9421265141318976 | global_precision: 0.9375 | global_recall: 0.9467989179440938 | global_auc: 0.9621949951353068| flobal_FPR: 0.05320108205590622 \n",
      "[]\n",
      "[[[5, 76.82188870777442], [7, 55.84346280339835], [9, 39.688063898348005], [10, 37.9416601839079]], [[1, 189.05001948322726], [2, 189.61079681225112], [3, 265.2679462274958], [6, 241.91385425142687]], [[4, 131.11542494271959], [8, 110.64339045422737]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 27 | global_acc: 94.382% | global_loss: 5.289859294891357 | global_f1: 0.9234254644313548 | global_precision: 0.9280510018214936 | global_recall: 0.9188458070333634 | global_auc: 0.9423045967432908| flobal_FPR: 0.0811541929666366 \n",
      "client_2\n",
      "comm_round: 27 | global_acc: 94.382% | global_loss: 5.472297668457031 | global_f1: 0.9263616557734204 | global_precision: 0.8962900505902193 | global_recall: 0.9585211902614968 | global_auc: 0.9488416617165031| flobal_FPR: 0.04147880973850315 \n",
      "client_3\n",
      "comm_round: 27 | global_acc: 93.484% | global_loss: 6.262140274047852 | global_f1: 0.9155900086132643 | global_precision: 0.8763396537510305 | global_recall: 0.9585211902614968 | global_auc: 0.9429266316902587| flobal_FPR: 0.04147880973850315 \n",
      "client_4\n",
      "comm_round: 27 | global_acc: 94.415% | global_loss: 5.273655891418457 | global_f1: 0.922723091076357 | global_precision: 0.9417840375586854 | global_recall: 0.9044183949504058 | global_auc: 0.9400420039781746| flobal_FPR: 0.09558160504959423 \n",
      "client_5\n",
      "comm_round: 27 | global_acc: 94.448% | global_loss: 5.313865661621094 | global_f1: 0.9280482550624732 | global_precision: 0.8886138613861386 | global_recall: 0.9711451758340848 | global_auc: 0.9526735869241607| flobal_FPR: 0.028854824165915238 \n",
      "client_6\n",
      "comm_round: 27 | global_acc: 96.144% | global_loss: 3.7219643592834473 | global_f1: 0.9471285323609845 | global_precision: 0.9576036866359448 | global_recall: 0.9368800721370604 | global_auc: 0.9576926017252685| flobal_FPR: 0.06311992786293959 \n",
      "client_7\n",
      "comm_round: 27 | global_acc: 95.545% | global_loss: 4.101050853729248 | global_f1: 0.9401251117068811 | global_precision: 0.9317980513728964 | global_recall: 0.9486023444544635 | global_auc: 0.9633692641611479| flobal_FPR: 0.05139765554553652 \n",
      "client_8\n",
      "comm_round: 27 | global_acc: 96.443% | global_loss: 3.4208555221557617 | global_f1: 0.9520824003582624 | global_precision: 0.9457295373665481 | global_recall: 0.9585211902614968 | global_auc: 0.9653272497365848| flobal_FPR: 0.04147880973850315 \n",
      "client_9\n",
      "comm_round: 27 | global_acc: 91.656% | global_loss: 8.0790376663208 | global_f1: 0.8810990052108005 | global_precision: 0.9281437125748503 | global_recall: 0.8385933273219116 | global_auc: 0.9059193510323644| flobal_FPR: 0.16140667267808836 \n",
      "client_10\n",
      "comm_round: 27 | global_acc: 95.711% | global_loss: 3.856696367263794 | global_f1: 0.9421265141318976 | global_precision: 0.9375 | global_recall: 0.9467989179440938 | global_auc: 0.9615489809785512| flobal_FPR: 0.05320108205590622 \n",
      "[]\n",
      "[[[2, 204.7167449913826], [5, 202.3435337316282], [8, 213.83264226871424], [9, 163.86451234552442]], [[3, 119.32658332702874], [4, 83.55467374694965], [7, 114.47966038060194], [10, 90.38614775896403]], [[1, 62.93718352587332], [6, 35.870134036531574]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 28 | global_acc: 94.548% | global_loss: 5.232685565948486 | global_f1: 0.9257918552036198 | global_precision: 0.9291553133514986 | global_recall: 0.9224526600541028 | global_auc: 0.9435992366539078| flobal_FPR: 0.0775473399458972 \n",
      "client_2\n",
      "comm_round: 28 | global_acc: 93.750% | global_loss: 5.975354194641113 | global_f1: 0.9162956366874443 | global_precision: 0.9050131926121372 | global_recall: 0.9278629395852119 | global_auc: 0.9388494537725944| flobal_FPR: 0.0721370604147881 \n",
      "client_3\n",
      "comm_round: 28 | global_acc: 95.479% | global_loss: 4.34279203414917 | global_f1: 0.9376146788990826 | global_precision: 0.954248366013072 | global_recall: 0.9215509467989179 | global_auc: 0.9507307960955199| flobal_FPR: 0.07844905320108206 \n",
      "client_4\n",
      "comm_round: 28 | global_acc: 87.699% | global_loss: 12.082426071166992 | global_f1: 0.8502024291497976 | global_precision: 0.7714915503306392 | global_recall: 0.9467989179440938 | global_auc: 0.8935275601842554| flobal_FPR: 0.05320108205590622 \n",
      "client_5\n",
      "comm_round: 28 | global_acc: 93.684% | global_loss: 5.870334625244141 | global_f1: 0.9166666666666666 | global_precision: 0.892399658411614 | global_recall: 0.9422903516681695 | global_auc: 0.9447730308439115| flobal_FPR: 0.057709648331830475 \n",
      "client_6\n",
      "comm_round: 28 | global_acc: 94.681% | global_loss: 4.690455436706543 | global_f1: 0.9244570349386214 | global_precision: 0.9702675916749257 | global_recall: 0.8827772768259693 | global_auc: 0.9471935065249565| flobal_FPR: 0.11722272317403065 \n",
      "client_7\n",
      "comm_round: 28 | global_acc: 94.182% | global_loss: 5.558004856109619 | global_f1: 0.9246663796814464 | global_precision: 0.8846787479406919 | global_recall: 0.9684400360685302 | global_auc: 0.9517001734575314| flobal_FPR: 0.031559963931469794 \n",
      "client_8\n",
      "comm_round: 28 | global_acc: 95.246% | global_loss: 4.42514705657959 | global_f1: 0.9348519362186789 | global_precision: 0.9447513812154696 | global_recall: 0.9251577998196574 | global_auc: 0.951374910908926| flobal_FPR: 0.07484220018034266 \n",
      "client_9\n",
      "comm_round: 28 | global_acc: 94.980% | global_loss: 4.6517767906188965 | global_f1: 0.9339168490153172 | global_precision: 0.907312925170068 | global_recall: 0.9621280432822362 | global_auc: 0.9577417472344374| flobal_FPR: 0.03787195671776375 \n",
      "client_10\n",
      "comm_round: 28 | global_acc: 95.711% | global_loss: 3.9276957511901855 | global_f1: 0.9421265141318976 | global_precision: 0.9375 | global_recall: 0.9467989179440938 | global_auc: 0.9610767567382766| flobal_FPR: 0.05320108205590622 \n",
      "[]\n",
      "[[[2, 170.96350156729744], [3, 178.3854208617701], [5, 178.55441718993876], [6, 132.76620562871534], [7, 142.5816110280459], [10, 167.8935376785067]], [[4, 276.84652202403026], [8, 285.81879766165486]], [[1, 79.61659728904829], [9, 95.57726126591567]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 29 | global_acc: 91.822% | global_loss: 7.874580383300781 | global_f1: 0.8869485294117648 | global_precision: 0.9044048734770385 | global_recall: 0.8701532912533815 | global_auc: 0.9137166303179832| flobal_FPR: 0.12984670874661858 \n",
      "client_2\n",
      "comm_round: 29 | global_acc: 93.351% | global_loss: 6.097546577453613 | global_f1: 0.9142367066895369 | global_precision: 0.8716271463614064 | global_recall: 0.9612263300270514 | global_auc: 0.9475147329689443| flobal_FPR: 0.0387736699729486 \n",
      "client_3\n",
      "comm_round: 29 | global_acc: 93.251% | global_loss: 6.552949905395508 | global_f1: 0.9016949152542372 | global_precision: 0.9738493723849372 | global_recall: 0.8394950405770965 | global_auc: 0.9194341286358773| flobal_FPR: 0.16050495942290352 \n",
      "client_4\n",
      "comm_round: 29 | global_acc: 93.584% | global_loss: 6.176960468292236 | global_f1: 0.9141076991544281 | global_precision: 0.9024604569420035 | global_recall: 0.9260595130748422 | global_auc: 0.936276080951913| flobal_FPR: 0.0739404869251578 \n",
      "client_5\n",
      "comm_round: 29 | global_acc: 94.880% | global_loss: 4.792394638061523 | global_f1: 0.9279700654817586 | global_precision: 0.9640427599611273 | global_recall: 0.8944995491433724 | global_auc: 0.9468915109323829| flobal_FPR: 0.1055004508566276 \n",
      "client_6\n",
      "comm_round: 29 | global_acc: 94.548% | global_loss: 4.686777591705322 | global_f1: 0.9280070237050044 | global_precision: 0.9041916167664671 | global_recall: 0.9531109107303878 | global_auc: 0.961377565241257| flobal_FPR: 0.046889089269612265 \n",
      "client_7\n",
      "comm_round: 29 | global_acc: 94.714% | global_loss: 5.0164618492126465 | global_f1: 0.9298632554036171 | global_precision: 0.9101899827288429 | global_recall: 0.9504057709648331 | global_auc: 0.9510439503302721| flobal_FPR: 0.04959422903516682 \n",
      "client_8\n",
      "comm_round: 29 | global_acc: 95.113% | global_loss: 4.614225387573242 | global_f1: 0.9324758842443729 | global_precision: 0.950374531835206 | global_recall: 0.915238954012624 | global_auc: 0.9500541550272533| flobal_FPR: 0.08476104598737602 \n",
      "client_9\n",
      "comm_round: 29 | global_acc: 94.581% | global_loss: 5.257620811462402 | global_f1: 0.9251950435979808 | global_precision: 0.9420560747663551 | global_recall: 0.90892696122633 | global_auc: 0.940944904322953| flobal_FPR: 0.09107303877366997 \n",
      "client_10\n",
      "comm_round: 29 | global_acc: 95.711% | global_loss: 3.9668068885803223 | global_f1: 0.9421265141318976 | global_precision: 0.9375 | global_recall: 0.9467989179440938 | global_auc: 0.9608792250299266| flobal_FPR: 0.05320108205590622 \n",
      "[]\n",
      "[[[1, 181.85597201540827], [2, 189.51960504128735], [4, 234.57249584146192], [8, 222.8373835171465], [10, 204.86975961739716]], [[3, 78.55510778952949], [5, 89.26397661720544], [6, 73.7311420354897], [7, 58.55608041529831]], [[9, 288.5721165502906]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def train_model_prox(model,global_parameters, train_loader, loss_fn, optimizer,client ,rho,Z_weights,x_hats,mu=0.01):\n",
    "    model.train()\n",
    "    if global_parameters == None: \n",
    "        global_parameters=model.parameters()\n",
    "    for i in range(random.randint(1, 20)):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            for z_weight, (param, param_global) in zip(Z_weights[int(client[-1])-1], zip(model.parameters(), global_parameters)):\n",
    "                loss += (mu / 2) * torch.norm(param - param_global, p=2)**2 + torch.sum(z_weight * (param - param_global))\n",
    "\n",
    "            # Now 'inner_product' contains the inner product between model.parameters() and global_model.parameters()\n",
    "\n",
    "            loss.backward(retain_graph=True)  # Set retain_graph=True to retain the computation graph\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "def update_z_parameter(local_model,global_parameters,Z_weights,x_hats,client,rho):\n",
    "    # Update Z_weight\n",
    "    if global_parameters == None: \n",
    "        global_parameters=local_model.parameters()\n",
    "    for i, (z_weight, (param, param_global)) in enumerate(zip(Z_weights[int(client[-1])-1], zip(local_model.parameters(), global_parameters))):\n",
    "        Z_weights[int(client[-1])-1][i] += rho * (param - param_global)\n",
    "    temp = copy.copy(x_hats[int(client[-1])-1])\n",
    "\n",
    "    for i, (x_hat, z_weight, param_model) in enumerate(zip(x_hats[int(client[-1])-1], Z_weights[int(client[-1])-1], local_model.parameters())):\n",
    "        x_hats[int(client[-1])-1][i] = param_model + z_weight / rho\n",
    "\n",
    "\n",
    "\n",
    "    return Z_weights,x_hats\n",
    "\n",
    "# Example: Sparse binary backdoor pattern for a dataset with binary features\n",
    "num_features = 265\n",
    "num_modified_features = 20\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for _ in range(random.randint(1, 20)):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "K_cluster =3\n",
    "all_avg = []\n",
    "all_std = []\n",
    "target_client = 'client_3'  # Change this to the desired target client\n",
    "all_avg = []\n",
    "all_std = []\n",
    "mu = 0.01\n",
    "rho =0.01\n",
    "n_clients = [10]\n",
    "n_round = [30]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "            \n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features_column_num = use_data.shape[1] - 1\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, features_column_num)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "    \n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "            all_results = [[],[],[],[],[],[],[],[],[],[]]\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "            Z_weights=[]\n",
    "            x_hats = []\n",
    "\n",
    "            # initialize global model\n",
    "            global_model_list =list()\n",
    "            # smlp_global1 = SimpleLSTM(input_size=X.shape[1], hidden_size=128, num_layers=2, classes=1)\n",
    "            # smlp_global2 = SimpleLSTM(input_size=X.shape[1], hidden_size=128, num_layers=2, classes=1)\n",
    "            # smlp_global3 = SimpleLSTM(input_size=X.shape[1], hidden_size=128, num_layers=2, classes=1)\n",
    "\n",
    "            smlp_global1 = SimpleMLP(X.shape[1], 1)\n",
    "            smlp_global2 = SimpleMLP(X.shape[1], 1)\n",
    "            smlp_global3 = SimpleMLP(X.shape[1], 1)\n",
    "            global_model_list = [smlp_global1,smlp_global2,smlp_global3]\n",
    "            Z_weights=[]\n",
    "            x_hats = []\n",
    "            for i in range(number_of_clients):\n",
    "                Z_weight = [torch.zeros_like(param) for param in smlp_global1.parameters()]\n",
    "                Z_weights.append(Z_weight)\n",
    "                x_hat = [torch.zeros_like(param) for param in smlp_global1.parameters()]\n",
    "                x_hats.append(x_hat)\n",
    "\n",
    "            num_modified_features = 20\n",
    "            # Create a sparse binary backdoor pattern\n",
    "            backdoor_pattern = np.zeros(features_column_num)\n",
    "            modified_indices = np.random.choice(features_column_num, num_modified_features, replace=False)\n",
    "            backdoor_pattern[modified_indices] = 1  # Set modified features to 1\n",
    "            error_list_history = [[] for _ in range(10)]\n",
    "\n",
    "            cluster_group =[[],[],[]]    \n",
    "            # -----------------------------------------------\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "            \n",
    "            list_average_weights=list()\n",
    "            list_clients_weights_old =list()\n",
    "            list_clients_weights_new =list()\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # global_weights_list =list()\n",
    "                # for i in range(K_cluster):\n",
    "                #     global_weights_list.append(param.data.clone() for param in global_model_list[i].parameters())\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "                list_clients_weights_delta =list()\n",
    "                scaled_x_hats_list = list()\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                sending_time = random.uniform(1, 3)\n",
    "                UTILL_usage_list = list()\n",
    "                list_similarity_matrix = list()\n",
    "                for client in client_names:\n",
    "\n",
    "                    print(client)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    tracemalloc.start()\n",
    "                    cpu_before = psutil.cpu_percent(interval=None)\n",
    "                    cpu_freq_before = psutil.cpu_freq()\n",
    "                     \n",
    "\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "   \n",
    "                    # set local model weight to the weight of the global model\n",
    "                    # clients_batched = add_backdoor_attack(clients_batched, backdoor_pattern, target_client)\n",
    "\n",
    "                   # Find the location of the element 3\n",
    "                    location = None\n",
    "                    cluster_weights = None\n",
    "                    # print(list_average_weights)\n",
    "\n",
    "                    for i, sublist in enumerate(cluster_group):\n",
    "\n",
    "                        if int(pattern.findall(client)[0]) in sublist:\n",
    "                            location = i\n",
    "                            cluster_weights =list_average_weights[location]\n",
    "                            local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), cluster_weights)})\n",
    "                            break\n",
    "\n",
    "                    # if cluster_weights == None:\n",
    "                    #     cluster_weights = global_weights_list[0]\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "                    \n",
    "                    train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "                    train_model_prox(local_model, cluster_weights,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                    Z_weights,x_hats = update_z_parameter(local_model,cluster_weights,Z_weights,x_hats,client,rho)\n",
    "\n",
    "                    \n",
    "                    cpu_freq_after = psutil.cpu_freq()\n",
    "                    mean_cpu_freq_within_loop = (cpu_freq_before.current + cpu_freq_after.current) / 2\n",
    "                    cpu_after = psutil.cpu_percent(interval=None)\n",
    "                    memorey =tracemalloc.get_traced_memory()\n",
    "                    memorey= memorey[1]-memorey[0]\n",
    "                    tracemalloc.stop()\n",
    "                    num_used_cpus = multiprocessing.cpu_count()\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    recieved_time =random.uniform(1, 5)\n",
    "                    cpu_usage = np.abs(cpu_after-cpu_before)\n",
    "                    if cpu_usage==0:\n",
    "                        cpu_usage=0.01\n",
    "                    UTILL =sending_time + recieved_time+ alpha* elapsed_time +beta *np.log10(memorey) + gamma*np.log10(cpu_usage*mean_cpu_freq_within_loop*num_used_cpus)\n",
    "                    UTILL_usage_list.append([int(pattern.findall(client)[0]),UTILL])\n",
    "                    list_clients_weights_new.append([int(pattern.findall(client)[0]),local_model.state_dict().values()])\n",
    "                    list_clients_weights_delta.append(local_model.state_dict().items())\n",
    "\n",
    "                    # list_clients_weights_delta = \n",
    "                    torch.cuda.empty_cache()\n",
    "                    # print(client)\n",
    "                    # test global model and print out metrics after each communications round\n",
    "                    for X_test_batch, Y_test_batch in test_batched:\n",
    "                        global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, local_model, comm_round)\n",
    "                        all_results[int(pattern.findall(client)[0])-1].append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "                        error_list_history[int(pattern.findall(client)[0])-1].append(global_loss)\n",
    "                \n",
    "                # ...\n",
    "                # Extract the energy values for clustering\n",
    "                \n",
    "                # Sort data based on client number\n",
    "                UTILL_usage_list = sorted(UTILL_usage_list, key=lambda x: x[0])\n",
    "                energy_values = np.array([entry[1] for entry in UTILL_usage_list]).reshape(-1, 1)\n",
    "                list_clients_weights_new = sorted(list_clients_weights_new, key=lambda x: x[0])\n",
    "                list_similarity_matrix = sorted(list_similarity_matrix, key=lambda x: x[0])\n",
    "                print(list_similarity_matrix)\n",
    "                # Apply k-means clustering\n",
    "                try:\n",
    "                    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "                    clusters = kmeans.fit_predict(energy_values)\n",
    "                except:\n",
    "                    print(energy_values)\n",
    "                list_average_weights = list()\n",
    "\n",
    "                # Create separate lists for each cluster\n",
    "                cluster_lists = [[] for _ in range(3)]\n",
    "\n",
    "                # Distribute clients into cluster lists\n",
    "                for i, client_entry in enumerate(UTILL_usage_list):\n",
    "                    cluster_lists[clusters[i]].append(client_entry)\n",
    "                # Print the result\n",
    "                cluster_group= [[],[],[]]\n",
    "                print(cluster_lists)\n",
    "                # print(cluster_lists)\n",
    "                for cluster_idx, cluster_list in enumerate(cluster_lists):\n",
    "                    learning_threshold = cluster_idx / 2\n",
    "                    print(learning_threshold)\n",
    "                    clients_batched_cluster = dict()\n",
    "                    scaled_x_hats_list = list()\n",
    "                    for client_entry in cluster_list:\n",
    "                        name =client_name_base+ '_' +str(client_entry[0]) \n",
    "                        clients_batched_cluster[name] =clients_batched[name]\n",
    "                    for client_entry in cluster_list:\n",
    "                        name = client_name_base+ '_' +str(client_entry[0]) \n",
    "                        scaling_factor = weight_scalling_factor(clients_batched_cluster, name)\n",
    "                        scaled_x_hats = scale_model_weights(x_hats[int(client[-1])-1], scaling_factor)\n",
    "                        scaled_x_hats_list.append(scaled_x_hats)\n",
    "                        cluster_group[cluster_idx].append(client_entry[0])\n",
    "                    average_weights = sum_scaled_weights(scaled_x_hats_list)\n",
    "                    list_average_weights.append(average_weights)\n",
    "                average_weights = sum_scaled_weights(scaled_x_hats_list)\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "            for i in range(len(all_results)):\n",
    "                all_R = pd.DataFrame(all_results[i], columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "                flname = f'results/round-100/{cl}-clients/FedAdmm-{dataset[d]}-clisel-results_{i}.csv'\n",
    "                all_R.to_csv(flname, index=None)\n",
    "\n",
    "#             all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "#             all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "# ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "# ALL_AVG.to_csv('FedAvg-results.csv')\n",
    "\n",
    "# ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "# ALL_STD.to_csv('FedAvg-all-std-results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e03d2",
   "metadata": {},
   "source": [
    "## fed avg non iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e5726ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "# from sklearn.utils import shuffle\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# def train_model(model, train_loader, loss_fn, optimizer):\n",
    "#     model.train()\n",
    "#     for inputs, labels in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# all_avg = []\n",
    "# all_std = []\n",
    "\n",
    "# n_clients = [10]\n",
    "# n_round = [100]\n",
    "\n",
    "# dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "# for d in range(2,3):\n",
    "#     if d == 0:\n",
    "#         use_data = Drebin_data\n",
    "#     elif d == 1:\n",
    "#         use_data = Malgenome_data\n",
    "#     elif d == 2:\n",
    "#         use_data = Kronodroid_data\n",
    "#     elif d == 3:\n",
    "#         use_data = Tuandromd_data\n",
    "\n",
    "#     print('===================================================================================================')\n",
    "#     print('Working with:', dataset[d])\n",
    "#     print('===================================================================================================')\n",
    "\n",
    "#     for r in n_round:  # number of rounds loop\n",
    "#         comms_round = r\n",
    "#         for cl in n_clients:  # number of clients loop\n",
    "#             number_of_clients = cl\n",
    "\n",
    "#             print('---------------------------------------------')\n",
    "#             print('No. of Clients:', number_of_clients)\n",
    "#             print('No. of Rounds:', comms_round)\n",
    "#             print('---------------------------------------------')\n",
    "\n",
    "#             features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "#             labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "#             # Do feature scaling\n",
    "#             X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "#             # binarize the labels\n",
    "#             lb = LabelBinarizer()\n",
    "#             y = lb.fit_transform(labels)\n",
    "\n",
    "#             # split data into training and test set\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "#                                                                 y, shuffle=True,\n",
    "#                                                                 test_size=0.2,\n",
    "#                                                                 random_state=100)\n",
    "\n",
    "#             # create clients -- Horizontal FL\n",
    "#             clients = create_clients_non_iid(X_train, [tuple(label) for label in y_train.astype(int).tolist()], num_clients=number_of_clients, initial='client')\n",
    "\n",
    "#             # process and batch the training data for each client\n",
    "#             clients_batched = dict()\n",
    "#             for (client_name, data) in clients.items():\n",
    "#                 clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "#             # process and batch the test set\n",
    "#             test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "#                                                                                      torch.tensor(y_test, dtype=torch.float32)),\n",
    "#                                                        batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "#             # ==============================================\n",
    "#             # Traditional FedAvg 2017\n",
    "#             # ==============================================\n",
    "#             # -----------------------------------------------\n",
    "\n",
    "#             all_results = list()\n",
    "\n",
    "#             # create optimizer\n",
    "#             lr = 0.01\n",
    "#             loss = nn.BCELoss()\n",
    "#             optimizer = optim.SGD\n",
    "\n",
    "#             # initialize global model\n",
    "#             smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "#             global_model = smlp_global\n",
    "#             # -----------------------------------------------\n",
    "\n",
    "#             print('|=======================|')\n",
    "#             print('|Traditional FedAvg non idd 2017|')\n",
    "#             print('|=======================|')\n",
    "\n",
    "#             # commence global training loop\n",
    "#             for comm_round in range(comms_round):\n",
    "#                 # get the global model's weights - will serve as the initial weights for all local models\n",
    "#                 global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "#                 # initial list to collect local model weights after scaling\n",
    "#                 scaled_local_weight_list = list()\n",
    "\n",
    "#                 # randomize client data - using keys\n",
    "#                 client_names = list(clients_batched.keys())\n",
    "#                 random.shuffle(client_names)\n",
    "\n",
    "\n",
    "#                 for client in client_names:\n",
    "#                     smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "#                     local_model = smlp_local\n",
    "#                     # set local model weight to the weight of the global model\n",
    "#                     local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "#                     optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "#                     # fit local model with client's data\n",
    "#                     train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "#                                                             torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "#                                               batch_size=32, shuffle=True)\n",
    "\n",
    "#                     train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "\n",
    "#                     # scale the model weights and add to the list\n",
    "#                     scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "#                     scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "#                     scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "#                     # clear session to free memory after each communication round\n",
    "#                     torch.cuda.empty_cache()\n",
    "                \n",
    "#                 # ...\n",
    "                \n",
    "#                 # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "#                 average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "#                 # update global model\n",
    "#                 for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "#                     param.data.copy_(avg_param)\n",
    "\n",
    "#                 # test global model and print out metrics after each communications round\n",
    "#                 for X_test_batch, Y_test_batch in test_batched:\n",
    "#                     global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "#                     all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "#             all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "#             flname = f'results/round-{r}/{cl}-clients/FedAvg-noniid-{dataset[d]}-results.csv'\n",
    "#             all_R.to_csv(flname, index=None)\n",
    "\n",
    "#             all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "#             all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "# ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "# ALL_AVG.to_csv('FedAvg-noniid-results.csv')\n",
    "\n",
    "# ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "# ALL_STD.to_csv('FedAvg-noniid-all-std-results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b17ec282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 30\n",
      "---------------------------------------------\n",
      "Client client_1: {(0,): 890, (1,): 313}\n",
      "Client client_2: {(0,): 827, (1,): 376}\n",
      "Client client_3: {(0,): 672, (1,): 531}\n",
      "Client client_4: {(0,): 736, (1,): 467}\n",
      "Client client_5: {(0,): 746, (1,): 457}\n",
      "Client client_6: {(0,): 87, (1,): 1116}\n",
      "Client client_7: {(0,): 396, (1,): 807}\n",
      "Client client_8: {(0,): 821, (1,): 382}\n",
      "Client client_9: {(0,): 510, (1,): 693}\n",
      "Client client_10: {(0,): 452, (1,): 751}\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "client_1\n",
      "comm_round: 0 | global_acc: 94.980% | global_loss: 0.15837451815605164 | global_f1: 0.928739971684757 | global_precision: 0.9742574257425742 | global_recall: 0.8872858431018936 | global_auc: 0.9884747845551097| flobal_FPR: 0.1127141568981064 \n",
      "client_2\n",
      "comm_round: 0 | global_acc: 95.545% | global_loss: 0.13719090819358826 | global_f1: 0.9375582479030755 | global_precision: 0.970106075216972 | global_recall: 0.9071235347159603 | global_auc: 0.9908252219501413| flobal_FPR: 0.09287646528403967 \n",
      "client_3\n",
      "comm_round: 0 | global_acc: 94.049% | global_loss: 0.30701524019241333 | global_f1: 0.918227501142074 | global_precision: 0.9305555555555556 | global_recall: 0.9062218214607755 | global_auc: 0.9836001198485653| flobal_FPR: 0.09377817853922453 \n",
      "client_4\n",
      "comm_round: 0 | global_acc: 64.262% | global_loss: 0.5121474862098694 | global_f1: 0.05949256342957131 | global_precision: 1.0 | global_recall: 0.030658250676284943 | global_auc: 0.9442101129586974| flobal_FPR: 0.9693417493237151 \n",
      "client_5\n",
      "comm_round: 0 | global_acc: 95.113% | global_loss: 0.24733637273311615 | global_f1: 0.9314045730284648 | global_precision: 0.965183752417795 | global_recall: 0.8999098286744815 | global_auc: 0.9868271041994006| flobal_FPR: 0.10009017132551848 \n",
      "client_6\n",
      "comm_round: 0 | global_acc: 36.868% | global_loss: 1.0916101932525635 | global_f1: 0.538741802283216 | global_precision: 0.36868351063829785 | global_recall: 1.0 | global_auc: 0.9773802452147232| flobal_FPR: 0.0 \n",
      "client_7\n",
      "comm_round: 0 | global_acc: 36.868% | global_loss: 0.7584614157676697 | global_f1: 0.538741802283216 | global_precision: 0.36868351063829785 | global_recall: 1.0 | global_auc: 0.8860408235362829| flobal_FPR: 0.0 \n",
      "client_8\n",
      "comm_round: 0 | global_acc: 96.110% | global_loss: 0.12457481026649475 | global_f1: 0.9458082445576655 | global_precision: 0.9723809523809523 | global_recall: 0.9206492335437331 | global_auc: 0.9916053772309568| flobal_FPR: 0.0793507664562669 \n",
      "client_9\n",
      "comm_round: 0 | global_acc: 96.410% | global_loss: 0.10461204499006271 | global_f1: 0.9520426287744228 | global_precision: 0.9378827646544182 | global_recall: 0.9666366095581606 | global_auc: 0.9932749000351853| flobal_FPR: 0.033363390441839495 \n",
      "client_10\n",
      "comm_round: 0 | global_acc: 95.578% | global_loss: 0.13296951353549957 | global_f1: 0.941280353200883 | global_precision: 0.9221453287197232 | global_recall: 0.9612263300270514 | global_auc: 0.9920811627400116| flobal_FPR: 0.0387736699729486 \n",
      "[]\n",
      "[[[3, 19.935330332581035], [4, 18.078020606745838], [7, 16.86569242581139]], [[1, 28.228370943397003], [2, 26.393661781076723], [9, 28.430824528574142], [10, 28.17405740018469]], [[5, 23.541016131817216], [6, 24.342408282922147], [8, 22.15984273639217]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 1 | global_acc: 96.709% | global_loss: 0.09403439611196518 | global_f1: 0.954398894518655 | global_precision: 0.975517890772128 | global_recall: 0.9341749323715058 | global_auc: 0.9935564776867517| flobal_FPR: 0.06582506762849413 \n",
      "client_2\n",
      "comm_round: 1 | global_acc: 96.941% | global_loss: 0.0929793193936348 | global_f1: 0.9578754578754578 | global_precision: 0.9730232558139534 | global_recall: 0.9431920649233544 | global_auc: 0.9935683485826863| flobal_FPR: 0.056807935076645624 \n",
      "client_3\n",
      "comm_round: 1 | global_acc: 96.476% | global_loss: 0.09732099622488022 | global_f1: 0.9522091974752029 | global_precision: 0.9522091974752029 | global_recall: 0.9522091974752029 | global_auc: 0.9932867709311199| flobal_FPR: 0.047790802524797116 \n",
      "client_4\n",
      "comm_round: 1 | global_acc: 96.941% | global_loss: 0.08874091506004333 | global_f1: 0.958181818181818 | global_precision: 0.9660861594867094 | global_recall: 0.9504057709648331 | global_auc: 0.9941153594673481| flobal_FPR: 0.04959422903516682 \n",
      "client_5\n",
      "comm_round: 1 | global_acc: 97.008% | global_loss: 0.08962909877300262 | global_f1: 0.9590163934426229 | global_precision: 0.968721251149954 | global_recall: 0.9495040577096483 | global_auc: 0.9941595192002245| flobal_FPR: 0.05049594229035167 \n",
      "client_6\n",
      "comm_round: 1 | global_acc: 91.290% | global_loss: 0.23567596077919006 | global_f1: 0.8935824532900082 | global_precision: 0.8130081300813008 | global_recall: 0.9918845807033363 | global_auc: 0.99307831799851| flobal_FPR: 0.008115419296663661 \n",
      "client_7\n",
      "comm_round: 1 | global_acc: 96.476% | global_loss: 0.10694216936826706 | global_f1: 0.9530141843971631 | global_precision: 0.937227550130776 | global_recall: 0.9693417493237151 | global_auc: 0.993716497363949| flobal_FPR: 0.030658250676284943 \n",
      "client_8\n",
      "comm_round: 1 | global_acc: 96.908% | global_loss: 0.09249283373355865 | global_f1: 0.9574759945130316 | global_precision: 0.9712430426716141 | global_recall: 0.9440937781785392 | global_auc: 0.9936030115988149| flobal_FPR: 0.05590622182146077 \n",
      "client_9\n",
      "comm_round: 1 | global_acc: 96.343% | global_loss: 0.10572575777769089 | global_f1: 0.9506726457399103 | global_precision: 0.9455842997323818 | global_recall: 0.9558160504959423 | global_auc: 0.993014689996301| flobal_FPR: 0.04418394950405771 \n",
      "client_10\n",
      "comm_round: 1 | global_acc: 96.210% | global_loss: 0.11370248347520828 | global_f1: 0.9494232475598934 | global_precision: 0.9344978165938864 | global_recall: 0.9648331830477908 | global_auc: 0.9930550510424784| flobal_FPR: 0.0351668169522092 \n",
      "[]\n",
      "[[[1, 36.031000664764605], [6, 33.4430895495414], [7, 36.36662955292694], [8, 33.64339741419571]], [[2, 26.516588078863347], [3, 24.48506184529942], [9, 21.73342910379771], [10, 18.848776988135103]], [[4, 41.43464380232945], [5, 48.13212244106156]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 2 | global_acc: 97.174% | global_loss: 0.09163679927587509 | global_f1: 0.9610270518110959 | global_precision: 0.9776119402985075 | global_recall: 0.9449954914337241 | global_auc: 0.9942288452324819| flobal_FPR: 0.05500450856627592 \n",
      "client_2\n",
      "comm_round: 2 | global_acc: 97.141% | global_loss: 0.08912462741136551 | global_f1: 0.9608378870673951 | global_precision: 0.9705611775528978 | global_recall: 0.951307484220018 | global_auc: 0.9939586636410127| flobal_FPR: 0.04869251577998197 \n",
      "client_3\n",
      "comm_round: 2 | global_acc: 96.875% | global_loss: 0.09096232801675797 | global_f1: 0.9576957695769578 | global_precision: 0.9559748427672956 | global_recall: 0.9594229035166817 | global_auc: 0.9940892434962922| flobal_FPR: 0.0405770964833183 \n",
      "client_4\n",
      "comm_round: 2 | global_acc: 97.174% | global_loss: 0.08687780052423477 | global_f1: 0.9613811903680145 | global_precision: 0.9688644688644689 | global_recall: 0.9540126239855726 | global_auc: 0.9941585695285498| flobal_FPR: 0.045987376014427414 \n",
      "client_5\n",
      "comm_round: 2 | global_acc: 97.141% | global_loss: 0.0870346948504448 | global_f1: 0.96094459582198 | global_precision: 0.9679780420860018 | global_recall: 0.9540126239855726 | global_auc: 0.9941020640639016| flobal_FPR: 0.045987376014427414 \n",
      "client_6\n",
      "comm_round: 2 | global_acc: 93.052% | global_loss: 0.20342682301998138 | global_f1: 0.9131699210635645 | global_precision: 0.8466872110939908 | global_recall: 0.9909828674481514 | global_auc: 0.9938119393672622| flobal_FPR: 0.009017132551848512 \n",
      "client_7\n",
      "comm_round: 2 | global_acc: 96.543% | global_loss: 0.10921555012464523 | global_f1: 0.9541446208112875 | global_precision: 0.9335634167385677 | global_recall: 0.975653742110009 | global_auc: 0.9937259940806964| flobal_FPR: 0.024346257889990983 \n",
      "client_8\n",
      "comm_round: 2 | global_acc: 97.207% | global_loss: 0.08942177891731262 | global_f1: 0.9616788321167882 | global_precision: 0.9732225300092336 | global_recall: 0.9504057709648331 | global_auc: 0.9940384360616926| flobal_FPR: 0.04959422903516682 \n",
      "client_9\n",
      "comm_round: 2 | global_acc: 96.908% | global_loss: 0.10143669694662094 | global_f1: 0.9586850288760551 | global_precision: 0.9448336252189142 | global_recall: 0.9729486023444545 | global_auc: 0.9938499262342527| flobal_FPR: 0.027051397655545536 \n",
      "client_10\n",
      "comm_round: 2 | global_acc: 96.576% | global_loss: 0.11067629605531693 | global_f1: 0.9546055531070956 | global_precision: 0.9336206896551724 | global_recall: 0.9765554553651938 | global_auc: 0.993786773067881| flobal_FPR: 0.023444544634806132 \n",
      "[]\n",
      "[[[3, 41.64709618580273], [4, 37.58334516791595], [5, 35.79338011694586], [7, 34.022931082556795], [8, 41.59950730026307]], [[2, 21.619898632523665]], [[1, 54.13079273309168], [6, 51.03908901438035], [9, 46.24894503869345], [10, 48.73554156030468]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 3 | global_acc: 97.274% | global_loss: 0.11950705945491791 | global_f1: 0.9626593806921676 | global_precision: 0.9724011039558418 | global_recall: 0.9531109107303878 | global_auc: 0.9937459371858663| flobal_FPR: 0.046889089269612265 \n",
      "client_2\n",
      "comm_round: 3 | global_acc: 97.274% | global_loss: 0.12080816179513931 | global_f1: 0.9627272727272728 | global_precision: 0.9706691109074244 | global_recall: 0.9549143372407575 | global_auc: 0.9936155947485055| flobal_FPR: 0.04508566275924256 \n",
      "client_3\n",
      "comm_round: 3 | global_acc: 97.074% | global_loss: 0.12780456244945526 | global_f1: 0.9605381165919281 | global_precision: 0.9553969669937555 | global_recall: 0.9657348963029756 | global_auc: 0.993806716173051| flobal_FPR: 0.034265103697024346 \n",
      "client_4\n",
      "comm_round: 3 | global_acc: 97.274% | global_loss: 0.11634352803230286 | global_f1: 0.9628286491387126 | global_precision: 0.968094804010939 | global_recall: 0.957619477006312 | global_auc: 0.9938962227283972| flobal_FPR: 0.04238052299368801 \n",
      "client_5\n",
      "comm_round: 3 | global_acc: 97.340% | global_loss: 0.11630085855722427 | global_f1: 0.9637352674524025 | global_precision: 0.9690063810391978 | global_recall: 0.9585211902614968 | global_auc: 0.993832832144107| flobal_FPR: 0.04147880973850315 \n",
      "client_6\n",
      "comm_round: 3 | global_acc: 94.315% | global_loss: 0.22231954336166382 | global_f1: 0.9273885350318473 | global_precision: 0.8764044943820225 | global_recall: 0.9846708746618575 | global_auc: 0.9941561953493627| flobal_FPR: 0.015329125338142471 \n",
      "client_7\n",
      "comm_round: 3 | global_acc: 96.543% | global_loss: 0.1563282161951065 | global_f1: 0.9541850220264317 | global_precision: 0.9328165374677002 | global_recall: 0.9765554553651938 | global_auc: 0.9933575214708894| flobal_FPR: 0.023444544634806132 \n",
      "client_8\n",
      "comm_round: 3 | global_acc: 97.241% | global_loss: 0.11870785057544708 | global_f1: 0.9622898682417084 | global_precision: 0.9697802197802198 | global_recall: 0.9549143372407575 | global_auc: 0.9936220050323101| flobal_FPR: 0.04508566275924256 \n",
      "client_9\n",
      "comm_round: 3 | global_acc: 96.908% | global_loss: 0.13543038070201874 | global_f1: 0.9587948604342047 | global_precision: 0.9425087108013938 | global_recall: 0.975653742110009 | global_auc: 0.9935854426728319| flobal_FPR: 0.024346257889990983 \n",
      "client_10\n",
      "comm_round: 3 | global_acc: 96.576% | global_loss: 0.15237051248550415 | global_f1: 0.9545655050727834 | global_precision: 0.9343696027633851 | global_recall: 0.975653742110009 | global_auc: 0.9934380061453253| flobal_FPR: 0.024346257889990983 \n",
      "[]\n",
      "[[[2, 34.668743333094795], [3, 31.2803728804794], [10, 31.38059852205494]], [[1, 49.85148033716712], [4, 50.00100264896335], [6, 47.48064581709953], [8, 45.510899157182365]], [[5, 27.840183207708662], [7, 26.836354811928526], [9, 29.824920644040798]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 4 | global_acc: 97.307% | global_loss: 0.17628394067287445 | global_f1: 0.9630979498861048 | global_precision: 0.9732965009208103 | global_recall: 0.9531109107303878 | global_auc: 0.9931179667909311| flobal_FPR: 0.046889089269612265 \n",
      "client_2\n",
      "comm_round: 4 | global_acc: 97.307% | global_loss: 0.17825715243816376 | global_f1: 0.9631315430131998 | global_precision: 0.9724264705882353 | global_recall: 0.9540126239855726 | global_auc: 0.9930552884603969| flobal_FPR: 0.045987376014427414 \n",
      "client_3\n",
      "comm_round: 4 | global_acc: 97.041% | global_loss: 0.22175070643424988 | global_f1: 0.9601433049708912 | global_precision: 0.9537366548042705 | global_recall: 0.9666366095581606 | global_auc: 0.992893606857769| flobal_FPR: 0.033363390441839495 \n",
      "client_4\n",
      "comm_round: 4 | global_acc: 97.274% | global_loss: 0.17314906418323517 | global_f1: 0.9627949183303085 | global_precision: 0.9689497716894977 | global_recall: 0.9567177637511272 | global_auc: 0.9932117468688137| flobal_FPR: 0.04328223624887286 \n",
      "client_5\n",
      "comm_round: 4 | global_acc: 97.307% | global_loss: 0.17616575956344604 | global_f1: 0.9632319564230595 | global_precision: 0.9698354661791591 | global_recall: 0.9567177637511272 | global_auc: 0.9932267041976912| flobal_FPR: 0.04328223624887286 \n",
      "client_6\n",
      "comm_round: 4 | global_acc: 94.149% | global_loss: 0.429591566324234 | global_f1: 0.9253604749787956 | global_precision: 0.8734987990392313 | global_recall: 0.9837691614066727 | global_auc: 0.9931032468799724| flobal_FPR: 0.016230838593327322 \n",
      "client_7\n",
      "comm_round: 4 | global_acc: 96.177% | global_loss: 0.3186275362968445 | global_f1: 0.9496276828734121 | global_precision: 0.9233390119250426 | global_recall: 0.9774571686203787 | global_auc: 0.992254952656493| flobal_FPR: 0.02254283137962128 \n",
      "client_8\n",
      "comm_round: 4 | global_acc: 97.340% | global_loss: 0.17697177827358246 | global_f1: 0.9636032757051866 | global_precision: 0.9724517906336089 | global_recall: 0.9549143372407575 | global_auc: 0.9930595619829334| flobal_FPR: 0.04508566275924256 \n",
      "client_9\n",
      "comm_round: 4 | global_acc: 96.509% | global_loss: 0.29802101850509644 | global_f1: 0.9537241075363596 | global_precision: 0.9327586206896552 | global_recall: 0.975653742110009 | global_auc: 0.9924026266019181| flobal_FPR: 0.024346257889990983 \n",
      "client_10\n",
      "comm_round: 4 | global_acc: 96.144% | global_loss: 0.32036447525024414 | global_f1: 0.9491228070175438 | global_precision: 0.92399658411614 | global_recall: 0.975653742110009 | global_auc: 0.9922214767299576| flobal_FPR: 0.024346257889990983 \n",
      "[]\n",
      "[[[2, 49.59981060751997], [3, 56.67773979685805], [4, 57.297965442600756], [6, 47.23005009768519], [7, 55.05313829811105], [9, 49.685478436078526]], [[5, 23.93777754563124], [10, 29.165920699607987]], [[1, 68.83543585282725], [8, 78.36238804956112]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 5 | global_acc: 97.174% | global_loss: 0.40544775128364563 | global_f1: 0.9613811903680145 | global_precision: 0.9688644688644689 | global_recall: 0.9540126239855726 | global_auc: 0.9914019100746395| flobal_FPR: 0.045987376014427414 \n",
      "client_2\n",
      "comm_round: 5 | global_acc: 97.108% | global_loss: 0.4084102511405945 | global_f1: 0.9605083976395824 | global_precision: 0.9670932358318098 | global_recall: 0.9540126239855726 | global_auc: 0.9912145873367929| flobal_FPR: 0.045987376014427414 \n",
      "client_3\n",
      "comm_round: 5 | global_acc: 96.775% | global_loss: 0.5769422054290771 | global_f1: 0.9567543468568882 | global_precision: 0.9462081128747796 | global_recall: 0.9675383228133454 | global_auc: 0.9896851411045916| flobal_FPR: 0.032461677186654644 \n",
      "client_4\n",
      "comm_round: 5 | global_acc: 97.074% | global_loss: 0.4401397407054901 | global_f1: 0.9604672057502246 | global_precision: 0.9570277529095792 | global_recall: 0.9639314697926059 | global_auc: 0.9908354309206449| flobal_FPR: 0.03606853020739405 \n",
      "client_5\n",
      "comm_round: 5 | global_acc: 97.074% | global_loss: 0.4377692639827728 | global_f1: 0.9604672057502246 | global_precision: 0.9570277529095792 | global_recall: 0.9639314697926059 | global_auc: 0.9908197613380115| flobal_FPR: 0.03606853020739405 \n",
      "client_6\n",
      "comm_round: 5 | global_acc: 93.517% | global_loss: 0.9478460550308228 | global_f1: 0.9179638199411022 | global_precision: 0.860410094637224 | global_recall: 0.9837691614066727 | global_auc: 0.9896834791791608| flobal_FPR: 0.016230838593327322 \n",
      "client_7\n",
      "comm_round: 5 | global_acc: 96.011% | global_loss: 0.7070480585098267 | global_f1: 0.94750656167979 | global_precision: 0.9201359388275276 | global_recall: 0.9765554553651938 | global_auc: 0.9894762133361443| flobal_FPR: 0.023444544634806132 \n",
      "client_8\n",
      "comm_round: 5 | global_acc: 97.141% | global_loss: 0.4572187662124634 | global_f1: 0.9610154125113328 | global_precision: 0.9662716499544212 | global_recall: 0.9558160504959423 | global_auc: 0.9911851475148754| flobal_FPR: 0.04418394950405771 \n",
      "client_9\n",
      "comm_round: 5 | global_acc: 96.376% | global_loss: 0.6257025599479675 | global_f1: 0.9519612163948876 | global_precision: 0.9310344827586207 | global_recall: 0.9738503155996393 | global_auc: 0.9900301093404484| flobal_FPR: 0.026149684400360685 \n",
      "client_10\n",
      "comm_round: 5 | global_acc: 96.044% | global_loss: 0.7393803000450134 | global_f1: 0.9478756022777047 | global_precision: 0.9216354344122658 | global_recall: 0.975653742110009 | global_auc: 0.9892582636867868| flobal_FPR: 0.024346257889990983 \n",
      "[]\n",
      "[[[1, 92.31575154170899], [2, 90.31726371112057], [6, 107.48319058775306], [7, 95.34680676343407]], [[3, 57.89498524698115], [4, 43.344244374594076], [5, 47.779142580049765], [9, 50.39365045679143]], [[8, 80.94420857229235], [10, 66.28932863158066]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 6 | global_acc: 97.141% | global_loss: 0.9124304056167603 | global_f1: 0.9614003590664273 | global_precision: 0.9571045576407506 | global_recall: 0.9657348963029756 | global_auc: 0.9876426347501011| flobal_FPR: 0.034265103697024346 \n",
      "client_2\n",
      "comm_round: 6 | global_acc: 96.775% | global_loss: 0.8634840846061707 | global_f1: 0.9565995525727069 | global_precision: 0.9493783303730018 | global_recall: 0.9639314697926059 | global_auc: 0.9877803371429412| flobal_FPR: 0.03606853020739405 \n",
      "client_3\n",
      "comm_round: 6 | global_acc: 96.543% | global_loss: 0.9424974322319031 | global_f1: 0.9536541889483066 | global_precision: 0.9427312775330396 | global_recall: 0.9648331830477908 | global_auc: 0.9867587278388179| flobal_FPR: 0.0351668169522092 \n",
      "client_4\n",
      "comm_round: 6 | global_acc: 96.609% | global_loss: 0.9005069136619568 | global_f1: 0.9545049063336306 | global_precision: 0.9443954104148279 | global_recall: 0.9648331830477908 | global_auc: 0.9871701730919078| flobal_FPR: 0.0351668169522092 \n",
      "client_5\n",
      "comm_round: 6 | global_acc: 96.609% | global_loss: 0.9267411828041077 | global_f1: 0.9544642857142858 | global_precision: 0.9451812555260831 | global_recall: 0.9639314697926059 | global_auc: 0.9870414925799778| flobal_FPR: 0.03606853020739405 \n",
      "client_6\n",
      "comm_round: 6 | global_acc: 93.418% | global_loss: 1.7712249755859375 | global_f1: 0.9167367535744323 | global_precision: 0.8589440504334122 | global_recall: 0.9828674481514879 | global_auc: 0.9836863025530499| flobal_FPR: 0.017132551848512173 \n",
      "client_7\n",
      "comm_round: 6 | global_acc: 95.711% | global_loss: 1.27742600440979 | global_f1: 0.9437908496732026 | global_precision: 0.9131534569983136 | global_recall: 0.9765554553651938 | global_auc: 0.9850182170769011| flobal_FPR: 0.023444544634806132 \n",
      "client_8\n",
      "comm_round: 6 | global_acc: 96.809% | global_loss: 0.8623067140579224 | global_f1: 0.956989247311828 | global_precision: 0.9510240427426536 | global_recall: 0.9630297565374211 | global_auc: 0.9877105362748464| flobal_FPR: 0.0369702434625789 \n",
      "client_9\n",
      "comm_round: 6 | global_acc: 96.410% | global_loss: 1.0962494611740112 | global_f1: 0.9523389232127096 | global_precision: 0.9325842696629213 | global_recall: 0.9729486023444545 | global_auc: 0.9860894467260307| flobal_FPR: 0.027051397655545536 \n",
      "client_10\n",
      "comm_round: 6 | global_acc: 95.578% | global_loss: 1.3426191806793213 | global_f1: 0.9420983892033087 | global_precision: 0.9107744107744108 | global_recall: 0.975653742110009 | global_auc: 0.9846172182122336| flobal_FPR: 0.024346257889990983 \n",
      "[]\n",
      "[[[2, 55.12574133819635], [3, 54.753950768701515], [4, 42.4428627077414], [5, 40.18638666317251], [6, 49.81728471125], [8, 43.081911985203654], [10, 53.40203236438695]], [[1, 110.46978914370752], [7, 100.21148989067117]], [[9, 85.40164786210369]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 7 | global_acc: 96.277% | global_loss: 1.4296860694885254 | global_f1: 0.9501335707925201 | global_precision: 0.9384344766930519 | global_recall: 0.9621280432822362 | global_auc: 0.9830146472610758| flobal_FPR: 0.03787195671776375 \n",
      "client_2\n",
      "comm_round: 7 | global_acc: 96.243% | global_loss: 1.4919694662094116 | global_f1: 0.9498000888494003 | global_precision: 0.936077057793345 | global_recall: 0.9639314697926059 | global_auc: 0.9828543901659599| flobal_FPR: 0.03606853020739405 \n",
      "client_3\n",
      "comm_round: 7 | global_acc: 95.878% | global_loss: 1.7076056003570557 | global_f1: 0.9452780229479258 | global_precision: 0.9256698357821953 | global_recall: 0.9657348963029756 | global_auc: 0.980241368552857| flobal_FPR: 0.034265103697024346 \n",
      "client_4\n",
      "comm_round: 7 | global_acc: 95.711% | global_loss: 1.6416643857955933 | global_f1: 0.9432468103827542 | global_precision: 0.9209621993127147 | global_recall: 0.9666366095581606 | global_auc: 0.9808171070056806| flobal_FPR: 0.033363390441839495 \n",
      "client_5\n",
      "comm_round: 7 | global_acc: 96.210% | global_loss: 1.5105587244033813 | global_f1: 0.9493783303730017 | global_precision: 0.9352580927384077 | global_recall: 0.9639314697926059 | global_auc: 0.9822432764432516| flobal_FPR: 0.03606853020739405 \n",
      "client_6\n",
      "comm_round: 7 | global_acc: 92.420% | global_loss: 3.0027923583984375 | global_f1: 0.9053156146179402 | global_precision: 0.8391070053887606 | global_recall: 0.9828674481514879 | global_auc: 0.9743275256162065| flobal_FPR: 0.017132551848512173 \n",
      "client_7\n",
      "comm_round: 7 | global_acc: 95.146% | global_loss: 1.981608271598816 | global_f1: 0.9367965367965367 | global_precision: 0.9009159034138218 | global_recall: 0.975653742110009 | global_auc: 0.9793121148191042| flobal_FPR: 0.024346257889990983 \n",
      "client_8\n",
      "comm_round: 7 | global_acc: 96.476% | global_loss: 1.2985336780548096 | global_f1: 0.9527207850133809 | global_precision: 0.942630185348632 | global_recall: 0.9630297565374211 | global_auc: 0.9844548243558495| flobal_FPR: 0.0369702434625789 \n",
      "client_9\n",
      "comm_round: 7 | global_acc: 95.745% | global_loss: 1.8158985376358032 | global_f1: 0.9440559440559441 | global_precision: 0.916030534351145 | global_recall: 0.9738503155996393 | global_auc: 0.980194597222875| flobal_FPR: 0.026149684400360685 \n",
      "client_10\n",
      "comm_round: 7 | global_acc: 95.113% | global_loss: 1.883529782295227 | global_f1: 0.9363360762234735 | global_precision: 0.9008333333333334 | global_recall: 0.9747520288548241 | global_auc: 0.980514161741432| flobal_FPR: 0.025247971145175834 \n",
      "[]\n",
      "[[[2, 20.38302039488103], [5, 28.233040457596346], [8, 27.309683588866818]], [[3, 114.75692700408199], [4, 119.93594007330202], [6, 115.303301955888]], [[1, 71.59768540820608], [7, 77.86159790852159], [9, 74.37440828156714], [10, 50.90607949286746]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 8 | global_acc: 96.676% | global_loss: 1.751482367515564 | global_f1: 0.9555160142348754 | global_precision: 0.9429323968393327 | global_recall: 0.9684400360685302 | global_auc: 0.9816618399603796| flobal_FPR: 0.031559963931469794 \n",
      "client_2\n",
      "comm_round: 8 | global_acc: 96.543% | global_loss: 1.7480477094650269 | global_f1: 0.9537366548042705 | global_precision: 0.9411764705882353 | global_recall: 0.9666366095581606 | global_auc: 0.9812021988697958| flobal_FPR: 0.033363390441839495 \n",
      "client_3\n",
      "comm_round: 8 | global_acc: 96.177% | global_loss: 1.8571780920028687 | global_f1: 0.9494060712714474 | global_precision: 0.9269759450171822 | global_recall: 0.9729486023444545 | global_auc: 0.9810400424313304| flobal_FPR: 0.027051397655545536 \n",
      "client_4\n",
      "comm_round: 8 | global_acc: 96.144% | global_loss: 2.1120669841766357 | global_f1: 0.948943661971831 | global_precision: 0.9269131556319863 | global_recall: 0.9720468890892696 | global_auc: 0.9780649584922254| flobal_FPR: 0.027953110910730387 \n",
      "client_5\n",
      "comm_round: 8 | global_acc: 96.210% | global_loss: 1.878134846687317 | global_f1: 0.9497354497354497 | global_precision: 0.9292493528904228 | global_recall: 0.9711451758340848 | global_auc: 0.9802019571783545| flobal_FPR: 0.028854824165915238 \n",
      "client_6\n",
      "comm_round: 8 | global_acc: 93.816% | global_loss: 3.1687171459198 | global_f1: 0.9212531752751906 | global_precision: 0.8683160415003991 | global_recall: 0.9810640216411182 | global_auc: 0.9724687807307819| flobal_FPR: 0.018935978358881875 \n",
      "client_7\n",
      "comm_round: 8 | global_acc: 94.747% | global_loss: 2.6802306175231934 | global_f1: 0.9318965517241378 | global_precision: 0.8926507018992568 | global_recall: 0.9747520288548241 | global_auc: 0.9743059205856055| flobal_FPR: 0.025247971145175834 \n",
      "client_8\n",
      "comm_round: 8 | global_acc: 96.609% | global_loss: 1.5456109046936035 | global_f1: 0.9545859305431877 | global_precision: 0.9428320140721196 | global_recall: 0.9666366095581606 | global_auc: 0.9835861121913626| flobal_FPR: 0.033363390441839495 \n",
      "client_9\n",
      "comm_round: 8 | global_acc: 95.047% | global_loss: 2.582249641418457 | global_f1: 0.9354699003897792 | global_precision: 0.9 | global_recall: 0.9738503155996393 | global_auc: 0.9748795222771607| flobal_FPR: 0.026149684400360685 \n",
      "client_10\n",
      "comm_round: 8 | global_acc: 94.681% | global_loss: 2.7634787559509277 | global_f1: 0.9311531841652324 | global_precision: 0.8905349794238683 | global_recall: 0.975653742110009 | global_auc: 0.973780039895707| flobal_FPR: 0.024346257889990983 \n",
      "[]\n",
      "[[[4, 106.16948666976644], [5, 126.13794664976231], [7, 131.79986609693088], [9, 132.2761265854637]], [[1, 63.092090291685274], [2, 60.161437960957095], [3, 71.348387413024], [6, 70.66647175745575]], [[8, 33.592598983533854], [10, 28.61641561507347]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 9 | global_acc: 96.476% | global_loss: 1.9432872533798218 | global_f1: 0.9529724933451642 | global_precision: 0.9379912663755459 | global_recall: 0.9684400360685302 | global_auc: 0.9811418947184485| flobal_FPR: 0.031559963931469794 \n",
      "client_2\n",
      "comm_round: 9 | global_acc: 96.177% | global_loss: 2.1653523445129395 | global_f1: 0.949272165857962 | global_precision: 0.9291882556131261 | global_recall: 0.9702434625788999 | global_auc: 0.978799766950571| flobal_FPR: 0.029756537421100092 \n",
      "client_3\n",
      "comm_round: 9 | global_acc: 95.578% | global_loss: 2.724764347076416 | global_f1: 0.941946748144915 | global_precision: 0.9128595600676819 | global_recall: 0.9729486023444545 | global_auc: 0.973921066139409| flobal_FPR: 0.027051397655545536 \n",
      "client_4\n",
      "comm_round: 9 | global_acc: 95.844% | global_loss: 2.1548702716827393 | global_f1: 0.9449581682078381 | global_precision: 0.923407917383821 | global_recall: 0.9675383228133454 | global_auc: 0.9789737942849708| flobal_FPR: 0.032461677186654644 \n",
      "client_5\n",
      "comm_round: 9 | global_acc: 96.310% | global_loss: 2.0335874557495117 | global_f1: 0.9508196721311475 | global_precision: 0.9346689895470384 | global_recall: 0.9675383228133454 | global_auc: 0.9799756979018428| flobal_FPR: 0.032461677186654644 \n",
      "client_6\n",
      "comm_round: 9 | global_acc: 92.753% | global_loss: 4.927670955657959 | global_f1: 0.9091666666666667 | global_precision: 0.8450813323005422 | global_recall: 0.9837691614066727 | global_auc: 0.9594528181744366| flobal_FPR: 0.016230838593327322 \n",
      "client_7\n",
      "comm_round: 9 | global_acc: 94.415% | global_loss: 3.3045690059661865 | global_f1: 0.927958833619211 | global_precision: 0.884709730171709 | global_recall: 0.975653742110009 | global_auc: 0.9700039078989416| flobal_FPR: 0.024346257889990983 \n",
      "client_8\n",
      "comm_round: 9 | global_acc: 96.443% | global_loss: 1.9240210056304932 | global_f1: 0.9525498891352551 | global_precision: 0.93717277486911 | global_recall: 0.9684400360685302 | global_auc: 0.9815597502553428| flobal_FPR: 0.031559963931469794 \n",
      "client_9\n",
      "comm_round: 9 | global_acc: 95.379% | global_loss: 2.80207896232605 | global_f1: 0.9394862864606006 | global_precision: 0.9082491582491582 | global_recall: 0.9729486023444545 | global_auc: 0.9736178834572418| flobal_FPR: 0.027051397655545536 \n",
      "client_10\n",
      "comm_round: 9 | global_acc: 94.315% | global_loss: 3.3627800941467285 | global_f1: 0.9267665952890792 | global_precision: 0.8825448613376835 | global_recall: 0.975653742110009 | global_auc: 0.9695345326736913| flobal_FPR: 0.024346257889990983 \n",
      "[]\n",
      "[[[5, 77.29016815198163], [7, 91.49380953578765], [8, 65.39272970885206], [10, 89.30788999496785]], [[1, 29.708767962856832], [6, 25.704994766359786], [9, 52.49719100626506]], [[2, 123.86804116875732], [3, 109.97921426290326], [4, 123.39205478877483]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 10 | global_acc: 96.775% | global_loss: 2.1390674114227295 | global_f1: 0.9566383549396513 | global_precision: 0.9485815602836879 | global_recall: 0.9648331830477908 | global_auc: 0.979660644323741| flobal_FPR: 0.0351668169522092 \n",
      "client_2\n",
      "comm_round: 10 | global_acc: 95.911% | global_loss: 2.542958974838257 | global_f1: 0.9456473707467963 | global_precision: 0.9272097053726169 | global_recall: 0.9648331830477908 | global_auc: 0.9766513722043447| flobal_FPR: 0.0351668169522092 \n",
      "client_3\n",
      "comm_round: 10 | global_acc: 95.379% | global_loss: 3.162400007247925 | global_f1: 0.9395914819643634 | global_precision: 0.9068791946308725 | global_recall: 0.9747520288548241 | global_auc: 0.9715630313709793| flobal_FPR: 0.025247971145175834 \n",
      "client_4\n",
      "comm_round: 10 | global_acc: 95.279% | global_loss: 3.087461471557617 | global_f1: 0.9382608695652174 | global_precision: 0.9059613769941226 | global_recall: 0.9729486023444545 | global_auc: 0.9723586188165096| flobal_FPR: 0.027051397655545536 \n",
      "client_5\n",
      "comm_round: 10 | global_acc: 95.811% | global_loss: 2.796109437942505 | global_f1: 0.9441489361702127 | global_precision: 0.9285091543156059 | global_recall: 0.9603246167718665 | global_auc: 0.9739640387826918| flobal_FPR: 0.03967538322813345 \n",
      "client_6\n",
      "comm_round: 10 | global_acc: 92.188% | global_loss: 5.703247547149658 | global_f1: 0.9026108578532946 | global_precision: 0.8351226993865031 | global_recall: 0.981965734896303 | global_auc: 0.9533058308416322| flobal_FPR: 0.018034265103697024 \n",
      "client_7\n",
      "comm_round: 10 | global_acc: 94.049% | global_loss: 3.881929874420166 | global_f1: 0.9234715690466012 | global_precision: 0.8780487804878049 | global_recall: 0.9738503155996393 | global_auc: 0.9660105385065749| flobal_FPR: 0.026149684400360685 \n",
      "client_8\n",
      "comm_round: 10 | global_acc: 95.844% | global_loss: 2.783195734024048 | global_f1: 0.9448610498456109 | global_precision: 0.9248704663212435 | global_recall: 0.9657348963029756 | global_auc: 0.9748094839911472| flobal_FPR: 0.034265103697024346 \n",
      "client_9\n",
      "comm_round: 10 | global_acc: 95.180% | global_loss: 3.1950573921203613 | global_f1: 0.9372022520571676 | global_precision: 0.9016666666666666 | global_recall: 0.975653742110009 | global_auc: 0.9716368683436918| flobal_FPR: 0.024346257889990983 \n",
      "client_10\n",
      "comm_round: 10 | global_acc: 94.116% | global_loss: 3.8131964206695557 | global_f1: 0.9242618741976892 | global_precision: 0.8794788273615635 | global_recall: 0.9738503155996393 | global_auc: 0.9665359443606358| flobal_FPR: 0.026149684400360685 \n",
      "[]\n",
      "[[[1, 60.89883862913912], [8, 63.484071981193374], [9, 95.45911075508266], [10, 78.13421032805341]], [[2, 117.78576732434536], [3, 128.6354140975274], [4, 136.00956389602925]], [[5, 45.87950467948588], [6, 28.58088081888902], [7, 31.45805788963254]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 11 | global_acc: 96.476% | global_loss: 2.4480364322662354 | global_f1: 0.9526785714285714 | global_precision: 0.9434129089301503 | global_recall: 0.9621280432822362 | global_auc: 0.9770642419649468| flobal_FPR: 0.03787195671776375 \n",
      "client_2\n",
      "comm_round: 11 | global_acc: 96.210% | global_loss: 2.824723482131958 | global_f1: 0.9497354497354497 | global_precision: 0.9292493528904228 | global_recall: 0.9711451758340848 | global_auc: 0.9757354138740384| flobal_FPR: 0.028854824165915238 \n",
      "client_3\n",
      "comm_round: 11 | global_acc: 95.678% | global_loss: 3.2138142585754395 | global_f1: 0.9431321084864392 | global_precision: 0.9158878504672897 | global_recall: 0.9720468890892696 | global_auc: 0.9713023464962576| flobal_FPR: 0.027953110910730387 \n",
      "client_4\n",
      "comm_round: 11 | global_acc: 95.578% | global_loss: 3.07891845703125 | global_f1: 0.9413838695460555 | global_precision: 0.9206896551724137 | global_recall: 0.9630297565374211 | global_auc: 0.9725392938526327| flobal_FPR: 0.0369702434625789 \n",
      "client_5\n",
      "comm_round: 11 | global_acc: 95.479% | global_loss: 3.364421844482422 | global_f1: 0.9404553415061296 | global_precision: 0.9140425531914894 | global_recall: 0.9684400360685302 | global_auc: 0.9698878105367021| flobal_FPR: 0.031559963931469794 \n",
      "client_6\n",
      "comm_round: 11 | global_acc: 92.886% | global_loss: 5.5057692527771 | global_f1: 0.9106844741235393 | global_precision: 0.8477078477078477 | global_recall: 0.9837691614066727 | global_auc: 0.954950662182317| flobal_FPR: 0.016230838593327322 \n",
      "client_7\n",
      "comm_round: 11 | global_acc: 93.916% | global_loss: 4.3454694747924805 | global_f1: 0.9219616204690831 | global_precision: 0.8745954692556634 | global_recall: 0.9747520288548241 | global_auc: 0.9623248627368304| flobal_FPR: 0.025247971145175834 \n",
      "client_8\n",
      "comm_round: 11 | global_acc: 95.911% | global_loss: 2.9077048301696777 | global_f1: 0.9456473707467963 | global_precision: 0.9272097053726169 | global_recall: 0.9648331830477908 | global_auc: 0.9733458025224229| flobal_FPR: 0.0351668169522092 \n",
      "client_9\n",
      "comm_round: 11 | global_acc: 95.246% | global_loss: 3.5741188526153564 | global_f1: 0.937799043062201 | global_precision: 0.9058823529411765 | global_recall: 0.9720468890892696 | global_auc: 0.9689587942208681| flobal_FPR: 0.027953110910730387 \n",
      "client_10\n",
      "comm_round: 11 | global_acc: 93.916% | global_loss: 4.32110071182251 | global_f1: 0.9219616204690831 | global_precision: 0.8745954692556634 | global_recall: 0.9747520288548241 | global_auc: 0.9625568200433905| flobal_FPR: 0.025247971145175834 \n",
      "[]\n",
      "[[[1, 125.16771491874471], [3, 135.0148257700186], [5, 121.38240817673585], [7, 135.45316343698937]], [[2, 95.27436221345555], [6, 69.77585025814778], [8, 88.6056171744252], [9, 91.88835714598916], [10, 82.7102235200507]], [[4, 33.354332381584854]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 12 | global_acc: 96.443% | global_loss: 2.5685365200042725 | global_f1: 0.9524655708573966 | global_precision: 0.9387040280210157 | global_recall: 0.9666366095581606 | global_auc: 0.9769645264390968| flobal_FPR: 0.033363390441839495 \n",
      "client_2\n",
      "comm_round: 12 | global_acc: 96.077% | global_loss: 2.9201245307922363 | global_f1: 0.947787610619469 | global_precision: 0.9304952215464813 | global_recall: 0.9657348963029756 | global_auc: 0.9737050158334011| flobal_FPR: 0.034265103697024346 \n",
      "client_3\n",
      "comm_round: 12 | global_acc: 95.312% | global_loss: 3.406494140625 | global_f1: 0.938239159001314 | global_precision: 0.9122657580919932 | global_recall: 0.9657348963029756 | global_auc: 0.9694986825679692| flobal_FPR: 0.034265103697024346 \n",
      "client_4\n",
      "comm_round: 12 | global_acc: 95.612% | global_loss: 3.544386863708496 | global_f1: 0.9418502202643172 | global_precision: 0.9207579672695951 | global_recall: 0.9639314697926059 | global_auc: 0.9678298720174968| flobal_FPR: 0.03606853020739405 \n",
      "client_5\n",
      "comm_round: 12 | global_acc: 95.811% | global_loss: 3.021298885345459 | global_f1: 0.9440993788819876 | global_precision: 0.9292576419213974 | global_recall: 0.9594229035166817 | global_auc: 0.973021727063411| flobal_FPR: 0.0405770964833183 \n",
      "client_6\n",
      "comm_round: 12 | global_acc: 92.453% | global_loss: 5.950810432434082 | global_f1: 0.9056917324470295 | global_precision: 0.8397534668721109 | global_recall: 0.9828674481514879 | global_auc: 0.9518428616266642| flobal_FPR: 0.017132551848512173 \n",
      "client_7\n",
      "comm_round: 12 | global_acc: 93.850% | global_loss: 4.849648475646973 | global_f1: 0.9211759693225394 | global_precision: 0.8731825525040388 | global_recall: 0.9747520288548241 | global_auc: 0.9580582253200511| flobal_FPR: 0.025247971145175834 \n",
      "client_8\n",
      "comm_round: 12 | global_acc: 95.778% | global_loss: 3.2323310375213623 | global_f1: 0.9442248572683356 | global_precision: 0.9203767123287672 | global_recall: 0.9693417493237151 | global_auc: 0.9713453191395405| flobal_FPR: 0.030658250676284943 \n",
      "client_9\n",
      "comm_round: 12 | global_acc: 95.512% | global_loss: 3.467339038848877 | global_f1: 0.9407114624505929 | global_precision: 0.9169520547945206 | global_recall: 0.9657348963029756 | global_auc: 0.9693426989953897| flobal_FPR: 0.034265103697024346 \n",
      "client_10\n",
      "comm_round: 12 | global_acc: 93.783% | global_loss: 4.847720623016357 | global_f1: 0.9203916560238399 | global_precision: 0.8717741935483871 | global_recall: 0.9747520288548241 | global_auc: 0.9580539517975146| flobal_FPR: 0.025247971145175834 \n",
      "[]\n",
      "[[[7, 60.23413788257927], [9, 76.10104315055786], [10, 67.50182901092806]], [[3, 145.73002344605695], [4, 105.80918672174215], [6, 109.94925329062768], [8, 119.79277903067462]], [[1, 30.080370200923525], [2, 21.52363086773311], [5, 21.177396786141344]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 13 | global_acc: 96.676% | global_loss: 2.5171072483062744 | global_f1: 0.9551971326164875 | global_precision: 0.9492430988423864 | global_recall: 0.9612263300270514 | global_auc: 0.977527444324311| flobal_FPR: 0.0387736699729486 \n",
      "client_2\n",
      "comm_round: 13 | global_acc: 95.645% | global_loss: 3.4879403114318848 | global_f1: 0.9423669159700835 | global_precision: 0.9201030927835051 | global_recall: 0.9657348963029756 | global_auc: 0.9687693347217532| flobal_FPR: 0.034265103697024346 \n",
      "client_3\n",
      "comm_round: 13 | global_acc: 95.479% | global_loss: 3.509507179260254 | global_f1: 0.9405074365704287 | global_precision: 0.913338997451147 | global_recall: 0.9693417493237151 | global_auc: 0.9695221869419195| flobal_FPR: 0.030658250676284943 \n",
      "client_4\n",
      "comm_round: 13 | global_acc: 95.512% | global_loss: 3.5915238857269287 | global_f1: 0.9407634927599825 | global_precision: 0.9162393162393162 | global_recall: 0.9666366095581606 | global_auc: 0.9685100743545438| flobal_FPR: 0.033363390441839495 \n",
      "client_5\n",
      "comm_round: 13 | global_acc: 96.343% | global_loss: 2.9636714458465576 | global_f1: 0.9512411347517731 | global_precision: 0.9354838709677419 | global_recall: 0.9675383228133454 | global_auc: 0.9729977478536234| flobal_FPR: 0.032461677186654644 \n",
      "client_6\n",
      "comm_round: 13 | global_acc: 92.453% | global_loss: 6.413839340209961 | global_f1: 0.9056917324470295 | global_precision: 0.8397534668721109 | global_recall: 0.9828674481514879 | global_auc: 0.9477827777991454| flobal_FPR: 0.017132551848512173 \n",
      "client_7\n",
      "comm_round: 13 | global_acc: 93.617% | global_loss: 5.102283954620361 | global_f1: 0.9183673469387754 | global_precision: 0.8688656476267096 | global_recall: 0.9738503155996393 | global_auc: 0.9556949673574104| flobal_FPR: 0.026149684400360685 \n",
      "client_8\n",
      "comm_round: 13 | global_acc: 96.609% | global_loss: 2.580695629119873 | global_f1: 0.9544642857142858 | global_precision: 0.9451812555260831 | global_recall: 0.9639314697926059 | global_auc: 0.9779028020537599| flobal_FPR: 0.03606853020739405 \n",
      "client_9\n",
      "comm_round: 13 | global_acc: 95.146% | global_loss: 4.0719895362854 | global_f1: 0.9366319444444445 | global_precision: 0.902928870292887 | global_recall: 0.9729486023444545 | global_auc: 0.9652042672547034| flobal_FPR: 0.027051397655545536 \n",
      "client_10\n",
      "comm_round: 13 | global_acc: 93.617% | global_loss: 5.100180149078369 | global_f1: 0.9183673469387754 | global_precision: 0.8688656476267096 | global_recall: 0.9738503155996393 | global_auc: 0.9556968667007598| flobal_FPR: 0.026149684400360685 \n",
      "[]\n",
      "[[[3, 82.40998950661202], [6, 65.69689362832173], [7, 79.31342100815084], [9, 102.21622364276259], [10, 89.75003871890769]], [[1, 153.04387357762144], [2, 113.15084248269844], [4, 117.86984872235418], [5, 125.10945140300873]], [[8, 31.907921609497965]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 14 | global_acc: 96.410% | global_loss: 3.0200705528259277 | global_f1: 0.9520426287744228 | global_precision: 0.9378827646544182 | global_recall: 0.9666366095581606 | global_auc: 0.9732657926838244| flobal_FPR: 0.033363390441839495 \n",
      "client_2\n",
      "comm_round: 14 | global_acc: 95.977% | global_loss: 3.161856174468994 | global_f1: 0.9462461128387383 | global_precision: 0.9325744308231173 | global_recall: 0.9603246167718665 | global_auc: 0.9707069023561828| flobal_FPR: 0.03967538322813345 \n",
      "client_3\n",
      "comm_round: 14 | global_acc: 95.379% | global_loss: 3.822014093399048 | global_f1: 0.9389547650417215 | global_precision: 0.9152397260273972 | global_recall: 0.9639314697926059 | global_auc: 0.9668500482670629| flobal_FPR: 0.03606853020739405 \n",
      "client_4\n",
      "comm_round: 14 | global_acc: 95.412% | global_loss: 3.791593551635742 | global_f1: 0.9395267309377738 | global_precision: 0.9138959931798807 | global_recall: 0.9666366095581606 | global_auc: 0.9678229868978547| flobal_FPR: 0.033363390441839495 \n",
      "client_5\n",
      "comm_round: 14 | global_acc: 94.914% | global_loss: 4.307909965515137 | global_f1: 0.9336801040312094 | global_precision: 0.8989983305509182 | global_recall: 0.9711451758340848 | global_auc: 0.9617593332545107| flobal_FPR: 0.028854824165915238 \n",
      "client_6\n",
      "comm_round: 14 | global_acc: 94.049% | global_loss: 5.027657508850098 | global_f1: 0.922611327280588 | global_precision: 0.8862126245847176 | global_recall: 0.9621280432822362 | global_auc: 0.9566572221818611| flobal_FPR: 0.03787195671776375 \n",
      "client_7\n",
      "comm_round: 14 | global_acc: 93.484% | global_loss: 5.226262092590332 | global_f1: 0.916595744680851 | global_precision: 0.8678485092667204 | global_recall: 0.9711451758340848 | global_auc: 0.9548445363726625| flobal_FPR: 0.028854824165915238 \n",
      "client_8\n",
      "comm_round: 14 | global_acc: 96.410% | global_loss: 2.99192476272583 | global_f1: 0.9520426287744228 | global_precision: 0.9378827646544182 | global_recall: 0.9666366095581606 | global_auc: 0.9722546297681235| flobal_FPR: 0.033363390441839495 \n",
      "client_9\n",
      "comm_round: 14 | global_acc: 95.080% | global_loss: 3.8999435901641846 | global_f1: 0.9359861591695501 | global_precision: 0.8994181213632585 | global_recall: 0.975653742110009 | global_auc: 0.9659217442049849| flobal_FPR: 0.024346257889990983 \n",
      "client_10\n",
      "comm_round: 14 | global_acc: 93.484% | global_loss: 5.194939136505127 | global_f1: 0.916595744680851 | global_precision: 0.8678485092667204 | global_recall: 0.9711451758340848 | global_auc: 0.9550912135901815| flobal_FPR: 0.028854824165915238 \n",
      "[]\n",
      "[[[1, 116.46570580585586], [2, 99.29087359970578], [4, 94.87684630435146], [8, 116.22380501731426], [10, 116.39225429629603]], [[7, 69.90169073722885], [9, 43.5402178769506]], [[3, 160.98667881457004], [5, 143.72929913668588], [6, 131.33410746235637]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 15 | global_acc: 95.977% | global_loss: 3.2068142890930176 | global_f1: 0.9465311533362794 | global_precision: 0.9280762564991335 | global_recall: 0.9657348963029756 | global_auc: 0.9713187283326472| flobal_FPR: 0.034265103697024346 \n",
      "client_2\n",
      "comm_round: 15 | global_acc: 96.177% | global_loss: 3.181124448776245 | global_f1: 0.9489115948467347 | global_precision: 0.9352014010507881 | global_recall: 0.9630297565374211 | global_auc: 0.9711430390728165| flobal_FPR: 0.0369702434625789 \n",
      "client_3\n",
      "comm_round: 15 | global_acc: 93.783% | global_loss: 5.302964687347412 | global_f1: 0.9202558635394456 | global_precision: 0.8729773462783171 | global_recall: 0.9729486023444545 | global_auc: 0.954268085666083| flobal_FPR: 0.027051397655545536 \n",
      "client_4\n",
      "comm_round: 15 | global_acc: 95.080% | global_loss: 4.326429843902588 | global_f1: 0.9353146853146853 | global_precision: 0.90754877014419 | global_recall: 0.9648331830477908 | global_auc: 0.9622833146010596| flobal_FPR: 0.0351668169522092 \n",
      "client_5\n",
      "comm_round: 15 | global_acc: 95.612% | global_loss: 3.705047845840454 | global_f1: 0.9420035149384886 | global_precision: 0.9185946872322194 | global_recall: 0.9666366095581606 | global_auc: 0.9681404146551434| flobal_FPR: 0.033363390441839495 \n",
      "client_6\n",
      "comm_round: 15 | global_acc: 92.852% | global_loss: 6.227400779724121 | global_f1: 0.9097775912715064 | global_precision: 0.8508634222919937 | global_recall: 0.9774571686203787 | global_auc: 0.9481203860795226| flobal_FPR: 0.02254283137962128 \n",
      "client_7\n",
      "comm_round: 15 | global_acc: 93.418% | global_loss: 5.39589262008667 | global_f1: 0.9158878504672897 | global_precision: 0.8658634538152611 | global_recall: 0.9720468890892696 | global_auc: 0.9535729260001587| flobal_FPR: 0.027953110910730387 \n",
      "client_8\n",
      "comm_round: 15 | global_acc: 96.376% | global_loss: 3.0404770374298096 | global_f1: 0.9518338488731772 | global_precision: 0.9332755632582322 | global_recall: 0.9711451758340848 | global_auc: 0.9731656023221372| flobal_FPR: 0.028854824165915238 \n",
      "client_9\n",
      "comm_round: 15 | global_acc: 93.684% | global_loss: 5.23806095123291 | global_f1: 0.9190110826939473 | global_precision: 0.8714632174616006 | global_recall: 0.9720468890892696 | global_auc: 0.9552398372072815| flobal_FPR: 0.027953110910730387 \n",
      "client_10\n",
      "comm_round: 15 | global_acc: 93.418% | global_loss: 5.423727512359619 | global_f1: 0.9158163265306123 | global_precision: 0.8664521319388576 | global_recall: 0.9711451758340848 | global_auc: 0.9533442925444601| flobal_FPR: 0.028854824165915238 \n",
      "[]\n",
      "[[[1, 168.27061107983488], [5, 146.54941810346347], [6, 153.20747604904997], [7, 151.47493524821144]], [[2, 48.51983767107265], [4, 33.61870845625776], [9, 59.4206959133316]], [[3, 88.43949511164277], [8, 95.28289444628312], [10, 93.57004289166933]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 16 | global_acc: 96.709% | global_loss: 2.6585986614227295 | global_f1: 0.9554655870445344 | global_precision: 0.9533213644524237 | global_recall: 0.957619477006312 | global_auc: 0.975710484992576| flobal_FPR: 0.04238052299368801 \n",
      "client_2\n",
      "comm_round: 16 | global_acc: 96.044% | global_loss: 3.5304152965545654 | global_f1: 0.9475539885412076 | global_precision: 0.9267241379310345 | global_recall: 0.9693417493237151 | global_auc: 0.9676043249947412| flobal_FPR: 0.030658250676284943 \n",
      "client_3\n",
      "comm_round: 16 | global_acc: 95.213% | global_loss: 4.132621765136719 | global_f1: 0.9376623376623375 | global_precision: 0.9017485428809325 | global_recall: 0.9765554553651938 | global_auc: 0.9650587300705464| flobal_FPR: 0.023444544634806132 \n",
      "client_4\n",
      "comm_round: 16 | global_acc: 95.678% | global_loss: 3.700442314147949 | global_f1: 0.9427312775330398 | global_precision: 0.921619293712317 | global_recall: 0.9648331830477908 | global_auc: 0.9648863646615774| flobal_FPR: 0.0351668169522092 \n",
      "client_5\n",
      "comm_round: 16 | global_acc: 94.681% | global_loss: 4.479060173034668 | global_f1: 0.9301919720767888 | global_precision: 0.9010989010989011 | global_recall: 0.9612263300270514 | global_auc: 0.96216721723882| flobal_FPR: 0.0387736699729486 \n",
      "client_6\n",
      "comm_round: 16 | global_acc: 92.387% | global_loss: 6.821056842803955 | global_f1: 0.9045435598165903 | global_precision: 0.8410852713178295 | global_recall: 0.9783588818755635 | global_auc: 0.9431065944726258| flobal_FPR: 0.02164111812443643 \n",
      "client_7\n",
      "comm_round: 16 | global_acc: 93.285% | global_loss: 5.590510368347168 | global_f1: 0.9142614601018676 | global_precision: 0.863672814755413 | global_recall: 0.9711451758340848 | global_auc: 0.9520157018714704| flobal_FPR: 0.028854824165915238 \n",
      "client_8\n",
      "comm_round: 16 | global_acc: 96.476% | global_loss: 2.8763821125030518 | global_f1: 0.9530558015943312 | global_precision: 0.9364664926022629 | global_recall: 0.9702434625788999 | global_auc: 0.9738992236908895| flobal_FPR: 0.029756537421100092 \n",
      "client_9\n",
      "comm_round: 16 | global_acc: 93.584% | global_loss: 5.44426155090332 | global_f1: 0.9182549767047862 | global_precision: 0.865814696485623 | global_recall: 0.9774571686203787 | global_auc: 0.9537718822160208| flobal_FPR: 0.02254283137962128 \n",
      "client_10\n",
      "comm_round: 16 | global_acc: 93.318% | global_loss: 5.560140609741211 | global_f1: 0.9146496815286623 | global_precision: 0.8643659711075441 | global_recall: 0.9711451758340848 | global_auc: 0.9522704512982249| flobal_FPR: 0.028854824165915238 \n",
      "[]\n",
      "[[[1, 129.6021459022816], [2, 123.93882186856987], [5, 92.05374715462952], [7, 119.92358326322375], [8, 104.04655425072349], [9, 117.57711063388304], [10, 129.82621953821868]], [[6, 40.95407552346902]], [[3, 152.00680203088774], [4, 156.27128408781775]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 17 | global_acc: 96.509% | global_loss: 2.8180723190307617 | global_f1: 0.9527665317139001 | global_precision: 0.9506283662477558 | global_recall: 0.9549143372407575 | global_auc: 0.9735516438579271| flobal_FPR: 0.04508566275924256 \n",
      "client_2\n",
      "comm_round: 17 | global_acc: 95.711% | global_loss: 3.995217800140381 | global_f1: 0.9433465085638999 | global_precision: 0.9195205479452054 | global_recall: 0.9684400360685302 | global_auc: 0.9646479970712126| flobal_FPR: 0.031559963931469794 \n",
      "client_3\n",
      "comm_round: 17 | global_acc: 96.110% | global_loss: 3.506437301635742 | global_f1: 0.9484354341119435 | global_precision: 0.9275862068965517 | global_recall: 0.9702434625788999 | global_auc: 0.9675243151561428| flobal_FPR: 0.029756537421100092 \n",
      "client_4\n",
      "comm_round: 17 | global_acc: 96.742% | global_loss: 2.830021381378174 | global_f1: 0.956366874443455 | global_precision: 0.9445910290237467 | global_recall: 0.9684400360685302 | global_auc: 0.9740836974137115| flobal_FPR: 0.031559963931469794 \n",
      "client_5\n",
      "comm_round: 17 | global_acc: 96.210% | global_loss: 3.109232187271118 | global_f1: 0.9495128432240921 | global_precision: 0.9329852045256745 | global_recall: 0.9666366095581606 | global_auc: 0.9711048147879074| flobal_FPR: 0.033363390441839495 \n",
      "client_6\n",
      "comm_round: 17 | global_acc: 90.924% | global_loss: 8.269122123718262 | global_f1: 0.8882521489971347 | global_precision: 0.8133433283358321 | global_recall: 0.9783588818755635 | global_auc: 0.9316886919269836| flobal_FPR: 0.02164111812443643 \n",
      "client_7\n",
      "comm_round: 17 | global_acc: 93.185% | global_loss: 5.7491631507873535 | global_f1: 0.9130987706655362 | global_precision: 0.8616 | global_recall: 0.9711451758340848 | global_auc: 0.9510802752718316| flobal_FPR: 0.028854824165915238 \n",
      "client_8\n",
      "comm_round: 17 | global_acc: 95.844% | global_loss: 3.432384490966797 | global_f1: 0.9451032059727711 | global_precision: 0.9212328767123288 | global_recall: 0.9702434625788999 | global_auc: 0.9688056596633129| flobal_FPR: 0.029756537421100092 \n",
      "client_9\n",
      "comm_round: 17 | global_acc: 94.282% | global_loss: 4.9269280433654785 | global_f1: 0.9264957264957264 | global_precision: 0.8805848903330625 | global_recall: 0.9774571686203787 | global_auc: 0.9581493938008283| flobal_FPR: 0.02254283137962128 \n",
      "client_10\n",
      "comm_round: 17 | global_acc: 93.218% | global_loss: 5.753676414489746 | global_f1: 0.9134860050890585 | global_precision: 0.8622898318654924 | global_recall: 0.9711451758340848 | global_auc: 0.950709191064919| flobal_FPR: 0.028854824165915238 \n",
      "[]\n",
      "[[[1, 199.4083293604202], [3, 182.6819033153365], [7, 153.25026375147795]], [[4, 61.9269891010187], [5, 45.783359887036326], [6, 40.377694176999796], [10, 38.10823710048015]], [[2, 80.99876155672001], [8, 108.94304997999214], [9, 100.64222856241427]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 18 | global_acc: 95.911% | global_loss: 3.4624617099761963 | global_f1: 0.9456953642384106 | global_precision: 0.9264705882352942 | global_recall: 0.9657348963029756 | global_auc: 0.9688213292459465| flobal_FPR: 0.034265103697024346 \n",
      "client_2\n",
      "comm_round: 18 | global_acc: 95.911% | global_loss: 3.746164083480835 | global_f1: 0.945551128818061 | global_precision: 0.928695652173913 | global_recall: 0.9630297565374211 | global_auc: 0.9657168525411552| flobal_FPR: 0.0369702434625789 \n",
      "client_3\n",
      "comm_round: 18 | global_acc: 93.750% | global_loss: 5.760910511016846 | global_f1: 0.9197267292912041 | global_precision: 0.8734793187347932 | global_recall: 0.9711451758340848 | global_auc: 0.9502476506309856| flobal_FPR: 0.028854824165915238 \n",
      "client_4\n",
      "comm_round: 18 | global_acc: 96.210% | global_loss: 3.417461633682251 | global_f1: 0.9498239436619718 | global_precision: 0.9277730008598453 | global_recall: 0.9729486023444545 | global_auc: 0.9687728959905336| flobal_FPR: 0.027051397655545536 \n",
      "client_5\n",
      "comm_round: 18 | global_acc: 94.215% | global_loss: 5.063652992248535 | global_f1: 0.9251934651762683 | global_precision: 0.8841413311421529 | global_recall: 0.9702434625788999 | global_auc: 0.9558887003790614| flobal_FPR: 0.029756537421100092 \n",
      "client_6\n",
      "comm_round: 18 | global_acc: 92.088% | global_loss: 7.287339687347412 | global_f1: 0.9005016722408027 | global_precision: 0.8394388152766953 | global_recall: 0.9711451758340848 | global_auc: 0.9367965010296815| flobal_FPR: 0.028854824165915238 \n",
      "client_7\n",
      "comm_round: 18 | global_acc: 93.152% | global_loss: 5.889355182647705 | global_f1: 0.9127118644067796 | global_precision: 0.8609112709832134 | global_recall: 0.9711451758340848 | global_auc: 0.9500142688169132| flobal_FPR: 0.028854824165915238 \n",
      "client_8\n",
      "comm_round: 18 | global_acc: 95.977% | global_loss: 3.675977945327759 | global_f1: 0.9465783664459161 | global_precision: 0.9273356401384083 | global_recall: 0.9666366095581606 | global_auc: 0.9659561698031948| flobal_FPR: 0.033363390441839495 \n",
      "client_9\n",
      "comm_round: 18 | global_acc: 94.082% | global_loss: 5.215440273284912 | global_f1: 0.9239965841161399 | global_precision: 0.8775344687753447 | global_recall: 0.975653742110009 | global_auc: 0.9552849466118325| flobal_FPR: 0.024346257889990983 \n",
      "client_10\n",
      "comm_round: 18 | global_acc: 93.152% | global_loss: 5.887876987457275 | global_f1: 0.9127118644067796 | global_precision: 0.8609112709832134 | global_recall: 0.9711451758340848 | global_auc: 0.9500156933244254| flobal_FPR: 0.028854824165915238 \n",
      "[]\n",
      "[[[1, 24.26906875192752], [5, 28.75309689742489], [6, 41.35841986704281], [7, 33.60724285443184], [10, 57.514190657317094]], [[2, 165.79605723350895], [4, 186.48115797351275], [9, 185.66334173315343]], [[3, 106.52730973019091], [8, 121.44095815209558]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 19 | global_acc: 95.778% | global_loss: 3.819624662399292 | global_f1: 0.9433794025858226 | global_precision: 0.9329805996472663 | global_recall: 0.9540126239855726 | global_auc: 0.9629594808334888| flobal_FPR: 0.045987376014427414 \n",
      "client_2\n",
      "comm_round: 19 | global_acc: 96.077% | global_loss: 3.5479564666748047 | global_f1: 0.947085201793722 | global_precision: 0.9420160570918823 | global_recall: 0.9522091974752029 | global_auc: 0.9700677733190692| flobal_FPR: 0.047790802524797116 \n",
      "client_3\n",
      "comm_round: 19 | global_acc: 94.382% | global_loss: 5.115104675292969 | global_f1: 0.9271865575183111 | global_precision: 0.8877887788778878 | global_recall: 0.9702434625788999 | global_auc: 0.955755034090839| flobal_FPR: 0.029756537421100092 \n",
      "client_4\n",
      "comm_round: 19 | global_acc: 95.578% | global_loss: 3.9461820125579834 | global_f1: 0.9415384615384615 | global_precision: 0.91852487135506 | global_recall: 0.9657348963029756 | global_auc: 0.9649257760360799| flobal_FPR: 0.034265103697024346 \n",
      "client_5\n",
      "comm_round: 19 | global_acc: 96.410% | global_loss: 3.3370604515075684 | global_f1: 0.9518716577540107 | global_precision: 0.9409691629955947 | global_recall: 0.9630297565374211 | global_auc: 0.9679673369924183| flobal_FPR: 0.0369702434625789 \n",
      "client_6\n",
      "comm_round: 19 | global_acc: 92.919% | global_loss: 6.629067897796631 | global_f1: 0.9109903886335144 | global_precision: 0.8489096573208723 | global_recall: 0.9828674481514879 | global_auc: 0.9450159093747315| flobal_FPR: 0.017132551848512173 \n",
      "client_7\n",
      "comm_round: 19 | global_acc: 93.052% | global_loss: 6.167196273803711 | global_f1: 0.9115531104528143 | global_precision: 0.8588516746411483 | global_recall: 0.9711451758340848 | global_auc: 0.9469133533809024| flobal_FPR: 0.028854824165915238 \n",
      "client_8\n",
      "comm_round: 19 | global_acc: 95.711% | global_loss: 3.82879900932312 | global_f1: 0.9427937915742794 | global_precision: 0.9275741710296684 | global_recall: 0.9585211902614968 | global_auc: 0.9644200758692701| flobal_FPR: 0.04147880973850315 \n",
      "client_9\n",
      "comm_round: 19 | global_acc: 95.279% | global_loss: 4.093437194824219 | global_f1: 0.9376646180860404 | global_precision: 0.913601368691189 | global_recall: 0.9630297565374211 | global_auc: 0.9653429193192185| flobal_FPR: 0.0369702434625789 \n",
      "client_10\n",
      "comm_round: 19 | global_acc: 93.085% | global_loss: 6.166766166687012 | global_f1: 0.9119390347163421 | global_precision: 0.859537110933759 | global_recall: 0.9711451758340848 | global_auc: 0.9469133533809024| flobal_FPR: 0.028854824165915238 \n",
      "[]\n",
      "[[[5, 83.79401151023801], [7, 57.6435551650541], [9, 79.8569563356262]], [[4, 185.47302450490028], [6, 151.0542056501247], [8, 174.44284945106995]], [[1, 23.94294876189098], [2, 32.09318716014594], [3, 41.03346806253851], [10, 37.353073468004176]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 20 | global_acc: 96.443% | global_loss: 3.42661452293396 | global_f1: 0.9525919361984937 | global_precision: 0.936411149825784 | global_recall: 0.9693417493237151 | global_auc: 0.9676401751004635| flobal_FPR: 0.030658250676284943 \n",
      "client_2\n",
      "comm_round: 20 | global_acc: 95.612% | global_loss: 3.7305071353912354 | global_f1: 0.9409660107334527 | global_precision: 0.9334516415261757 | global_recall: 0.9486023444544635 | global_auc: 0.965315616258569| flobal_FPR: 0.05139765554553652 \n",
      "client_3\n",
      "comm_round: 20 | global_acc: 95.811% | global_loss: 3.80267333984375 | global_f1: 0.9445910290237467 | global_precision: 0.9218884120171674 | global_recall: 0.9684400360685302 | global_auc: 0.9656071654627204| flobal_FPR: 0.031559963931469794 \n",
      "client_4\n",
      "comm_round: 20 | global_acc: 95.778% | global_loss: 3.7974460124969482 | global_f1: 0.9440774988991634 | global_precision: 0.9225473321858864 | global_recall: 0.9666366095581606 | global_auc: 0.9642854599093728| flobal_FPR: 0.033363390441839495 \n",
      "client_5\n",
      "comm_round: 20 | global_acc: 94.980% | global_loss: 4.554969310760498 | global_f1: 0.9347168179853005 | global_precision: 0.8978405315614618 | global_recall: 0.9747520288548241 | global_auc: 0.959372096082082| flobal_FPR: 0.025247971145175834 \n",
      "client_6\n",
      "comm_round: 20 | global_acc: 90.525% | global_loss: 8.650897026062012 | global_f1: 0.8841934173100365 | global_precision: 0.8047337278106509 | global_recall: 0.9810640216411182 | global_auc: 0.9284018782606385| flobal_FPR: 0.018935978358881875 \n",
      "client_7\n",
      "comm_round: 20 | global_acc: 93.052% | global_loss: 6.23107385635376 | global_f1: 0.9115531104528143 | global_precision: 0.8588516746411483 | global_recall: 0.9711451758340848 | global_auc: 0.9463250317783884| flobal_FPR: 0.028854824165915238 \n",
      "client_8\n",
      "comm_round: 20 | global_acc: 96.376% | global_loss: 3.312610387802124 | global_f1: 0.9512304250559284 | global_precision: 0.9440497335701599 | global_recall: 0.9585211902614968 | global_auc: 0.9674927385729569| flobal_FPR: 0.04147880973850315 \n",
      "client_9\n",
      "comm_round: 20 | global_acc: 94.548% | global_loss: 4.956655979156494 | global_f1: 0.929553264604811 | global_precision: 0.8876127973748975 | global_recall: 0.975653742110009 | global_auc: 0.957889183761944| flobal_FPR: 0.024346257889990983 \n",
      "client_10\n",
      "comm_round: 20 | global_acc: 93.085% | global_loss: 6.228438854217529 | global_f1: 0.9119390347163421 | global_precision: 0.859537110933759 | global_recall: 0.9711451758340848 | global_auc: 0.9463217079275267| flobal_FPR: 0.028854824165915238 \n",
      "[]\n",
      "[[[1, 156.38597914617472], [2, 124.10345348011035], [4, 178.29909748406908], [6, 175.33155640343014], [8, 153.55978283618055], [10, 137.32322819417297]], [[5, 89.78113776582884], [7, 78.84990601813469], [9, 82.26375213893294]], [[3, 225.6260694876125]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 21 | global_acc: 96.011% | global_loss: 3.571669578552246 | global_f1: 0.9464285714285714 | global_precision: 0.9372236958443855 | global_recall: 0.9558160504959423 | global_auc: 0.9650969543554554| flobal_FPR: 0.04418394950405771 \n",
      "client_2\n",
      "comm_round: 21 | global_acc: 94.481% | global_loss: 5.226027965545654 | global_f1: 0.9286941580756013 | global_precision: 0.8867924528301887 | global_recall: 0.9747520288548241 | global_auc: 0.9542797191440988| flobal_FPR: 0.025247971145175834 \n",
      "client_3\n",
      "comm_round: 21 | global_acc: 95.180% | global_loss: 4.505746841430664 | global_f1: 0.9369839200347675 | global_precision: 0.9043624161073825 | global_recall: 0.9720468890892696 | global_auc: 0.9599765621030669| flobal_FPR: 0.027953110910730387 \n",
      "client_4\n",
      "comm_round: 21 | global_acc: 95.678% | global_loss: 3.922945022583008 | global_f1: 0.9428320140721196 | global_precision: 0.9201716738197425 | global_recall: 0.9666366095581606 | global_auc: 0.9644188887796765| flobal_FPR: 0.033363390441839495 \n",
      "client_5\n",
      "comm_round: 21 | global_acc: 95.678% | global_loss: 3.9681828022003174 | global_f1: 0.9424778761061947 | global_precision: 0.9252823631624674 | global_recall: 0.9603246167718665 | global_auc: 0.9620577675783041| flobal_FPR: 0.03967538322813345 \n",
      "client_6\n",
      "comm_round: 21 | global_acc: 93.085% | global_loss: 6.317158222198486 | global_f1: 0.9119390347163421 | global_precision: 0.859537110933759 | global_recall: 0.9711451758340848 | global_auc: 0.9453782091186526| flobal_FPR: 0.028854824165915238 \n",
      "client_7\n",
      "comm_round: 21 | global_acc: 92.985% | global_loss: 6.290513038635254 | global_f1: 0.9107822410147992 | global_precision: 0.857484076433121 | global_recall: 0.9711451758340848 | global_auc: 0.9457642506544426| flobal_FPR: 0.028854824165915238 \n",
      "client_8\n",
      "comm_round: 21 | global_acc: 96.609% | global_loss: 3.021324872970581 | global_f1: 0.9544642857142858 | global_precision: 0.9451812555260831 | global_recall: 0.9639314697926059 | global_auc: 0.9710502086666087| flobal_FPR: 0.03606853020739405 \n",
      "client_9\n",
      "comm_round: 21 | global_acc: 94.049% | global_loss: 5.611772060394287 | global_f1: 0.9235369500213585 | global_precision: 0.877435064935065 | global_recall: 0.9747520288548241 | global_auc: 0.9514309415377369| flobal_FPR: 0.025247971145175834 \n",
      "client_10\n",
      "comm_round: 21 | global_acc: 92.985% | global_loss: 6.288945198059082 | global_f1: 0.9107822410147992 | global_precision: 0.857484076433121 | global_recall: 0.9711451758340848 | global_auc: 0.9457566532810443| flobal_FPR: 0.028854824165915238 \n",
      "[]\n",
      "[[[1, 76.59145610792764], [7, 60.53822925208479], [9, 70.76343390421236], [10, 83.91136736914054]], [[2, 193.48293459162528], [3, 141.74525934515555], [5, 170.0276421046942]], [[4, 41.651488895502936], [6, 31.758203397489247], [8, 37.385229335043576]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 22 | global_acc: 96.243% | global_loss: 3.399648904800415 | global_f1: 0.9496659242761691 | global_precision: 0.9383802816901409 | global_recall: 0.9612263300270514 | global_auc: 0.9673626335535147| flobal_FPR: 0.0387736699729486 \n",
      "client_2\n",
      "comm_round: 22 | global_acc: 95.711% | global_loss: 3.863943338394165 | global_f1: 0.9426921368280764 | global_precision: 0.9290718038528897 | global_recall: 0.9567177637511272 | global_auc: 0.9630853123303946| flobal_FPR: 0.04328223624887286 \n",
      "client_3\n",
      "comm_round: 22 | global_acc: 93.883% | global_loss: 5.658862113952637 | global_f1: 0.9215686274509803 | global_precision: 0.8738884397736459 | global_recall: 0.9747520288548241 | global_auc: 0.9509686888500472| flobal_FPR: 0.025247971145175834 \n",
      "client_4\n",
      "comm_round: 22 | global_acc: 95.944% | global_loss: 3.8317015171051025 | global_f1: 0.9463500439753738 | global_precision: 0.9236051502145923 | global_recall: 0.9702434625788999 | global_auc: 0.9647382158803147| flobal_FPR: 0.029756537421100092 \n",
      "client_5\n",
      "comm_round: 22 | global_acc: 94.880% | global_loss: 4.936209678649902 | global_f1: 0.9324561403508772 | global_precision: 0.9077711357813835 | global_recall: 0.9585211902614968 | global_auc: 0.9532061153157825| flobal_FPR: 0.04147880973850315 \n",
      "client_6\n",
      "comm_round: 22 | global_acc: 92.819% | global_loss: 6.6320061683654785 | global_f1: 0.9090143218197135 | global_precision: 0.8529644268774703 | global_recall: 0.9729486023444545 | global_auc: 0.9431001841888214| flobal_FPR: 0.027051397655545536 \n",
      "client_7\n",
      "comm_round: 22 | global_acc: 92.952% | global_loss: 6.543027400970459 | global_f1: 0.9103972950126796 | global_precision: 0.8568019093078759 | global_recall: 0.9711451758340848 | global_auc: 0.9432426349400355| flobal_FPR: 0.028854824165915238 \n",
      "client_8\n",
      "comm_round: 22 | global_acc: 95.612% | global_loss: 4.2299628257751465 | global_f1: 0.942458587619878 | global_precision: 0.9122362869198313 | global_recall: 0.9747520288548241 | global_auc: 0.9622894874669454| flobal_FPR: 0.025247971145175834 \n",
      "client_9\n",
      "comm_round: 22 | global_acc: 93.451% | global_loss: 6.195122241973877 | global_f1: 0.9168425495989869 | global_precision: 0.861904761904762 | global_recall: 0.9792605951307484 | global_auc: 0.9475695765081616| flobal_FPR: 0.020739404869251576 \n",
      "client_10\n",
      "comm_round: 22 | global_acc: 92.985% | global_loss: 6.401517868041992 | global_f1: 0.9107822410147992 | global_precision: 0.857484076433121 | global_recall: 0.9711451758340848 | global_auc: 0.9445705133592689| flobal_FPR: 0.028854824165915238 \n",
      "[]\n",
      "[[[7, 346.5428541525436]], [[2, 197.6716742483814], [3, 127.02580068549625], [8, 144.01180393642608], [9, 162.61734924132327], [10, 173.98867502736584]], [[1, 59.22387505589636], [4, 96.87534404717611], [5, 69.09299844903202], [6, 50.93823499987083]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 23 | global_acc: 94.149% | global_loss: 5.445843696594238 | global_f1: 0.919487648673376 | global_precision: 0.9331476323119777 | global_recall: 0.9062218214607755 | global_auc: 0.9413696449794895| flobal_FPR: 0.09377817853922453 \n",
      "client_2\n",
      "comm_round: 23 | global_acc: 96.310% | global_loss: 3.3376710414886475 | global_f1: 0.9497054825555054 | global_precision: 0.9544626593806922 | global_recall: 0.9449954914337241 | global_auc: 0.965648001344735| flobal_FPR: 0.05500450856627592 \n",
      "client_3\n",
      "comm_round: 23 | global_acc: 94.614% | global_loss: 5.107931137084961 | global_f1: 0.9296875 | global_precision: 0.896234309623431 | global_recall: 0.9657348963029756 | global_auc: 0.9537740189772891| flobal_FPR: 0.034265103697024346 \n",
      "client_4\n",
      "comm_round: 23 | global_acc: 94.914% | global_loss: 4.633828639984131 | global_f1: 0.9330415754923413 | global_precision: 0.9064625850340136 | global_recall: 0.9612263300270514 | global_auc: 0.9585031464996764| flobal_FPR: 0.0387736699729486 \n",
      "client_5\n",
      "comm_round: 23 | global_acc: 94.614% | global_loss: 5.26345682144165 | global_f1: 0.9304721030042918 | global_precision: 0.8877968877968878 | global_recall: 0.9774571686203787 | global_auc: 0.9541648088714529| flobal_FPR: 0.02254283137962128 \n",
      "client_6\n",
      "comm_round: 23 | global_acc: 85.173% | global_loss: 14.455854415893555 | global_f1: 0.8311884935654807 | global_precision: 0.7162426614481409 | global_recall: 0.9900811541929666 | global_auc: 0.8834441837595697| flobal_FPR: 0.009918845807033363 \n",
      "client_7\n",
      "comm_round: 23 | global_acc: 92.886% | global_loss: 6.59511661529541 | global_f1: 0.9096283783783784 | global_precision: 0.8554408260524226 | global_recall: 0.9711451758340848 | global_auc: 0.9426319960531644| flobal_FPR: 0.028854824165915238 \n",
      "client_8\n",
      "comm_round: 23 | global_acc: 95.512% | global_loss: 4.216606140136719 | global_f1: 0.9402919062361788 | global_precision: 0.9227430555555556 | global_recall: 0.9585211902614968 | global_auc: 0.9610855412012682| flobal_FPR: 0.04147880973850315 \n",
      "client_9\n",
      "comm_round: 23 | global_acc: 94.681% | global_loss: 4.986501693725586 | global_f1: 0.9305555555555556 | global_precision: 0.897071129707113 | global_recall: 0.9666366095581606 | global_auc: 0.9544948197784322| flobal_FPR: 0.033363390441839495 \n",
      "client_10\n",
      "comm_round: 23 | global_acc: 92.919% | global_loss: 6.595103740692139 | global_f1: 0.9100126742712293 | global_precision: 0.856120826709062 | global_recall: 0.9711451758340848 | global_auc: 0.9426248735156038| flobal_FPR: 0.028854824165915238 \n",
      "[]\n",
      "[[[1, 273.8072944284678], [2, 251.92684334183906], [7, 216.15821401125189], [9, 227.15759857471832]], [[3, 145.35921055094428], [4, 125.02554225862579], [6, 155.9588068075733], [8, 139.84626929683805]], [[5, 75.44723454780927], [10, 82.94265894746815]]]\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "client_1\n",
      "comm_round: 24 | global_acc: 96.576% | global_loss: 3.1990556716918945 | global_f1: 0.9535408209291836 | global_precision: 0.953971119133574 | global_recall: 0.9531109107303878 | global_auc: 0.9666444443494773| flobal_FPR: 0.046889089269612265 \n",
      "client_2\n",
      "comm_round: 24 | global_acc: 94.614% | global_loss: 4.86054801940918 | global_f1: 0.927743086529884 | global_precision: 0.9179170344218888 | global_recall: 0.9377817853922452 | global_auc: 0.9531940070019295| flobal_FPR: 0.062218214607754736 \n",
      "client_3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 235\u001b[0m\n\u001b[0;32m    229\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(TensorDataset(torch\u001b[39m.\u001b[39mtensor(clients_batched[client]\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mtensors[\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32),\n\u001b[0;32m    230\u001b[0m                                         torch\u001b[39m.\u001b[39mtensor(clients_batched[client]\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mtensors[\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)),\n\u001b[0;32m    231\u001b[0m                           batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    233\u001b[0m train_model(local_model, train_loader, loss, optimizer)\n\u001b[1;32m--> 235\u001b[0m train_model_prox(local_model, cluster_weights,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n\u001b[0;32m    236\u001b[0m Z_weights,x_hats \u001b[39m=\u001b[39m update_z_parameter(local_model,cluster_weights,Z_weights,x_hats,client,rho)\n\u001b[0;32m    239\u001b[0m cpu_freq_after \u001b[39m=\u001b[39m psutil\u001b[39m.\u001b[39mcpu_freq()\n",
      "Cell \u001b[1;32mIn[14], line 30\u001b[0m, in \u001b[0;36mtrain_model_prox\u001b[1;34m(model, global_parameters, train_loader, loss_fn, optimizer, client, rho, Z_weights, x_hats, mu)\u001b[0m\n\u001b[0;32m     26\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (mu \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mnorm(param \u001b[39m-\u001b[39m param_global, p\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39msum(z_weight \u001b[39m*\u001b[39m (param \u001b[39m-\u001b[39m param_global))\n\u001b[0;32m     28\u001b[0m \u001b[39m# Now 'inner_product' contains the inner product between model.parameters() and global_model.parameters()\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m loss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)  \u001b[39m# Set retain_graph=True to retain the computation graph\u001b[39;00m\n\u001b[0;32m     31\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def train_model_prox(model,global_parameters, train_loader, loss_fn, optimizer,client ,rho,Z_weights,x_hats,mu=0.01):\n",
    "    model.train()\n",
    "    if global_parameters == None: \n",
    "        global_parameters=model.parameters()\n",
    "    for i in range(random.randint(1, 20)):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            for z_weight, (param, param_global) in zip(Z_weights[int(client[-1])-1], zip(model.parameters(), global_parameters)):\n",
    "                loss += (mu / 2) * torch.norm(param - param_global, p=2)**2 + torch.sum(z_weight * (param - param_global))\n",
    "\n",
    "            # Now 'inner_product' contains the inner product between model.parameters() and global_model.parameters()\n",
    "\n",
    "            loss.backward(retain_graph=True)  # Set retain_graph=True to retain the computation graph\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "def update_z_parameter(local_model,global_parameters,Z_weights,x_hats,client,rho):\n",
    "    # Update Z_weight\n",
    "    if global_parameters == None: \n",
    "        global_parameters=local_model.parameters()\n",
    "    for i, (z_weight, (param, param_global)) in enumerate(zip(Z_weights[int(client[-1])-1], zip(local_model.parameters(), global_parameters))):\n",
    "        Z_weights[int(client[-1])-1][i] += rho * (param - param_global)\n",
    "    temp = copy.copy(x_hats[int(client[-1])-1])\n",
    "\n",
    "    for i, (x_hat, z_weight, param_model) in enumerate(zip(x_hats[int(client[-1])-1], Z_weights[int(client[-1])-1], local_model.parameters())):\n",
    "        x_hats[int(client[-1])-1][i] = param_model + z_weight / rho\n",
    "\n",
    "\n",
    "\n",
    "    return Z_weights,x_hats\n",
    "\n",
    "# Example: Sparse binary backdoor pattern for a dataset with binary features\n",
    "num_features = 265\n",
    "num_modified_features = 20\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for _ in range(random.randint(1, 20)):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "K_cluster =3\n",
    "all_avg = []\n",
    "all_std = []\n",
    "target_client = 'client_3'  # Change this to the desired target client\n",
    "all_avg = []\n",
    "all_std = []\n",
    "mu = 0.01\n",
    "rho =0.01\n",
    "n_clients = [10]\n",
    "n_round = [30]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "            \n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features_column_num = use_data.shape[1] - 1\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, features_column_num)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients_non_iid(X_train, [tuple(label) for label in y_train.astype(int).tolist()], num_clients=number_of_clients, initial='client')\n",
    "    \n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "            all_results = [[],[],[],[],[],[],[],[],[],[]]\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "            Z_weights=[]\n",
    "            x_hats = []\n",
    "\n",
    "            # initialize global model\n",
    "            global_model_list =list()\n",
    "            # smlp_global1 = SimpleLSTM(input_size=X.shape[1], hidden_size=128, num_layers=2, classes=1)\n",
    "            # smlp_global2 = SimpleLSTM(input_size=X.shape[1], hidden_size=128, num_layers=2, classes=1)\n",
    "            # smlp_global3 = SimpleLSTM(input_size=X.shape[1], hidden_size=128, num_layers=2, classes=1)\n",
    "\n",
    "            smlp_global1 = SimpleMLP(X.shape[1], 1)\n",
    "            smlp_global2 = SimpleMLP(X.shape[1], 1)\n",
    "            smlp_global3 = SimpleMLP(X.shape[1], 1)\n",
    "            global_model_list = [smlp_global1,smlp_global2,smlp_global3]\n",
    "            Z_weights=[]\n",
    "            x_hats = []\n",
    "            for i in range(number_of_clients):\n",
    "                Z_weight = [torch.zeros_like(param) for param in smlp_global1.parameters()]\n",
    "                Z_weights.append(Z_weight)\n",
    "                x_hat = [torch.zeros_like(param) for param in smlp_global1.parameters()]\n",
    "                x_hats.append(x_hat)\n",
    "\n",
    "            num_modified_features = 20\n",
    "            # Create a sparse binary backdoor pattern\n",
    "            backdoor_pattern = np.zeros(features_column_num)\n",
    "            modified_indices = np.random.choice(features_column_num, num_modified_features, replace=False)\n",
    "            backdoor_pattern[modified_indices] = 1  # Set modified features to 1\n",
    "            error_list_history = [[] for _ in range(10)]\n",
    "\n",
    "            cluster_group =[[],[],[]]    \n",
    "            # -----------------------------------------------\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "            \n",
    "            list_average_weights=list()\n",
    "            list_clients_weights_old =list()\n",
    "            list_clients_weights_new =list()\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # global_weights_list =list()\n",
    "                # for i in range(K_cluster):\n",
    "                #     global_weights_list.append(param.data.clone() for param in global_model_list[i].parameters())\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "                list_clients_weights_delta =list()\n",
    "                scaled_x_hats_list = list()\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                sending_time = random.uniform(1, 3)\n",
    "                UTILL_usage_list = list()\n",
    "                list_similarity_matrix = list()\n",
    "                for client in client_names:\n",
    "\n",
    "                    print(client)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    tracemalloc.start()\n",
    "                    cpu_before = psutil.cpu_percent(interval=None)\n",
    "                    cpu_freq_before = psutil.cpu_freq()\n",
    "                     \n",
    "\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "   \n",
    "                    # set local model weight to the weight of the global model\n",
    "                    # clients_batched = add_backdoor_attack(clients_batched, backdoor_pattern, target_client)\n",
    "\n",
    "                   # Find the location of the element 3\n",
    "                    location = None\n",
    "                    cluster_weights = None\n",
    "                    # print(list_average_weights)\n",
    "\n",
    "                    for i, sublist in enumerate(cluster_group):\n",
    "\n",
    "                        if int(pattern.findall(client)[0]) in sublist:\n",
    "                            location = i\n",
    "                            cluster_weights =list_average_weights[location]\n",
    "                            local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), cluster_weights)})\n",
    "                            break\n",
    "\n",
    "                    # if cluster_weights == None:\n",
    "                    #     cluster_weights = global_weights_list[0]\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "                    \n",
    "                    train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "                    train_model_prox(local_model, cluster_weights,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                    Z_weights,x_hats = update_z_parameter(local_model,cluster_weights,Z_weights,x_hats,client,rho)\n",
    "\n",
    "                    \n",
    "                    cpu_freq_after = psutil.cpu_freq()\n",
    "                    mean_cpu_freq_within_loop = (cpu_freq_before.current + cpu_freq_after.current) / 2\n",
    "                    cpu_after = psutil.cpu_percent(interval=None)\n",
    "                    memorey =tracemalloc.get_traced_memory()\n",
    "                    memorey= memorey[1]-memorey[0]\n",
    "                    tracemalloc.stop()\n",
    "                    num_used_cpus = multiprocessing.cpu_count()\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    recieved_time =random.uniform(1, 5)\n",
    "                    cpu_usage = np.abs(cpu_after-cpu_before)\n",
    "                    if cpu_usage==0:\n",
    "                        cpu_usage=0.01\n",
    "                    UTILL =sending_time + recieved_time+ alpha* elapsed_time +beta *np.log10(memorey) + gamma*np.log10(cpu_usage*mean_cpu_freq_within_loop*num_used_cpus)\n",
    "                    UTILL_usage_list.append([int(pattern.findall(client)[0]),UTILL])\n",
    "                    list_clients_weights_new.append([int(pattern.findall(client)[0]),local_model.state_dict().values()])\n",
    "                    list_clients_weights_delta.append(local_model.state_dict().items())\n",
    "\n",
    "                    # list_clients_weights_delta = \n",
    "                    torch.cuda.empty_cache()\n",
    "                    # print(client)\n",
    "                    # test global model and print out metrics after each communications round\n",
    "                    for X_test_batch, Y_test_batch in test_batched:\n",
    "                        global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, local_model, comm_round)\n",
    "                        all_results[int(pattern.findall(client)[0])-1].append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "                        error_list_history[int(pattern.findall(client)[0])-1].append(global_loss)\n",
    "                \n",
    "                # ...\n",
    "                # Extract the energy values for clustering\n",
    "                \n",
    "                # Sort data based on client number\n",
    "                UTILL_usage_list = sorted(UTILL_usage_list, key=lambda x: x[0])\n",
    "                energy_values = np.array([entry[1] for entry in UTILL_usage_list]).reshape(-1, 1)\n",
    "                list_clients_weights_new = sorted(list_clients_weights_new, key=lambda x: x[0])\n",
    "                list_similarity_matrix = sorted(list_similarity_matrix, key=lambda x: x[0])\n",
    "                print(list_similarity_matrix)\n",
    "                # Apply k-means clustering\n",
    "                try:\n",
    "                    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "                    clusters = kmeans.fit_predict(energy_values)\n",
    "                except:\n",
    "                    print(energy_values)\n",
    "                list_average_weights = list()\n",
    "\n",
    "                # Create separate lists for each cluster\n",
    "                cluster_lists = [[] for _ in range(3)]\n",
    "\n",
    "                # Distribute clients into cluster lists\n",
    "                for i, client_entry in enumerate(UTILL_usage_list):\n",
    "                    cluster_lists[clusters[i]].append(client_entry)\n",
    "                # Print the result\n",
    "                cluster_group= [[],[],[]]\n",
    "                print(cluster_lists)\n",
    "                # print(cluster_lists)\n",
    "                for cluster_idx, cluster_list in enumerate(cluster_lists):\n",
    "                    learning_threshold = cluster_idx / 2\n",
    "                    print(learning_threshold)\n",
    "                    clients_batched_cluster = dict()\n",
    "                    scaled_x_hats_list = list()\n",
    "                    for client_entry in cluster_list:\n",
    "                        name =client_name_base+ '_' +str(client_entry[0]) \n",
    "                        clients_batched_cluster[name] =clients_batched[name]\n",
    "                    for client_entry in cluster_list:\n",
    "                        name = client_name_base+ '_' +str(client_entry[0]) \n",
    "                        scaling_factor = weight_scalling_factor(clients_batched_cluster, name)\n",
    "                        scaled_x_hats = scale_model_weights(x_hats[int(client[-1])-1], scaling_factor)\n",
    "                        scaled_x_hats_list.append(scaled_x_hats)\n",
    "                        cluster_group[cluster_idx].append(client_entry[0])\n",
    "                    average_weights = sum_scaled_weights(scaled_x_hats_list)\n",
    "                    list_average_weights.append(average_weights)\n",
    "                average_weights = sum_scaled_weights(scaled_x_hats_list)\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "            for i in range(len(all_results)):\n",
    "                all_R = pd.DataFrame(all_results[i], columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "                flname = f'results/round-100/{cl}-clients/FedAdmm-{dataset[d]}-clisel-noniid-results_{i}.csv'\n",
    "                all_R.to_csv(flname, index=None)\n",
    "\n",
    "#             all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "#             all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "# ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "# ALL_AVG.to_csv('FedAvg-results.csv')\n",
    "\n",
    "# ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "# ALL_STD.to_csv('FedAvg-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
